{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Instance Regret\n",
    "\n",
    "Bernoulli Bandit with 3 arms. We study per-timestep and cumulative instance regret\n",
    "for Greedy, Thompson Sampling, and Bayes-UCB agents.\n",
    "\n",
    "**Parameters:** `num_sims = 500`, `num_timesteps = 2000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "NUM_SIMS = 500\n",
    "NUM_TIMESTEPS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Bandit Environment and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit:\n",
    "    \"\"\"Bernoulli bandit environment with K arms.\"\"\"\n",
    "    def __init__(self, probs):\n",
    "        self.probs = np.array(probs)\n",
    "        self.k = len(probs)\n",
    "        self.best_arm = np.argmax(probs)\n",
    "        self.best_prob = np.max(probs)\n",
    "\n",
    "    def pull(self, arm):\n",
    "        return np.random.binomial(1, self.probs[arm])\n",
    "\n",
    "\n",
    "class GreedyAgent:\n",
    "    \"\"\"Greedy agent with Beta(1,1) uniform prior. Always picks highest posterior mean.\"\"\"\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.alpha = np.ones(k)  # Beta prior: alpha=1\n",
    "        self.beta = np.ones(k)   # Beta prior: beta=1\n",
    "\n",
    "    def select_arm(self, t):\n",
    "        means = self.alpha / (self.alpha + self.beta)\n",
    "        return np.argmax(means)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.alpha[arm] += reward\n",
    "        self.beta[arm] += 1 - reward\n",
    "\n",
    "\n",
    "class ThompsonSamplingAgent:\n",
    "    \"\"\"Thompson Sampling with Beta(1,1) uniform prior.\"\"\"\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.alpha = np.ones(k)\n",
    "        self.beta = np.ones(k)\n",
    "\n",
    "    def select_arm(self, t):\n",
    "        samples = np.array([np.random.beta(self.alpha[a], self.beta[a]) for a in range(self.k)])\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.alpha[arm] += reward\n",
    "        self.beta[arm] += 1 - reward\n",
    "\n",
    "\n",
    "class BayesUCBAgent:\n",
    "    \"\"\"Bayes-UCB agent with Beta(1,1) uniform prior.\n",
    "    Uses quantile 1 - 1/(t+1) of the posterior as the UCB.\"\"\"\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.alpha = np.ones(k)\n",
    "        self.beta = np.ones(k)\n",
    "\n",
    "    def select_arm(self, t):\n",
    "        quantile = 1.0 - 1.0 / (t + 1)\n",
    "        ucb_values = np.array([\n",
    "            stats.beta.ppf(quantile, self.alpha[a], self.beta[a])\n",
    "            for a in range(self.k)\n",
    "        ])\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        self.alpha[arm] += reward\n",
    "        self.beta[arm] += 1 - reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(env, agent_class, num_sims, num_timesteps):\n",
    "    \"\"\"\n",
    "    Run bandit simulation and collect per-timestep instance regret.\n",
    "    \n",
    "    Instance regret at time t: regret(e, t) = p* - p_{a_t}\n",
    "    where a_t is the arm pulled at time t.\n",
    "    \n",
    "    We average over num_sims simulations to estimate E[regret(e, t)].\n",
    "    \"\"\"\n",
    "    # regret_per_step[sim, t] = p* - p_{arm pulled at t}\n",
    "    regret_per_step = np.zeros((num_sims, num_timesteps))\n",
    "    \n",
    "    for sim in range(num_sims):\n",
    "        agent = agent_class(env.k)\n",
    "        for t in range(num_timesteps):\n",
    "            arm = agent.select_arm(t)\n",
    "            reward = env.pull(arm)\n",
    "            # Instance regret: difference between best arm prob and chosen arm prob\n",
    "            regret_per_step[sim, t] = env.best_prob - env.probs[arm]\n",
    "            agent.update(arm, reward)\n",
    "    \n",
    "    # Average per-timestep regret across simulations\n",
    "    avg_regret = np.mean(regret_per_step, axis=0)\n",
    "    # Cumulative regret (averaged)\n",
    "    avg_cumulative_regret = np.cumsum(avg_regret)\n",
    "    \n",
    "    return avg_regret, avg_cumulative_regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Instance Regret for $e_p$ ($p_1=0.9, p_2=0.8, p_3=0.7$)\n",
    "\n",
    "Plot per-timestep instance regret $\\text{regret}(e_p, t)$ and cumulative instance regret\n",
    "$\\text{Regret}(e_p, T)$ for Greedy, Thompson Sampling, and Bayes-UCB agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment e_p\n",
    "probs_ep = [0.9, 0.8, 0.7]\n",
    "env_ep = BernoulliBandit(probs_ep)\n",
    "\n",
    "print(f\"Environment e_p: probabilities = {probs_ep}\")\n",
    "print(f\"Optimal arm: {env_ep.best_arm} with p* = {env_ep.best_prob}\")\n",
    "\n",
    "# Run simulations for all three agents\n",
    "agents = {\n",
    "    'Greedy': GreedyAgent,\n",
    "    'Thompson Sampling': ThompsonSamplingAgent,\n",
    "    'Bayes-UCB': BayesUCBAgent\n",
    "}\n",
    "\n",
    "results_ep = {}\n",
    "for name, agent_class in agents.items():\n",
    "    print(f\"Running {name}...\")\n",
    "    avg_regret, avg_cum_regret = run_simulation(env_ep, agent_class, NUM_SIMS, NUM_TIMESTEPS)\n",
    "    results_ep[name] = (avg_regret, avg_cum_regret)\n",
    "    print(f\"  Final cumulative regret: {avg_cum_regret[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-timestep instance regret for e_p\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = {'Greedy': 'red', 'Thompson Sampling': 'blue', 'Bayes-UCB': 'green'}\n",
    "\n",
    "# Per-timestep regret\n",
    "ax = axes[0]\n",
    "for name, (avg_regret, _) in results_ep.items():\n",
    "    ax.plot(avg_regret, label=name, color=colors[name], alpha=0.8)\n",
    "ax.set_xlabel('Timestep t')\n",
    "ax.set_ylabel('regret($e_p$, t)')\n",
    "ax.set_title('Per-Timestep Instance Regret for $e_p$')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative regret\n",
    "ax = axes[1]\n",
    "for name, (_, avg_cum_regret) in results_ep.items():\n",
    "    ax.plot(avg_cum_regret, label=name, color=colors[name], alpha=0.8)\n",
    "ax.set_xlabel('Timestep T')\n",
    "ax.set_ylabel('Regret($e_p$, T)')\n",
    "ax.set_title('Cumulative Instance Regret for $e_p$')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_regret_ep.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: part1_regret_ep.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vanishing Regret Analysis\n",
    "\n",
    "An agent has vanishing per-timestep regret if $\\text{regret}(e_p, t) \\to 0$ as $t \\to \\infty$.\n",
    "\n",
    "From the plots above:\n",
    "\n",
    "- **Thompson Sampling**: The per-timestep regret clearly converges toward zero over time. This is\n",
    "  because Thompson Sampling explores efficiently via posterior sampling — as it gathers more data,\n",
    "  the posterior concentrates around the true arm probabilities, and the agent increasingly selects\n",
    "  the optimal arm. The cumulative regret grows sub-linearly (logarithmically), confirming vanishing\n",
    "  per-timestep regret.\n",
    "\n",
    "- **Bayes-UCB**: Similarly, the per-timestep regret converges toward zero. Bayes-UCB uses\n",
    "  optimistic upper confidence bounds from the posterior quantiles. As data accumulates, these\n",
    "  quantiles tighten, and the agent reliably selects the best arm. Cumulative regret also grows\n",
    "  sub-linearly.\n",
    "\n",
    "- **Greedy**: The per-timestep regret does **not** converge to zero — it plateaus at a positive\n",
    "  value. The greedy agent can lock onto a suboptimal arm early on if its initial observations are\n",
    "  misleading. Without explicit exploration, it never recovers. The cumulative regret grows\n",
    "  **linearly**, confirming non-vanishing per-timestep regret.\n",
    "\n",
    "**Conclusion**: Thompson Sampling and Bayes-UCB have vanishing per-timestep regret. The Greedy\n",
    "agent does not, because it lacks an exploration mechanism and can permanently lock onto a\n",
    "suboptimal arm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Thompson Sampling on $e_{p'}$ ($p'_1=0.81, p'_2=0.80, p'_3=0.79$)\n",
    "\n",
    "Compare with the previous instance $e_p$ where the gaps between arms were much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment e_p'\n",
    "probs_epp = [0.81, 0.80, 0.79]\n",
    "env_epp = BernoulliBandit(probs_epp)\n",
    "\n",
    "print(f\"Environment e_p': probabilities = {probs_epp}\")\n",
    "print(f\"Optimal arm: {env_epp.best_arm} with p* = {env_epp.best_prob}\")\n",
    "\n",
    "# Run Thompson Sampling on e_p'\n",
    "print(\"Running Thompson Sampling on e_p'...\")\n",
    "avg_regret_ts_epp, avg_cum_regret_ts_epp = run_simulation(\n",
    "    env_epp, ThompsonSamplingAgent, NUM_SIMS, NUM_TIMESTEPS\n",
    ")\n",
    "print(f\"  Final cumulative regret: {avg_cum_regret_ts_epp[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Thompson Sampling on e_p vs e_p'\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Per-timestep regret comparison\n",
    "ax = axes[0]\n",
    "ax.plot(results_ep['Thompson Sampling'][0], label=\"$e_p$ (0.9, 0.8, 0.7)\", color='blue', alpha=0.8)\n",
    "ax.plot(avg_regret_ts_epp, label=\"$e_{p'}$ (0.81, 0.80, 0.79)\", color='orange', alpha=0.8)\n",
    "ax.set_xlabel('Timestep t')\n",
    "ax.set_ylabel('regret(e, t)')\n",
    "ax.set_title('Thompson Sampling: Per-Timestep Instance Regret')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative regret comparison\n",
    "ax = axes[1]\n",
    "ax.plot(results_ep['Thompson Sampling'][1], label=\"$e_p$ (0.9, 0.8, 0.7)\", color='blue', alpha=0.8)\n",
    "ax.plot(avg_cum_regret_ts_epp, label=\"$e_{p'}$ (0.81, 0.80, 0.79)\", color='orange', alpha=0.8)\n",
    "ax.set_xlabel('Timestep T')\n",
    "ax.set_ylabel('Regret(e, T)')\n",
    "ax.set_title('Thompson Sampling: Cumulative Instance Regret')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part3_thompson_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: part3_thompson_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print key comparison values\n",
    "cum_ep_500 = results_ep['Thompson Sampling'][1][499]\n",
    "cum_epp_500 = avg_cum_regret_ts_epp[499]\n",
    "cum_ep_2000 = results_ep['Thompson Sampling'][1][-1]\n",
    "cum_epp_2000 = avg_cum_regret_ts_epp[-1]\n",
    "\n",
    "print(f\"Cumulative regret at T=500:\")\n",
    "print(f\"  e_p  (0.9, 0.8, 0.7):    {cum_ep_500:.2f}\")\n",
    "print(f\"  e_p' (0.81, 0.80, 0.79): {cum_epp_500:.2f}\")\n",
    "print(f\"  e_p' {'>' if cum_epp_500 > cum_ep_500 else '<'} e_p at T=500\")\n",
    "print()\n",
    "print(f\"Cumulative regret at T=2000:\")\n",
    "print(f\"  e_p  (0.9, 0.8, 0.7):    {cum_ep_2000:.2f}\")\n",
    "print(f\"  e_p' (0.81, 0.80, 0.79): {cum_epp_2000:.2f}\")\n",
    "print(f\"  e_p' {'>' if cum_epp_2000 > cum_ep_2000 else '<'} e_p at T=2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Part 3 Analysis\n\n**Is the instance cumulative regret after 500 timesteps larger or smaller than in Part (1)?**\n\nThe cumulative regret for $e_{p'}$ at $T=500$ is **smaller** than that of $e_p$ (4.41 vs 9.06).\nThis is because the maximum per-timestep regret for $e_{p'}$ is only $0.02$ (gap between best\nand worst arm), compared to $0.2$ for $e_p$. Even though the agent makes more \"mistakes\" (pulls\nsuboptimal arms more often since they are harder to distinguish), each mistake costs much less.\n\n**Is the instance cumulative regret after 2000 timesteps larger or smaller than in Part (1)?**\n\nThe cumulative regret for $e_{p'}$ at $T=2000$ is **larger** than that of $e_p$ (15.33 vs 12.00).\nBy this time horizon, the difficulty of distinguishing close arms has caused the agent to\naccumulate enough small-regret mistakes to surpass the total regret from the easier-to-learn\ninstance $e_p$.\n\n**Why does one instance yield smaller cumulative regret in the short term but larger in the\nlong term?**\n\nInstance $e_{p'}$ has smaller gaps between arm probabilities ($\\Delta_{\\max}=0.02$) versus $e_p$\n($\\Delta_{\\max}=0.2$). This creates a fundamental tradeoff:\n\n- **Short term**: $e_{p'}$ has smaller regret because each suboptimal pull costs very little\n  (small $\\Delta$). Even though the agent explores more, the per-mistake cost is 10x smaller.\n\n- **Long term**: $e_{p'}$ accumulates **more** cumulative regret because the arms are so\n  close together that it takes much longer for the agent to identify the optimal arm with\n  confidence. The number of suboptimal pulls grows roughly as $O(1/\\Delta^2)$ (from the\n  information-theoretic lower bound), which more than compensates for the smaller per-pull cost\n  of $\\Delta$. The cumulative regret scales as $O(\\log T / \\Delta)$, so smaller gaps actually\n  lead to **larger** long-term cumulative regret.\n\nThis is the classic **exploration-exploitation tradeoff**: instances that are easy to exploit\n(large gaps) are also easy to explore (quickly distinguish arms), while instances that are hard\nto explore (small gaps) have small per-step cost but accumulate regret over many more timesteps."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}