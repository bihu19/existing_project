{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:24:21.700477Z",
     "iopub.execute_input": "2025-11-06T12:24:21.700723Z",
     "iopub.status.idle": "2025-11-06T12:24:21.980852Z",
     "shell.execute_reply.started": "2025-11-06T12:24:21.700698Z",
     "shell.execute_reply": "2025-11-06T12:24:21.980042Z"
    },
    "id": "2DNfiGClG4Ir"
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "## The CNN/DailyMail\n- The dataset includes approximately 300,000 pairs of news articles and their summaries, created from bullet points attached by CNN and DailyMail to their articles.",
   "metadata": {
    "id": "ERokaT3jG4It"
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install transformers",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:24:21.981717Z",
     "iopub.execute_input": "2025-11-06T12:24:21.982027Z",
     "iopub.status.idle": "2025-11-06T12:24:25.242964Z",
     "shell.execute_reply.started": "2025-11-06T12:24:21.982009Z",
     "shell.execute_reply": "2025-11-06T12:24:25.241926Z"
    },
    "id": "YEo7EAoIG4Iv",
    "outputId": "df2a3d7b-369a-4fea-b189-f38aa60c4b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "!pip install datasets",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:24:25.244203Z",
     "iopub.execute_input": "2025-11-06T12:24:25.244869Z",
     "iopub.status.idle": "2025-11-06T12:24:28.530786Z",
     "shell.execute_reply.started": "2025-11-06T12:24:25.244837Z",
     "shell.execute_reply": "2025-11-06T12:24:28.530012Z"
    },
    "id": "-5mX5GohG4Iv",
    "outputId": "8f7c7b0c-5acc-4a9a-e880-c545b69e4125"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.19.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "import torch\n\n# Check if PyTorch is using the GPU\nprint(\"Is CUDA available? \", torch.cuda.is_available())\nprint(\"Device name: \", torch.cuda.get_device_name(0))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:24:28.533045Z",
     "iopub.execute_input": "2025-11-06T12:24:28.533321Z",
     "iopub.status.idle": "2025-11-06T12:24:28.538417Z",
     "shell.execute_reply.started": "2025-11-06T12:24:28.533300Z",
     "shell.execute_reply": "2025-11-06T12:24:28.537484Z"
    },
    "id": "sccCVbKTG4Iw",
    "outputId": "0e7db75d-1d4d-4ef2-e562-6ea21cd123fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Is CUDA available?  True\nDevice name:  Tesla P100-PCIE-16GB\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "from datasets import load_dataset\n\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n\nprint(f\"Features: {dataset['train'].column_names}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:24:28.541266Z",
     "iopub.execute_input": "2025-11-06T12:24:28.541495Z",
     "iopub.status.idle": "2025-11-06T12:24:51.494933Z",
     "shell.execute_reply.started": "2025-11-06T12:24:28.541470Z",
     "shell.execute_reply": "2025-11-06T12:24:51.494082Z"
    },
    "id": "1eg44NYFG4Iw",
    "outputId": "100caf0c-c611-4579-c73e-b850c42c0f4b",
    "colab": {
     "referenced_widgets": [
      "5f4131accf6b46418c69afed3ce370cc",
      "6de9bd76b838441b8ea26ee3f4c0cc47",
      "0998386871da4e7bb3931af36e6ce743",
      "8edf5f7e33b246cb9203da6a131550a2",
      "f4b994b265f24c31a29ec1d7f728bee2",
      "505ff33f44d94919b5cf7ffd41f30e64",
      "74b681c536bd499d945b87553a7f12cb",
      "0b81ebada4e349cf8f8e2a40736fbc22",
      "e773e189d9d940ba95f38b7c125abe2c"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d8a421096b542899219cd42ebdc1459"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "3.0.0/train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "124100eeb5de44d6a356e3746b79f51e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "3.0.0/train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fed7a89b10f54c2dbee1ea855d554163"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "3.0.0/train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecc5bad008054182b95535b6fc8d0c40"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "3.0.0/validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06d356e8a7fd401fa1a2ddb9e7d16e6f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "3.0.0/test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb129eaf8d1049ff99376e650ae802dd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d842b79a03cb415eb9ffb25e11cbd0e5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe265faac05a4733924f31448315f048"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f217fcbd68247798f2f6cd9103c5bd8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Features: ['article', 'highlights', 'id']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "dataset",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:24:51.496669Z",
     "iopub.execute_input": "2025-11-06T12:24:51.497171Z",
     "iopub.status.idle": "2025-11-06T12:24:51.503335Z",
     "shell.execute_reply.started": "2025-11-06T12:24:51.497144Z",
     "shell.execute_reply": "2025-11-06T12:24:51.502541Z"
    },
    "id": "scLyw4dfG4Iw",
    "outputId": "9398df4a-0b6c-4f41-de17-595d40435902"
   },
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"
     },
     "metadata": {}
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "sample = dataset[\"train\"][1]\n\nprint(f\"\"\"\n\nArticle (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n\n\"\"\")\n\nprint(sample[\"article\"][:500])\n\nprint(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n\nprint(sample[\"highlights\"])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:24:51.504705Z",
     "iopub.execute_input": "2025-11-06T12:24:51.504998Z",
     "iopub.status.idle": "2025-11-06T12:24:58.274934Z",
     "shell.execute_reply.started": "2025-11-06T12:24:51.504974Z",
     "shell.execute_reply": "2025-11-06T12:24:58.273934Z"
    },
    "id": "XLjCWJzxG4Ix",
    "outputId": "3784f845-4352-4c53-e32d-0cc76018fde7"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\n\nArticle (excerpt of 500 characters, total length: 4051):\n\n\nEditor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n\nSummary (length: 281):\nMentally ill inmates in Miami are housed on the \"forgotten floor\"\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\nLeifman says the system is unjust and he's fighting for change .\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "# Text Summarization Pipelines\n# restrict the input text to 2,000 characters to have the same input for all models\nsample_text = dataset[\"train\"][1][\"article\"][:2000]\n\n# We'll collect the generated summaries of each model in a dictionary\nsummaries = {}",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:25:15.695495Z",
     "iopub.execute_input": "2025-11-06T12:25:15.696005Z",
     "iopub.status.idle": "2025-11-06T12:25:15.700146Z",
     "shell.execute_reply.started": "2025-11-06T12:25:15.695981Z",
     "shell.execute_reply": "2025-11-06T12:25:15.699435Z"
    },
    "id": "K3Dp58OEG4Ix"
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "# A convention in summarization is to separate the summary sentences by a\n# newline. Wecould add a newline token after each full stop, but this\n# simple heuristic would fail forstrings like “U.S.” or “U.N.” The Natural Language Toolkit (NLTK) package includes amore sophisticated algorithm that can\n# differentiate the end of a sentence from punctua-tion that occurs in abbreviations:\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download(\"punkt_tab\")\n\nstring = \"The U.S. are a country. The U.N. is an organization.\"\n\nsent_tokenize(string)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:26:05.142726Z",
     "iopub.execute_input": "2025-11-06T12:26:05.143604Z",
     "iopub.status.idle": "2025-11-06T12:26:06.176404Z",
     "shell.execute_reply.started": "2025-11-06T12:26:05.143570Z",
     "shell.execute_reply": "2025-11-06T12:26:06.175683Z"
    },
    "id": "hVG-7Sm9G4Ix",
    "outputId": "03fccfe9-472b-49b1-a155-aef9b5f13dd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n",
     "output_type": "stream"
    },
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['The U.S. are a country.', 'The U.N. is an organization.']"
     },
     "metadata": {}
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "# Summarization Baseline\n# (1) simply take the first three sentences of the article.\n# With NLTK’s sentence tokenizer, we can easily implement such abaseline:\ndef three_sentence_summary(text):\n    return \"\\n\".join(sent_tokenize(text)[:3])\n\nsummaries[\"baseline\"] = three_sentence_summary(sample_text)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:26:43.709405Z",
     "iopub.execute_input": "2025-11-06T12:26:43.710194Z",
     "iopub.status.idle": "2025-11-06T12:26:43.714649Z",
     "shell.execute_reply.started": "2025-11-06T12:26:43.710170Z",
     "shell.execute_reply": "2025-11-06T12:26:43.713889Z"
    },
    "id": "4ML3WFcCG4Iy"
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "# (2) load some large models like snall-t5\nfrom transformers import pipeline, set_seed\n\npipe = pipeline(\"summarization\", model=\"t5-small\")\n\npipe_out = pipe(sample_text)\n\nsummaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:27:05.922925Z",
     "iopub.execute_input": "2025-11-06T12:27:05.923249Z",
     "iopub.status.idle": "2025-11-06T12:27:36.086765Z",
     "shell.execute_reply.started": "2025-11-06T12:27:05.923225Z",
     "shell.execute_reply": "2025-11-06T12:27:36.086112Z"
    },
    "id": "ja7o95m_G4Iy",
    "outputId": "2d6ede73-85f4-4163-ce6d-85d4ee2adb91",
    "colab": {
     "referenced_widgets": [
      "d4e69d7b0d964d8aa60c5b42ce57f995",
      "1b7932a88fd546f0a5b98a7a06db86b1",
      "f6fd7dd998ce4e8da7923f6f264665e8",
      "96d14ca049024e63ab22a75bd2242158",
      "50fba300640a498381e058e573821fa1",
      "f5d531040e484c5189ee3834ed679bd7",
      "dceef3e593b04b598441ee41d0d58f0d"
     ]
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2025-11-06 12:27:12.252749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762432032.446976      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762432032.498906      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dd684ebfcbe4738ac61d102448307fe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f4ee37053af46f9905e6fc2482ac2cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c05a90e3d8748ed817cba5271bfb143"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ad23c0f74e14d4db7df82b9c53c4a79"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "280bfe294ccf46e1b318b4ae258af1b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb9b9ae049ab4e28a7a6de791413d9ad"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Device set to use cuda:0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "# Comparing with ground truth\nprint(\"GROUND TRUTH\")\nprint(dataset[\"train\"][1][\"highlights\"])\nprint(\"\")\nprint(\"T5-small\")\nprint(summaries[\"t5\"])\nprint(\"\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:27:36.087973Z",
     "iopub.execute_input": "2025-11-06T12:27:36.088636Z",
     "iopub.status.idle": "2025-11-06T12:27:36.093710Z",
     "shell.execute_reply.started": "2025-11-06T12:27:36.088616Z",
     "shell.execute_reply": "2025-11-06T12:27:36.092921Z"
    },
    "id": "9zjABpSlG4Iy",
    "outputId": "8412c3b7-b735-4a0e-98e3-2c591e55416a"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "GROUND TRUTH\nMentally ill inmates in Miami are housed on the \"forgotten floor\"\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\nLeifman says the system is unjust and he's fighting for change .\n\nT5-small\ninmates with most severe mental illnesses are incarcerated until they're ready to appear in court .\nmost often, they face drug charges or charges of assaulting an officer .\nthey end up on the ninth floor severely mentally disturbed, but not getting real help .\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "!pip install evaluate",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:27:48.708979Z",
     "iopub.execute_input": "2025-11-06T12:27:48.709556Z",
     "iopub.status.idle": "2025-11-06T12:27:52.572151Z",
     "shell.execute_reply.started": "2025-11-06T12:27:48.709533Z",
     "shell.execute_reply": "2025-11-06T12:27:52.571097Z"
    },
    "id": "mZ2nqhtUG4Iy",
    "outputId": "05284388-a14c-4f98-e263-297bf66ed0be"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.1.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.6\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "# Measuring the Quality of Generated Text with BLUE\n# from datasets import load_metric # deprecated\nimport evaluate # use this instead\nimport pandas as pd\nimport numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Define reference and prediction\nreference = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]]\n#reference = [[\"the\", \"the\", \"the\", \"the\", \"the\", \"the\"]]\nprediction = [\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]\n\n# Compute BLEU score\nsmooth_fn = SmoothingFunction().method4  # Method 4 applies a floor smoothing\n\n# Calculate BLEU score with smoothing\nscore = sentence_bleu(reference, prediction, smoothing_function=smooth_fn)\n\n# Prepare results in a similar format for display\nresults = {\n    \"bleu\": score,\n    \"precisions\": [np.round(score, 2) for _ in range(4)],  # Mock precisions as an example\n}\n\n# Format results for display\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\ndisplay_df = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])\ndisplay_df",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:28:29.256340Z",
     "iopub.execute_input": "2025-11-06T12:28:29.257167Z",
     "iopub.status.idle": "2025-11-06T12:28:29.267887Z",
     "shell.execute_reply.started": "2025-11-06T12:28:29.257140Z",
     "shell.execute_reply": "2025-11-06T12:28:29.267153Z"
    },
    "id": "ZIHPv3B1G4Iy",
    "outputId": "c219c14c-0182-4bdd-b65f-814befb9cd9d"
   },
   "outputs": [
    {
     "execution_count": 18,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           Value\nbleu                         1.0\nprecisions  [1.0, 1.0, 1.0, 1.0]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bleu</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>precisions</th>\n      <td>[1.0, 1.0, 1.0, 1.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "# Evaluating t5-small on the CNN/DailyMail Dataset\n\n# Simple version for using for testing the next block only\n\ndef evaluate_summaries_baseline_simple(dataset, metric,\n                                column_text=\"article\",  # Column containing the articles\n                                column_summary=\"highlights\"):  # Column containing the corresponding summaries\n    # Generate a list of summaries from the dataset using the three_sentence_summary function\n    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n\n   # Load the BLEU metric from the evaluate library\n    bleu_metric = evaluate.load(\"bleu\")\n\n    # Add the generated summaries and their corresponding references (true summaries) to the metric\n    bleu_metric.add_batch(predictions=summaries,  # The predicted summaries\n                          references=dataset[column_summary])  # The true summaries (ground truth)\n\n    # Compute the BLEU score\n    bleu_score = bleu_metric.compute()\n\n    # Return the BLEU score\n    return bleu_score",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:28:47.713092Z",
     "iopub.execute_input": "2025-11-06T12:28:47.713403Z",
     "iopub.status.idle": "2025-11-06T12:28:47.718374Z",
     "shell.execute_reply.started": "2025-11-06T12:28:47.713383Z",
     "shell.execute_reply": "2025-11-06T12:28:47.717590Z"
    },
    "id": "1KPRot_2G4Iy"
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "# keeping the calculations rela-tively fast, we’ll subsample the test set and run the evaluation on 1,000 samples\ntest_sampled = dataset[\"test\"].shuffle(seed=42).select(range(100))\n\n# Load the BLEU metric\nbleu_metric = evaluate.load(\"bleu\")\n\n# Evaluate using the BLEU metric\nscore = evaluate_summaries_baseline_simple(test_sampled, bleu_metric)\n\n# Format the BLEU score for display\nbleu_dict = {\"bleu\": score[\"bleu\"]}  # Extracting the BLEU score\n\n# Display the results as a DataFrame\npd.DataFrame.from_dict(bleu_dict, orient=\"index\", columns=[\"baseline\"]).T",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:28:55.417465Z",
     "iopub.execute_input": "2025-11-06T12:28:55.418015Z",
     "iopub.status.idle": "2025-11-06T12:29:00.446726Z",
     "shell.execute_reply.started": "2025-11-06T12:28:55.417990Z",
     "shell.execute_reply": "2025-11-06T12:29:00.445988Z"
    },
    "id": "Us3p12a5G4Iz",
    "outputId": "6facee5c-636c-497c-8354-384d7eaca62b"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d4804f7500c44deb27a23b79f96b670"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e26af2315c74de3905d6a830fca33ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading extra modules: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6ec21f1fca045208d20956efaa1a82a"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "             bleu\nbaseline  0.11598",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline</th>\n      <td>0.11598</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "# Import necessary libraries\nfrom tqdm import tqdm  # Progress bar for loops\nimport torch  # PyTorch library for deep learning\n\n# Set device for computation (GPU if available, otherwise CPU)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:30:13.108454Z",
     "iopub.execute_input": "2025-11-06T12:30:13.109314Z",
     "iopub.status.idle": "2025-11-06T12:30:13.116202Z",
     "shell.execute_reply.started": "2025-11-06T12:30:13.109279Z",
     "shell.execute_reply": "2025-11-06T12:30:13.115297Z"
    }
   },
   "outputs": [
    {
     "execution_count": 23,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'cuda'"
     },
     "metadata": {}
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": "from tqdm import tqdm\nimport evaluate  # For loading and calculating BLEU score\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Full version for using in this notebook\n\ndef evaluate_summaries_baseline(dataset, metric,\n                                model, tokenizer,\n                                batch_size=8,  # Add batch_size parameter for processing in batches\n                                column_text=\"article\",  # Column containing the articles\n                                column_summary=\"highlights\"):  # Column containing the corresponding summaries\n    # Chunk the dataset into batches of size `batch_size`\n    article_batches = [dataset[column_text][i:i+batch_size] for i in range(0, len(dataset[column_text]), batch_size)]\n    target_batches = [dataset[column_summary][i:i+batch_size] for i in range(0, len(dataset[column_summary]), batch_size)]\n\n    # Iterate over the batches\n    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n\n        # Tokenize the article batch for the T5 model\n        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n                           padding=\"max_length\", return_tensors=\"pt\")\n\n        # Generate summaries using the T5 model\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                                   attention_mask=inputs[\"attention_mask\"].to(device),\n                                   length_penalty=0.8, num_beams=8, max_length=128)\n\n        # Decode the generated summaries into human-readable text\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                             for s in summaries]\n\n        # Add the predicted and reference summaries to the BLEU metric\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n\n    # Compute the BLEU score after processing all batches\n    bleu_score = metric.compute()\n\n    # Return the BLEU score\n    return bleu_score\n\n# Load the T5 tokenizer and model\nmodel_ckpt = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n# Load the BLEU metric\nbleu_metric = evaluate.load(\"bleu\")\n\n# Assuming you have the `test_sampled` dataset to evaluate\nscore = evaluate_summaries_baseline(test_sampled, bleu_metric, model, tokenizer, batch_size=8)\n\n# Extract and display the BLEU score\nbleu_score = score['bleu']\nprint(f\"BLEU score: {bleu_score}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:30:18.418952Z",
     "iopub.execute_input": "2025-11-06T12:30:18.419785Z",
     "iopub.status.idle": "2025-11-06T12:31:07.410837Z",
     "shell.execute_reply.started": "2025-11-06T12:30:18.419759Z",
     "shell.execute_reply": "2025-11-06T12:31:07.409975Z"
    },
    "id": "NvSNRjKwG4Iz",
    "outputId": "41221a12-7bf8-4f55-ff3a-94bb0ee983d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 13/13 [00:45<00:00,  3.50s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "BLEU score: 0.09064421391666046\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "# Function to split the dataset into batches of a specified size\ndef chunks(list_of_elements, batch_size):\n    \"\"\"\n    Yield successive batch-sized chunks from the input list_of_elements.\n\n    Args:\n    list_of_elements (list): The list to split into chunks.\n    batch_size (int): The size of each chunk.\n\n    Yields:\n    list: A batch of elements from the input list.\n    \"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i: i + batch_size]\n\n# Function to evaluate T5-small model on the given dataset using a specified metric\ndef evaluate_summaries_t5(dataset, metric, model, tokenizer,\n                          batch_size=16, device=device,\n                          column_text=\"article\", column_summary=\"highlights\"):\n    \"\"\"\n    Evaluate the T5-small model on the dataset by comparing predicted summaries with the true summaries.\n\n    Args:\n    dataset (Dataset): The dataset containing the articles and summaries.\n    metric (Metric): The evaluation metric (e.g., BLEU, ROUGE).\n    model (Model): The pre-trained T5-small model to use for generating summaries.\n    tokenizer (Tokenizer): The tokenizer to process input and output text.\n    batch_size (int): The number of samples to process per batch.\n    device (str): The device to run the model on ('cuda' for GPU or 'cpu').\n    column_text (str): The name of the column containing the articles.\n    column_summary (str): The name of the column containing the corresponding summaries.\n\n    Returns:\n    dict: The computed score for the model's predictions using the specified metric.\n    \"\"\"\n\n    # Split the dataset into batches of articles and target summaries\n    article_batches = list(chunks(dataset[column_text], batch_size))\n    target_batches = list(chunks(dataset[column_summary], batch_size))\n\n    # Iterate over the batches of articles and target summaries\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n\n        # Tokenize the article batch and prepare input tensors\n        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n                           padding=\"max_length\", return_tensors=\"pt\")\n\n        # Generate summaries using the T5 model\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                                   attention_mask=inputs[\"attention_mask\"].to(device),\n                                   length_penalty=0.8, num_beams=8, max_length=128)\n\n        # Decode the generated summaries into human-readable text\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n                                              clean_up_tokenization_spaces=True)\n                             for s in summaries]\n\n        # Replace the placeholder token \"<n>\" with spaces in the decoded summaries\n        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n\n        # Add the predicted summaries and their corresponding true summaries to the metric\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n\n    # Compute and return the final evaluation score using the metric\n    score = metric.compute()\n    return score\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:31:14.377815Z",
     "iopub.execute_input": "2025-11-06T12:31:14.378134Z",
     "iopub.status.idle": "2025-11-06T12:31:14.385865Z",
     "shell.execute_reply.started": "2025-11-06T12:31:14.378084Z",
     "shell.execute_reply": "2025-11-06T12:31:14.385047Z"
    },
    "id": "6kmslWejG4Iz"
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": "## Training a Summarization Model\nTo train a custom text summarization model, we will use the SAMSum dataset developed by Samsung, which contains dialogues paired with brief summaries. In an enterprise context, these dialogues could represent interactions between customers and a support center. Generating accurate summaries from these interactions can enhance customer service and help identify common patterns in customer requests.",
   "metadata": {
    "id": "dxTStGr9G4Iz"
   }
  },
  {
   "cell_type": "code",
   "source": "# install the py7zr module to handle compressed .7z files\n!pip install py7zr",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:31:21.368987Z",
     "iopub.execute_input": "2025-11-06T12:31:21.369908Z",
     "iopub.status.idle": "2025-11-06T12:31:27.218316Z",
     "shell.execute_reply.started": "2025-11-06T12:31:21.369869Z",
     "shell.execute_reply": "2025-11-06T12:31:27.217336Z"
    },
    "id": "X64Muq3LG4Iz",
    "outputId": "197c8328-771a-4491-a10c-4bdf4b2a28eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Collecting py7zr\n  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: texttable in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.7.0)\nRequirement already satisfied: pycryptodomex>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (3.23.0)\nRequirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (1.1.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr) (7.1.0)\nCollecting pyzstd>=0.16.1 (from py7zr)\n  Downloading pyzstd-0.18.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\nCollecting pyppmd<1.3.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: typing-extensions>=4.13.2 in /usr/local/lib/python3.11/dist-packages (from pyzstd>=0.16.1->py7zr) (4.15.0)\nDownloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.18.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (428 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyzstd, pyppmd, pybcj, multivolumefile, inflate64, py7zr\nSuccessfully installed inflate64-1.0.3 multivolumefile-0.2.3 py7zr-1.0.0 pybcj-1.0.6 pyppmd-1.2.0 pyzstd-0.18.0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": "from datasets import load_dataset\n\ndataset_samsum = load_dataset(\"knkarthick/samsum\")\nprint(dataset_samsum)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:49:32.927780Z",
     "iopub.execute_input": "2025-11-06T12:49:32.928448Z",
     "iopub.status.idle": "2025-11-06T12:49:35.081548Z",
     "shell.execute_reply.started": "2025-11-06T12:49:32.928420Z",
     "shell.execute_reply": "2025-11-06T12:49:35.080900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14731\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n})\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "# lengths\nsplit_lengths = {name: len(ds) for name, ds in dataset_samsum.items()}\nprint(f\"Split lengths: {split_lengths}\")\n\n# columns of train\nprint(f\"Features: {dataset_samsum['train'].column_names}\")\n\n# sample from test\n# Print a sample dialogue from the 'test' split\nprint(\"\\nDialogue:\")\nprint(dataset_samsum[\"test\"][0][\"dialogue\"])\n# Print the corresponding summary for the first dialogue in the 'test' split\nprint(\"\\nSummary:\")\nprint(dataset_samsum[\"test\"][0][\"summary\"])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:50:52.212861Z",
     "iopub.execute_input": "2025-11-06T12:50:52.213187Z",
     "iopub.status.idle": "2025-11-06T12:50:52.219008Z",
     "shell.execute_reply.started": "2025-11-06T12:50:52.213166Z",
     "shell.execute_reply": "2025-11-06T12:50:52.218267Z"
    },
    "id": "2OPSPvm1G4Iz",
    "outputId": "41f86785-d598-47d9-de1d-b06a014ad74f"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Split lengths: {'train': 14731, 'validation': 818, 'test': 819}\nFeatures: ['id', 'dialogue', 'summary']\n\nDialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nSummary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": "# Run the evaluation\nscore = evaluate_summaries_t5(dataset_samsum[\"test\"], bleu_metric, model, tokenizer, column_text=\"dialogue\", column_summary=\"summary\", batch_size=8)\n\n# Extract BLEU score for display\nbleu_score = score['bleu']\n\n# Display BLEU score\nprint(f\"BLEU Score: {bleu_score}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:51:00.637582Z",
     "iopub.execute_input": "2025-11-06T12:51:00.638222Z",
     "iopub.status.idle": "2025-11-06T12:57:03.640156Z",
     "shell.execute_reply.started": "2025-11-06T12:51:00.638195Z",
     "shell.execute_reply": "2025-11-06T12:57:03.639495Z"
    },
    "id": "OM14RhvZG4Iz",
    "outputId": "766ac222-6a79-4f47-c686-76fadb5b1a1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 103/103 [06:02<00:00,  3.52s/it]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "BLEU Score: 0.029378185229893435\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\n# Calculate the token lengths for each dialogue in the training set\nd_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n\n# Calculate the token lengths for each summary in the training set\ns_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n\n# Create a figure with two subplots (side by side) to compare dialogue and summary token lengths\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n\n# Plot a histogram for the dialogue token lengths\naxes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")  # Using blue color for dialogue\naxes[0].set_title(\"Dialogue Token Length\")  # Title of the first subplot\naxes[0].set_xlabel(\"Length\")  # Label for the x-axis (length of tokens)\naxes[0].set_ylabel(\"Count\")  # Label for the y-axis (number of dialogues in each length range)\n\n# Plot a histogram for the summary token lengths\naxes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")  # Using blue color for summaries\naxes[1].set_title(\"Summary Token Length\")  # Title of the second subplot\naxes[1].set_xlabel(\"Length\")  # Label for the x-axis (length of tokens)\n\n# Adjust layout to avoid overlap and make the plots clear\nplt.tight_layout()\n\n# Display the histograms\nplt.show()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:57:34.718958Z",
     "iopub.execute_input": "2025-11-06T12:57:34.719264Z",
     "iopub.status.idle": "2025-11-06T12:57:44.914362Z",
     "shell.execute_reply.started": "2025-11-06T12:57:34.719240Z",
     "shell.execute_reply": "2025-11-06T12:57:44.913520Z"
    },
    "id": "KK_TR9zJG4Iz",
    "outputId": "bae77bb6-7da0-4ece-ef8f-feeaf1121242"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1000x350 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": "# Define a function to convert the examples in the dataset into features suitable for the model\ndef convert_examples_to_features(example_batch):\n    # Tokenize the input dialogues, limit the length to 1024 tokens, and truncate if needed\n    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024,\n                                truncation=True)\n\n    # Tokenize the target summaries, limit the length to 128 tokens, and truncate if needed\n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\n                                     truncation=True)\n\n    # Return a dictionary of the tokenized inputs and target summaries\n    return {\"input_ids\": input_encodings[\"input_ids\"],\n            \"attention_mask\": input_encodings[\"attention_mask\"],\n            \"labels\": target_encodings[\"input_ids\"]}\n\n# Apply the convert_examples_to_features function to the entire dataset using the map function\n# 'batched=True' ensures that the function processes multiple examples at once for efficiency\ndataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\n                                       batched=True)\n\n# Define the columns to keep in the dataset in the PyTorch format: input_ids, labels, and attention_mask\ncolumns = [\"input_ids\", \"labels\", \"attention_mask\"]\n\n# Convert the dataset to the PyTorch format, with the specified columns for input and target\ndataset_samsum_pt.set_format(type=\"torch\", columns=columns)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:58:00.582895Z",
     "iopub.execute_input": "2025-11-06T12:58:00.583209Z",
     "iopub.status.idle": "2025-11-06T12:58:06.492990Z",
     "shell.execute_reply.started": "2025-11-06T12:58:00.583187Z",
     "shell.execute_reply": "2025-11-06T12:58:06.492273Z"
    },
    "id": "aunU5Sl-G4I0",
    "outputId": "fb7f1006-bac6-4d56-b6f5-113593299069",
    "colab": {
     "referenced_widgets": [
      "b59bd11f69924236a9db3c158fc01063",
      "5482550ff1d349c6bb494cf15df391a5",
      "9d8aa7fafb3246acbbd709dd8690e1b6"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/14731 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d81bb99ae994616a5020f24a7aa6b6a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/818 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ec0e37a48614290926ea574a9a9739e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/819 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "328b3b0796d747e89060ac6f020ceb4f"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": "# Importing the DataCollatorForSeq2Seq class from the transformers library\nfrom transformers import DataCollatorForSeq2Seq\n\n# Initializing the data collator for sequence-to-sequence tasks like summarization or translation.\n# This collator will pad the input and target sequences and handle the batching process\n# based on the tokenizer and model you are using.\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# The DataCollatorForSeq2Seq will:\n# 1. Automatically pad the input sequences (like dialogues) and target sequences (like summaries)\n#    to the maximum length in a batch, ensuring uniformity for model processing.\n# 2. Truncate sequences that exceed the model's maximum allowable length.\n# 3. Format the data to ensure the correct structure (e.g., input IDs, attention masks, labels) for the model.\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:58:42.126511Z",
     "iopub.execute_input": "2025-11-06T12:58:42.127118Z",
     "iopub.status.idle": "2025-11-06T12:58:42.131207Z",
     "shell.execute_reply.started": "2025-11-06T12:58:42.127083Z",
     "shell.execute_reply": "2025-11-06T12:58:42.130529Z"
    },
    "id": "HPuRZt3xG4I0"
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": "### Feature Extraction",
   "metadata": {
    "id": "44rTYKpqG4I0"
   }
  },
  {
   "cell_type": "code",
   "source": "### Fill you code ###\n\n# Load a fresh T5-small model for layer-specific fine-tuning\nfrom transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n\n# Load the model\nmodel_ckpt = \"t5-small\"\nmodel_layerwise = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n# Step 1: Freeze all parameters\nfor param in model_layerwise.parameters():\n    param.requires_grad = False\n\n# Step 2: Unfreeze specific decoder layers\n# We'll unfreeze the last 3 decoder blocks (blocks 3, 4, and 5)\n# Each decoder block contains self-attention, cross-attention, and feed-forward layers\ndecoder_blocks_to_unfreeze = [3, 4, 5]\n\nfor block_idx in decoder_blocks_to_unfreeze:\n    for param in model_layerwise.decoder.block[block_idx].parameters():\n        param.requires_grad = True\n\n# Also unfreeze the final layer norm of the decoder\nfor param in model_layerwise.decoder.final_layer_norm.parameters():\n    param.requires_grad = True\n\n# Step 3: Verify which parameters are trainable\nprint(\"Trainable parameters:\")\ntrainable_params = 0\nall_params = 0\nfor name, param in model_layerwise.named_parameters():\n    all_params += param.numel()\n    if param.requires_grad:\n        trainable_params += param.numel()\n        print(f\"  {name}\")\n\nprint(f\"\\nTrainable params: {trainable_params:,} || All params: {all_params:,} || Trainable %: {100 * trainable_params / all_params:.2f}%\")\n\n# Step 4: Define training arguments for layer-specific fine-tuning\ntraining_args_layerwise = TrainingArguments(\n    output_dir='t5-layerwise',  # Directory to save model checkpoints\n    num_train_epochs=10,  # Number of training epochs\n    warmup_steps=500,  # Warmup steps before full learning rate\n    per_device_train_batch_size=1,  # Batch size per device during training\n    per_device_eval_batch_size=1,  # Batch size per device during evaluation\n    weight_decay=0.01,  # Weight decay for regularization\n    logging_steps=10,  # Log every 10 steps\n    push_to_hub=False,  # Don't push to Hugging Face Hub\n    evaluation_strategy='steps',  # Evaluate during training\n    eval_steps=500,  # Evaluate every 500 steps\n    save_steps=1e6,  # Save checkpoint every 1M steps (effectively at end)\n    gradient_accumulation_steps=16,  # Accumulate gradients over 16 steps\n    report_to=\"none\"  # Disable wandb logging\n)\n\n# Step 5: Initialize the Trainer\ntrainer_layerwise = Trainer(\n    model=model_layerwise,  # The model with frozen/unfrozen layers\n    args=training_args_layerwise,  # Training arguments\n    tokenizer=tokenizer,  # Tokenizer\n    data_collator=seq2seq_data_collator,  # Data collator for padding\n    train_dataset=dataset_samsum_pt[\"train\"],  # Training dataset\n    eval_dataset=dataset_samsum_pt[\"validation\"]  # Validation dataset\n)\n\n# Step 6: Train the model\nprint(\"\\n\" + \"=\"*50)\nprint(\"Starting training with layer-specific fine-tuning...\")\nprint(\"=\"*50 + \"\\n\")\ntrainer_layerwise.train()",
   "metadata": {
    "trusted": true,
    "id": "YeA8sQdYG4I0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# measure blue score\n\n# Reload the BLEU metric for evaluation\nbleu_metric = evaluate.load(\"bleu\")\n\n# Evaluate the fine-tuned model on the test set\nprint(\"Evaluating the layer-specific fine-tuned model...\")\nscore_layerwise = evaluate_summaries_t5(\n    dataset_samsum[\"test\"], \n    bleu_metric, \n    trainer_layerwise.model,  # Use the trained model from trainer\n    tokenizer,\n    batch_size=8,  # Batch size for evaluation\n    column_text=\"dialogue\",  # Column name for input dialogues\n    column_summary=\"summary\"  # Column name for reference summaries\n)\n\n# Extract the BLEU score\nbleu_score_layerwise = score_layerwise[\"bleu\"]\n\n# Create a DataFrame to display the results\nprint(\"\\n\" + \"=\"*50)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"BLEU Score (Layer-specific fine-tuning): {bleu_score_layerwise:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\n# Create a comparison dataframe\nimport pandas as pd\nresults_df = pd.DataFrame({\n    \"Model\": [\"T5-small (Layer-specific)\"],\n    \"BLEU\": [bleu_score_layerwise]\n})\ndisplay(results_df)",
   "metadata": {
    "trusted": true,
    "id": "bqHxvtSdG4I0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#  generating Dialogue Summarie\n\n# Import necessary modules\nimport transformers\nfrom transformers import pipeline\n\n# Set logging to error only to suppress warnings\ntransformers.logging.set_verbosity_error()\n\n# Define generation parameters\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\": 8, \"max_length\": 128}\n\n# Create a summarization pipeline with the fine-tuned model\npipe_layerwise = pipeline(\"summarization\", model=trainer_layerwise.model, tokenizer=tokenizer)\n\n# Test on the first example from the test set\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\nreference = dataset_samsum[\"test\"][0][\"summary\"]\n\nprint(\"=\"*70)\nprint(\"DIALOGUE SUMMARY GENERATION - Layer-Specific Fine-tuned Model\")\nprint(\"=\"*70)\nprint(\"\\nDialogue:\")\nprint(sample_text)\nprint(\"\\nReference Summary:\")\nprint(reference)\nprint(\"\\nModel Summary (Layer-specific fine-tuning):\")\nprint(pipe_layerwise(sample_text, **gen_kwargs)[0][\"summary_text\"])\nprint(\"=\"*70)\n\n# Test on additional examples from the test set\nprint(\"\\n\" + \"=\"*70)\nprint(\"ADDITIONAL TEST EXAMPLES\")\nprint(\"=\"*70)\n\nfor i in range(1, 4):  # Test on 3 more examples\n    sample_text = dataset_samsum[\"test\"][i][\"dialogue\"]\n    reference = dataset_samsum[\"test\"][i][\"summary\"]\n    \n    print(f\"\\n--- Example {i+1} ---\")\n    print(\"\\nDialogue:\")\n    print(sample_text[:300] + \"...\" if len(sample_text) > 300 else sample_text)\n    print(\"\\nReference Summary:\")\n    print(reference)\n    print(\"\\nModel Summary (Layer-specific fine-tuning):\")\n    print(pipe_layerwise(sample_text, **gen_kwargs)[0][\"summary_text\"])\n    print(\"-\" * 70)\n\n# Test on a custom dialogue\nprint(\"\\n\" + \"=\"*70)\nprint(\"CUSTOM DIALOGUE TEST\")\nprint(\"=\"*70)\n\ncustom_dialogue = \"\"\"\\\nThom: Hi guys, have you heard of transformers?\nLewis: Yes, I used them recently!\nLeandro: Indeed, there is a great library by Hugging Face.\nThom: I know, I helped build it ;)\nLewis: Cool, maybe we should write a book about it. What do you think?\nLeandro: Great idea, how hard can it be?!\nThom: I am in!\nLewis: Awesome, let's do it together!\n\"\"\"\n\nprint(\"\\nCustom Dialogue:\")\nprint(custom_dialogue)\nprint(\"\\nModel Summary (Layer-specific fine-tuning):\")\nprint(pipe_layerwise(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])\nprint(\"=\"*70)",
   "metadata": {
    "trusted": true,
    "id": "gZbh0LB4G4I0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Fine Tuning (นิสิตไม่ต้องทำอะไรต่อจากนี้ ให้ไว้เพื่อดู code เป็นตัวอย่าง)",
   "metadata": {
    "id": "K4n8UcBQG4I0"
   }
  },
  {
   "cell_type": "code",
   "source": "# basicly you dont have to use this block if not publish your work to the hub\n# otherwise if you are required to have some password in your code you can use this code block if needs\n#import os\n\n# Set your Hugging Face token\n#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your token\"\n# Verify that the token is set\n#print(\"Hugging Face Token:\", os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") is not None)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-09T17:21:33.835670Z",
     "iopub.execute_input": "2024-11-09T17:21:33.836380Z",
     "iopub.status.idle": "2024-11-09T17:21:33.841804Z",
     "shell.execute_reply.started": "2024-11-09T17:21:33.836308Z",
     "shell.execute_reply": "2024-11-09T17:21:33.840814Z"
    },
    "id": "LNVw0PRPG4I0",
    "outputId": "d0e2b693-5daf-4bd2-a6b5-0f107b57279a"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Hugging Face Token: True\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# basicly you dont have to track your work for logging\n# otherwise if your code cannot be run, feel free to use this bloxk for diable this loggging if needs\n#import os\n\n# Disable Weights & Biases\n#os.environ['WANDB_DISABLED'] = 'true'",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-09T17:26:57.971277Z",
     "iopub.execute_input": "2024-11-09T17:26:57.971664Z",
     "iopub.status.idle": "2024-11-09T17:26:57.978186Z",
     "shell.execute_reply.started": "2024-11-09T17:26:57.971630Z",
     "shell.execute_reply": "2024-11-09T17:26:57.977364Z"
    },
    "id": "_mm_8Nv1G4I0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Check if the parameters are frozen correctly\nfor name, param in model.named_parameters():\n    print(f\"{name}: requires_grad={param.requires_grad}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T12:59:25.945576Z",
     "iopub.execute_input": "2025-11-06T12:59:25.945848Z",
     "iopub.status.idle": "2025-11-06T12:59:25.952882Z",
     "shell.execute_reply.started": "2025-11-06T12:59:25.945829Z",
     "shell.execute_reply": "2025-11-06T12:59:25.951999Z"
    },
    "id": "tAHJzoIjG4I0",
    "outputId": "db42f1af-27ac-47dc-d335-ed73a1cdbaa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "shared.weight: requires_grad=True\nencoder.block.0.layer.0.SelfAttention.q.weight: requires_grad=True\nencoder.block.0.layer.0.SelfAttention.k.weight: requires_grad=True\nencoder.block.0.layer.0.SelfAttention.v.weight: requires_grad=True\nencoder.block.0.layer.0.SelfAttention.o.weight: requires_grad=True\nencoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: requires_grad=True\nencoder.block.0.layer.0.layer_norm.weight: requires_grad=True\nencoder.block.0.layer.1.DenseReluDense.wi.weight: requires_grad=True\nencoder.block.0.layer.1.DenseReluDense.wo.weight: requires_grad=True\nencoder.block.0.layer.1.layer_norm.weight: requires_grad=True\nencoder.block.1.layer.0.SelfAttention.q.weight: requires_grad=True\nencoder.block.1.layer.0.SelfAttention.k.weight: requires_grad=True\nencoder.block.1.layer.0.SelfAttention.v.weight: requires_grad=True\nencoder.block.1.layer.0.SelfAttention.o.weight: requires_grad=True\nencoder.block.1.layer.0.layer_norm.weight: requires_grad=True\nencoder.block.1.layer.1.DenseReluDense.wi.weight: requires_grad=True\nencoder.block.1.layer.1.DenseReluDense.wo.weight: requires_grad=True\nencoder.block.1.layer.1.layer_norm.weight: requires_grad=True\nencoder.block.2.layer.0.SelfAttention.q.weight: requires_grad=True\nencoder.block.2.layer.0.SelfAttention.k.weight: requires_grad=True\nencoder.block.2.layer.0.SelfAttention.v.weight: requires_grad=True\nencoder.block.2.layer.0.SelfAttention.o.weight: requires_grad=True\nencoder.block.2.layer.0.layer_norm.weight: requires_grad=True\nencoder.block.2.layer.1.DenseReluDense.wi.weight: requires_grad=True\nencoder.block.2.layer.1.DenseReluDense.wo.weight: requires_grad=True\nencoder.block.2.layer.1.layer_norm.weight: requires_grad=True\nencoder.block.3.layer.0.SelfAttention.q.weight: requires_grad=True\nencoder.block.3.layer.0.SelfAttention.k.weight: requires_grad=True\nencoder.block.3.layer.0.SelfAttention.v.weight: requires_grad=True\nencoder.block.3.layer.0.SelfAttention.o.weight: requires_grad=True\nencoder.block.3.layer.0.layer_norm.weight: requires_grad=True\nencoder.block.3.layer.1.DenseReluDense.wi.weight: requires_grad=True\nencoder.block.3.layer.1.DenseReluDense.wo.weight: requires_grad=True\nencoder.block.3.layer.1.layer_norm.weight: requires_grad=True\nencoder.block.4.layer.0.SelfAttention.q.weight: requires_grad=True\nencoder.block.4.layer.0.SelfAttention.k.weight: requires_grad=True\nencoder.block.4.layer.0.SelfAttention.v.weight: requires_grad=True\nencoder.block.4.layer.0.SelfAttention.o.weight: requires_grad=True\nencoder.block.4.layer.0.layer_norm.weight: requires_grad=True\nencoder.block.4.layer.1.DenseReluDense.wi.weight: requires_grad=True\nencoder.block.4.layer.1.DenseReluDense.wo.weight: requires_grad=True\nencoder.block.4.layer.1.layer_norm.weight: requires_grad=True\nencoder.block.5.layer.0.SelfAttention.q.weight: requires_grad=True\nencoder.block.5.layer.0.SelfAttention.k.weight: requires_grad=True\nencoder.block.5.layer.0.SelfAttention.v.weight: requires_grad=True\nencoder.block.5.layer.0.SelfAttention.o.weight: requires_grad=True\nencoder.block.5.layer.0.layer_norm.weight: requires_grad=True\nencoder.block.5.layer.1.DenseReluDense.wi.weight: requires_grad=True\nencoder.block.5.layer.1.DenseReluDense.wo.weight: requires_grad=True\nencoder.block.5.layer.1.layer_norm.weight: requires_grad=True\nencoder.final_layer_norm.weight: requires_grad=True\ndecoder.block.0.layer.0.SelfAttention.q.weight: requires_grad=True\ndecoder.block.0.layer.0.SelfAttention.k.weight: requires_grad=True\ndecoder.block.0.layer.0.SelfAttention.v.weight: requires_grad=True\ndecoder.block.0.layer.0.SelfAttention.o.weight: requires_grad=True\ndecoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: requires_grad=True\ndecoder.block.0.layer.0.layer_norm.weight: requires_grad=True\ndecoder.block.0.layer.1.EncDecAttention.q.weight: requires_grad=True\ndecoder.block.0.layer.1.EncDecAttention.k.weight: requires_grad=True\ndecoder.block.0.layer.1.EncDecAttention.v.weight: requires_grad=True\ndecoder.block.0.layer.1.EncDecAttention.o.weight: requires_grad=True\ndecoder.block.0.layer.1.layer_norm.weight: requires_grad=True\ndecoder.block.0.layer.2.DenseReluDense.wi.weight: requires_grad=True\ndecoder.block.0.layer.2.DenseReluDense.wo.weight: requires_grad=True\ndecoder.block.0.layer.2.layer_norm.weight: requires_grad=True\ndecoder.block.1.layer.0.SelfAttention.q.weight: requires_grad=True\ndecoder.block.1.layer.0.SelfAttention.k.weight: requires_grad=True\ndecoder.block.1.layer.0.SelfAttention.v.weight: requires_grad=True\ndecoder.block.1.layer.0.SelfAttention.o.weight: requires_grad=True\ndecoder.block.1.layer.0.layer_norm.weight: requires_grad=True\ndecoder.block.1.layer.1.EncDecAttention.q.weight: requires_grad=True\ndecoder.block.1.layer.1.EncDecAttention.k.weight: requires_grad=True\ndecoder.block.1.layer.1.EncDecAttention.v.weight: requires_grad=True\ndecoder.block.1.layer.1.EncDecAttention.o.weight: requires_grad=True\ndecoder.block.1.layer.1.layer_norm.weight: requires_grad=True\ndecoder.block.1.layer.2.DenseReluDense.wi.weight: requires_grad=True\ndecoder.block.1.layer.2.DenseReluDense.wo.weight: requires_grad=True\ndecoder.block.1.layer.2.layer_norm.weight: requires_grad=True\ndecoder.block.2.layer.0.SelfAttention.q.weight: requires_grad=True\ndecoder.block.2.layer.0.SelfAttention.k.weight: requires_grad=True\ndecoder.block.2.layer.0.SelfAttention.v.weight: requires_grad=True\ndecoder.block.2.layer.0.SelfAttention.o.weight: requires_grad=True\ndecoder.block.2.layer.0.layer_norm.weight: requires_grad=True\ndecoder.block.2.layer.1.EncDecAttention.q.weight: requires_grad=True\ndecoder.block.2.layer.1.EncDecAttention.k.weight: requires_grad=True\ndecoder.block.2.layer.1.EncDecAttention.v.weight: requires_grad=True\ndecoder.block.2.layer.1.EncDecAttention.o.weight: requires_grad=True\ndecoder.block.2.layer.1.layer_norm.weight: requires_grad=True\ndecoder.block.2.layer.2.DenseReluDense.wi.weight: requires_grad=True\ndecoder.block.2.layer.2.DenseReluDense.wo.weight: requires_grad=True\ndecoder.block.2.layer.2.layer_norm.weight: requires_grad=True\ndecoder.block.3.layer.0.SelfAttention.q.weight: requires_grad=True\ndecoder.block.3.layer.0.SelfAttention.k.weight: requires_grad=True\ndecoder.block.3.layer.0.SelfAttention.v.weight: requires_grad=True\ndecoder.block.3.layer.0.SelfAttention.o.weight: requires_grad=True\ndecoder.block.3.layer.0.layer_norm.weight: requires_grad=True\ndecoder.block.3.layer.1.EncDecAttention.q.weight: requires_grad=True\ndecoder.block.3.layer.1.EncDecAttention.k.weight: requires_grad=True\ndecoder.block.3.layer.1.EncDecAttention.v.weight: requires_grad=True\ndecoder.block.3.layer.1.EncDecAttention.o.weight: requires_grad=True\ndecoder.block.3.layer.1.layer_norm.weight: requires_grad=True\ndecoder.block.3.layer.2.DenseReluDense.wi.weight: requires_grad=True\ndecoder.block.3.layer.2.DenseReluDense.wo.weight: requires_grad=True\ndecoder.block.3.layer.2.layer_norm.weight: requires_grad=True\ndecoder.block.4.layer.0.SelfAttention.q.weight: requires_grad=True\ndecoder.block.4.layer.0.SelfAttention.k.weight: requires_grad=True\ndecoder.block.4.layer.0.SelfAttention.v.weight: requires_grad=True\ndecoder.block.4.layer.0.SelfAttention.o.weight: requires_grad=True\ndecoder.block.4.layer.0.layer_norm.weight: requires_grad=True\ndecoder.block.4.layer.1.EncDecAttention.q.weight: requires_grad=True\ndecoder.block.4.layer.1.EncDecAttention.k.weight: requires_grad=True\ndecoder.block.4.layer.1.EncDecAttention.v.weight: requires_grad=True\ndecoder.block.4.layer.1.EncDecAttention.o.weight: requires_grad=True\ndecoder.block.4.layer.1.layer_norm.weight: requires_grad=True\ndecoder.block.4.layer.2.DenseReluDense.wi.weight: requires_grad=True\ndecoder.block.4.layer.2.DenseReluDense.wo.weight: requires_grad=True\ndecoder.block.4.layer.2.layer_norm.weight: requires_grad=True\ndecoder.block.5.layer.0.SelfAttention.q.weight: requires_grad=True\ndecoder.block.5.layer.0.SelfAttention.k.weight: requires_grad=True\ndecoder.block.5.layer.0.SelfAttention.v.weight: requires_grad=True\ndecoder.block.5.layer.0.SelfAttention.o.weight: requires_grad=True\ndecoder.block.5.layer.0.layer_norm.weight: requires_grad=True\ndecoder.block.5.layer.1.EncDecAttention.q.weight: requires_grad=True\ndecoder.block.5.layer.1.EncDecAttention.k.weight: requires_grad=True\ndecoder.block.5.layer.1.EncDecAttention.v.weight: requires_grad=True\ndecoder.block.5.layer.1.EncDecAttention.o.weight: requires_grad=True\ndecoder.block.5.layer.1.layer_norm.weight: requires_grad=True\ndecoder.block.5.layer.2.DenseReluDense.wi.weight: requires_grad=True\ndecoder.block.5.layer.2.DenseReluDense.wo.weight: requires_grad=True\ndecoder.block.5.layer.2.layer_norm.weight: requires_grad=True\ndecoder.final_layer_norm.weight: requires_grad=True\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": "from transformers import TrainingArguments, Trainer\n\n\n# Defining the training arguments for model fine-tuning\ntraining_args = TrainingArguments(\n    output_dir='t5',  # Directory where the model and training logs will be saved\n    num_train_epochs=10,  # Number of epochs to train the model\n    warmup_steps=500,  # Number of warmup steps to perform before starting the actual training\n    per_device_train_batch_size=1,  # Batch size for training (number of samples per batch during training)\n    per_device_eval_batch_size=1,  # Batch size for evaluation (number of samples per batch during evaluation)\n    weight_decay=0.01,  # Weight decay for regularization, helps prevent overfitting\n    logging_steps=10,  # Logging frequency (steps at which to log training progress)\n    push_to_hub=False,  # Disable pushing the model to the Hugging Face Hub after training\n    #evaluation_strategy='steps',  # Define when to evaluate the model during training (evaluate every 'steps' steps)\n    eval_steps=500,  # Number of steps between evaluations\n    save_steps=1e6,  # Save model after every 1 million steps (or you can adjust this)\n    gradient_accumulation_steps=16,  # Gradient accumulation steps for larger batch sizes, reduces memory usage during training\n    report_to=\"none\"  # Disables logging to WandB and other integrations\n)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:02:27.307017Z",
     "iopub.execute_input": "2025-11-06T13:02:27.307323Z",
     "iopub.status.idle": "2025-11-06T13:02:27.336958Z",
     "shell.execute_reply.started": "2025-11-06T13:02:27.307304Z",
     "shell.execute_reply": "2025-11-06T13:02:27.336148Z"
    },
    "id": "8iO2vbPFG4I0",
    "outputId": "92fb8f15-1ab9-4177-84eb-4cc9c727bcab"
   },
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "source": "# Initializing the Trainer class to handle the training and evaluation process\ntrainer = Trainer(\n    model=model,  # The pre-trained model to be fine-tuned\n    args=training_args,  # The training arguments, including batch size, number of epochs, etc.\n    tokenizer=tokenizer,  # The tokenizer to encode the input text for the model\n    data_collator=seq2seq_data_collator,  # Data collator used to pad sequences dynamically during training\n    train_dataset=dataset_samsum_pt[\"train\"],  # The training dataset to be used for training\n    eval_dataset=dataset_samsum_pt[\"validation\"]  # The validation dataset to be used for evaluation\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-06T13:02:40.829179Z",
     "iopub.execute_input": "2025-11-06T13:02:40.829703Z",
     "iopub.status.idle": "2025-11-06T13:02:40.841671Z",
     "shell.execute_reply.started": "2025-11-06T13:02:40.829684Z",
     "shell.execute_reply": "2025-11-06T13:02:40.840769Z"
    },
    "id": "lNF8DKYEG4I1"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_37/3311845328.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "source": "# Train the model\ntrainer.train()\n\n# Evaluate the model's performance using the T5 model and BLEU metric\nscore = evaluate_summaries_t5(\n    dataset_samsum[\"test\"], bleu_metric, trainer.model, tokenizer,\n    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\"\n)\n\n# Extract the BLEU score\nbleu_score = score[\"bleu\"]\n\n# Create a dictionary to store the BLEU score for T5\nbleu_dict = {\"BLEU\": bleu_score}\n\n# Convert the score to a pandas DataFrame for better visualization\nimport pandas as pd\npd.DataFrame(bleu_dict, index=[f\"T5\"])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-09T18:40:10.587840Z",
     "iopub.execute_input": "2024-11-09T18:40:10.588287Z",
     "iopub.status.idle": "2024-11-09T20:42:16.419998Z",
     "shell.execute_reply.started": "2024-11-09T18:40:10.588245Z",
     "shell.execute_reply": "2024-11-09T20:42:16.419061Z"
    },
    "id": "fZPneTP9G4I1",
    "outputId": "065dac2c-4c42-469c-ccf3-9d2acccc516e"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "{'loss': 2.0541, 'grad_norm': 2.914802312850952, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.010860711376595167}\n{'loss': 2.0893, 'grad_norm': 5.96926212310791, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.021721422753190334}\n{'loss': 2.0626, 'grad_norm': 2.1943347454071045, 'learning_rate': 3e-06, 'epoch': 0.0325821341297855}\n{'loss': 2.0248, 'grad_norm': 2.605330467224121, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.04344284550638067}\n{'loss': 1.9349, 'grad_norm': 2.9679934978485107, 'learning_rate': 5e-06, 'epoch': 0.054303556882975834}\n{'loss': 1.9922, 'grad_norm': 3.1404731273651123, 'learning_rate': 6e-06, 'epoch': 0.065164268259571}\n{'loss': 2.0248, 'grad_norm': 3.851630687713623, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.07602497963616617}\n{'loss': 1.9781, 'grad_norm': 2.870978593826294, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.08688569101276133}\n{'loss': 2.0196, 'grad_norm': 2.190049171447754, 'learning_rate': 9e-06, 'epoch': 0.0977464023893565}\n{'loss': 2.0288, 'grad_norm': 2.1035003662109375, 'learning_rate': 1e-05, 'epoch': 0.10860711376595167}\n{'loss': 1.9519, 'grad_norm': 2.6851887702941895, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.11946782514254684}\n{'loss': 1.9111, 'grad_norm': 2.430756092071533, 'learning_rate': 1.2e-05, 'epoch': 0.130328536519142}\n{'loss': 1.9898, 'grad_norm': 2.4656524658203125, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.14118924789573717}\n{'loss': 1.9686, 'grad_norm': 4.435159683227539, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.15204995927233234}\n{'loss': 1.9622, 'grad_norm': 3.1294727325439453, 'learning_rate': 1.5e-05, 'epoch': 0.1629106706489275}\n{'loss': 1.9908, 'grad_norm': 2.3783490657806396, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.17377138202552267}\n{'loss': 2.0232, 'grad_norm': 2.57096266746521, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.18463209340211784}\n{'loss': 1.9626, 'grad_norm': 2.1775624752044678, 'learning_rate': 1.8e-05, 'epoch': 0.195492804778713}\n{'loss': 1.9323, 'grad_norm': 2.5000996589660645, 'learning_rate': 1.9e-05, 'epoch': 0.20635351615530817}\n{'loss': 1.9964, 'grad_norm': 2.0404648780822754, 'learning_rate': 2e-05, 'epoch': 0.21721422753190334}\n{'loss': 1.9535, 'grad_norm': 5.197328090667725, 'learning_rate': 2.1e-05, 'epoch': 0.2280749389084985}\n{'loss': 1.8448, 'grad_norm': 2.33958101272583, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.23893565028509367}\n{'loss': 2.0299, 'grad_norm': 1.9934524297714233, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.24979636166168884}\n{'loss': 1.9763, 'grad_norm': 2.482975959777832, 'learning_rate': 2.4e-05, 'epoch': 0.260657073038284}\n{'loss': 1.9567, 'grad_norm': 2.8965673446655273, 'learning_rate': 2.5e-05, 'epoch': 0.27151778441487917}\n{'loss': 1.9881, 'grad_norm': 2.1063549518585205, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.28237849579147434}\n{'loss': 1.8651, 'grad_norm': 2.5068271160125732, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.2932392071680695}\n{'loss': 1.8669, 'grad_norm': 2.0783495903015137, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.30409991854466467}\n{'loss': 1.9444, 'grad_norm': 2.3580236434936523, 'learning_rate': 2.9e-05, 'epoch': 0.31496062992125984}\n{'loss': 1.8957, 'grad_norm': 1.9151312112808228, 'learning_rate': 3e-05, 'epoch': 0.325821341297855}\n{'loss': 1.975, 'grad_norm': 2.411365509033203, 'learning_rate': 3.1e-05, 'epoch': 0.33668205267445017}\n{'loss': 2.0615, 'grad_norm': 3.708738088607788, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.34754276405104534}\n{'loss': 1.976, 'grad_norm': 3.257444143295288, 'learning_rate': 3.3e-05, 'epoch': 0.3584034754276405}\n{'loss': 1.9323, 'grad_norm': 2.879213809967041, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.3692641868042357}\n{'loss': 1.9158, 'grad_norm': 2.202707529067993, 'learning_rate': 3.5e-05, 'epoch': 0.38012489818083084}\n{'loss': 1.853, 'grad_norm': 4.0245890617370605, 'learning_rate': 3.6e-05, 'epoch': 0.390985609557426}\n{'loss': 1.872, 'grad_norm': 3.1874823570251465, 'learning_rate': 3.7e-05, 'epoch': 0.4018463209340212}\n{'loss': 1.931, 'grad_norm': 3.580278158187866, 'learning_rate': 3.8e-05, 'epoch': 0.41270703231061634}\n{'loss': 1.9702, 'grad_norm': 22.307920455932617, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.4235677436872115}\n{'loss': 1.9276, 'grad_norm': 3.0936691761016846, 'learning_rate': 4e-05, 'epoch': 0.4344284550638067}\n{'loss': 1.8844, 'grad_norm': 3.4383974075317383, 'learning_rate': 4.1e-05, 'epoch': 0.44528916644040184}\n{'loss': 1.8457, 'grad_norm': 2.5830020904541016, 'learning_rate': 4.2e-05, 'epoch': 0.456149877816997}\n{'loss': 1.9297, 'grad_norm': 3.2107462882995605, 'learning_rate': 4.3e-05, 'epoch': 0.4670105891935922}\n{'loss': 1.9467, 'grad_norm': 2.8771119117736816, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.47787130057018734}\n{'loss': 1.8409, 'grad_norm': 3.118415117263794, 'learning_rate': 4.5e-05, 'epoch': 0.4887320119467825}\n{'loss': 1.9114, 'grad_norm': 2.076026678085327, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.4995927233233777}\n{'loss': 1.9076, 'grad_norm': 2.227616310119629, 'learning_rate': 4.7e-05, 'epoch': 0.5104534346999728}\n{'loss': 1.8697, 'grad_norm': 2.900143623352051, 'learning_rate': 4.8e-05, 'epoch': 0.521314146076568}\n{'loss': 1.8852, 'grad_norm': 2.5515689849853516, 'learning_rate': 4.9e-05, 'epoch': 0.5321748574531632}\n{'loss': 1.8987, 'grad_norm': 8.821070671081543, 'learning_rate': 5e-05, 'epoch': 0.5430355688297583}\n{'eval_loss': 1.7778496742248535, 'eval_runtime': 12.5567, 'eval_samples_per_second': 65.145, 'eval_steps_per_second': 65.145, 'epoch': 0.5430355688297583}\n{'loss': 1.9197, 'grad_norm': 2.3980836868286133, 'learning_rate': 4.9942528735632185e-05, 'epoch': 0.5538962802063535}\n{'loss': 1.8467, 'grad_norm': 2.574667453765869, 'learning_rate': 4.988505747126437e-05, 'epoch': 0.5647569915829487}\n{'loss': 1.8734, 'grad_norm': 2.772669792175293, 'learning_rate': 4.982758620689655e-05, 'epoch': 0.5756177029595438}\n{'loss': 1.7925, 'grad_norm': 2.6186015605926514, 'learning_rate': 4.977011494252874e-05, 'epoch': 0.586478414336139}\n{'loss': 1.9487, 'grad_norm': 3.4988200664520264, 'learning_rate': 4.971264367816092e-05, 'epoch': 0.5973391257127342}\n{'loss': 1.9609, 'grad_norm': 2.7244484424591064, 'learning_rate': 4.9655172413793107e-05, 'epoch': 0.6081998370893293}\n{'loss': 1.9967, 'grad_norm': 5.907276153564453, 'learning_rate': 4.959770114942529e-05, 'epoch': 0.6190605484659245}\n{'loss': 1.8879, 'grad_norm': 2.5775270462036133, 'learning_rate': 4.954022988505747e-05, 'epoch': 0.6299212598425197}\n{'loss': 1.7904, 'grad_norm': 2.98748517036438, 'learning_rate': 4.9482758620689655e-05, 'epoch': 0.6407819712191148}\n{'loss': 2.0041, 'grad_norm': 2.397435426712036, 'learning_rate': 4.9425287356321845e-05, 'epoch': 0.65164268259571}\n{'loss': 1.8645, 'grad_norm': 2.3095080852508545, 'learning_rate': 4.936781609195403e-05, 'epoch': 0.6625033939723052}\n{'loss': 1.9206, 'grad_norm': 2.1395630836486816, 'learning_rate': 4.931034482758621e-05, 'epoch': 0.6733641053489003}\n{'loss': 1.9504, 'grad_norm': 2.6711418628692627, 'learning_rate': 4.9252873563218394e-05, 'epoch': 0.6842248167254955}\n{'loss': 1.8769, 'grad_norm': 3.0314674377441406, 'learning_rate': 4.9195402298850577e-05, 'epoch': 0.6950855281020907}\n{'loss': 1.816, 'grad_norm': 2.3200294971466064, 'learning_rate': 4.913793103448276e-05, 'epoch': 0.7059462394786858}\n{'loss': 1.8736, 'grad_norm': 4.636402606964111, 'learning_rate': 4.908045977011494e-05, 'epoch': 0.716806950855281}\n{'loss': 1.9016, 'grad_norm': 3.708186388015747, 'learning_rate': 4.902298850574713e-05, 'epoch': 0.7276676622318762}\n{'loss': 1.8768, 'grad_norm': 3.3550827503204346, 'learning_rate': 4.896551724137931e-05, 'epoch': 0.7385283736084713}\n{'loss': 1.8288, 'grad_norm': 2.357234477996826, 'learning_rate': 4.89080459770115e-05, 'epoch': 0.7493890849850665}\n{'loss': 1.9801, 'grad_norm': 2.6592631340026855, 'learning_rate': 4.885057471264368e-05, 'epoch': 0.7602497963616617}\n{'loss': 1.9177, 'grad_norm': 2.3912062644958496, 'learning_rate': 4.8793103448275864e-05, 'epoch': 0.7711105077382568}\n{'loss': 1.8905, 'grad_norm': 3.160606861114502, 'learning_rate': 4.8735632183908047e-05, 'epoch': 0.781971219114852}\n{'loss': 1.8451, 'grad_norm': 3.6864962577819824, 'learning_rate': 4.8678160919540236e-05, 'epoch': 0.7928319304914472}\n{'loss': 2.0159, 'grad_norm': 1.8313910961151123, 'learning_rate': 4.862068965517241e-05, 'epoch': 0.8036926418680423}\n{'loss': 1.8804, 'grad_norm': 2.39848256111145, 'learning_rate': 4.85632183908046e-05, 'epoch': 0.8145533532446375}\n{'loss': 1.9332, 'grad_norm': 2.9691965579986572, 'learning_rate': 4.8505747126436785e-05, 'epoch': 0.8254140646212327}\n{'loss': 1.8323, 'grad_norm': 2.7936177253723145, 'learning_rate': 4.844827586206897e-05, 'epoch': 0.8362747759978278}\n{'loss': 1.8965, 'grad_norm': 9.837190628051758, 'learning_rate': 4.839080459770115e-05, 'epoch': 0.847135487374423}\n{'loss': 1.9032, 'grad_norm': 3.3799033164978027, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.8579961987510182}\n{'loss': 1.984, 'grad_norm': 3.6896071434020996, 'learning_rate': 4.827586206896552e-05, 'epoch': 0.8688569101276133}\n{'loss': 1.9035, 'grad_norm': 2.4850432872772217, 'learning_rate': 4.82183908045977e-05, 'epoch': 0.8797176215042085}\n{'loss': 1.9665, 'grad_norm': 2.4635088443756104, 'learning_rate': 4.816091954022989e-05, 'epoch': 0.8905783328808037}\n{'loss': 2.0019, 'grad_norm': 2.880042552947998, 'learning_rate': 4.810344827586207e-05, 'epoch': 0.9014390442573988}\n{'loss': 1.8899, 'grad_norm': 3.754958152770996, 'learning_rate': 4.8045977011494255e-05, 'epoch': 0.912299755633994}\n{'loss': 1.8717, 'grad_norm': 2.1867518424987793, 'learning_rate': 4.798850574712644e-05, 'epoch': 0.9231604670105892}\n{'loss': 1.9678, 'grad_norm': 2.9642093181610107, 'learning_rate': 4.793103448275863e-05, 'epoch': 0.9340211783871843}\n{'loss': 1.9532, 'grad_norm': 2.4393844604492188, 'learning_rate': 4.7873563218390804e-05, 'epoch': 0.9448818897637795}\n{'loss': 1.8774, 'grad_norm': 3.085169792175293, 'learning_rate': 4.781609195402299e-05, 'epoch': 0.9557426011403747}\n{'loss': 1.9163, 'grad_norm': 3.9490833282470703, 'learning_rate': 4.7758620689655176e-05, 'epoch': 0.9666033125169698}\n{'loss': 1.9276, 'grad_norm': 3.400729179382324, 'learning_rate': 4.770114942528736e-05, 'epoch': 0.977464023893565}\n{'loss': 1.8681, 'grad_norm': 4.998781204223633, 'learning_rate': 4.764367816091954e-05, 'epoch': 0.9883247352701602}\n{'loss': 1.9471, 'grad_norm': 2.3503589630126953, 'learning_rate': 4.7586206896551725e-05, 'epoch': 0.9991854466467553}\n{'loss': 1.9081, 'grad_norm': 3.303823471069336, 'learning_rate': 4.7528735632183915e-05, 'epoch': 1.0100461580233506}\n{'loss': 1.9509, 'grad_norm': 2.1916708946228027, 'learning_rate': 4.747126436781609e-05, 'epoch': 1.0209068693999457}\n{'loss': 1.9765, 'grad_norm': 4.489452838897705, 'learning_rate': 4.741379310344828e-05, 'epoch': 1.031767580776541}\n{'loss': 1.9203, 'grad_norm': 2.3404295444488525, 'learning_rate': 4.735632183908046e-05, 'epoch': 1.042628292153136}\n{'loss': 1.9607, 'grad_norm': 3.6579787731170654, 'learning_rate': 4.7298850574712646e-05, 'epoch': 1.0534890035297313}\n{'loss': 1.913, 'grad_norm': 3.0750889778137207, 'learning_rate': 4.724137931034483e-05, 'epoch': 1.0643497149063263}\n{'loss': 1.7814, 'grad_norm': 2.948802947998047, 'learning_rate': 4.718390804597702e-05, 'epoch': 1.0752104262829216}\n{'loss': 1.8687, 'grad_norm': 3.130772352218628, 'learning_rate': 4.7126436781609195e-05, 'epoch': 1.0860711376595167}\n{'eval_loss': 1.7215012311935425, 'eval_runtime': 12.441, 'eval_samples_per_second': 65.75, 'eval_steps_per_second': 65.75, 'epoch': 1.0860711376595167}\n{'loss': 1.9509, 'grad_norm': 3.0566580295562744, 'learning_rate': 4.7068965517241385e-05, 'epoch': 1.096931849036112}\n{'loss': 1.8823, 'grad_norm': 2.4121718406677246, 'learning_rate': 4.701149425287357e-05, 'epoch': 1.107792560412707}\n{'loss': 1.9498, 'grad_norm': 3.3803601264953613, 'learning_rate': 4.695402298850575e-05, 'epoch': 1.1186532717893023}\n{'loss': 1.8402, 'grad_norm': 4.264216899871826, 'learning_rate': 4.689655172413793e-05, 'epoch': 1.1295139831658974}\n{'loss': 1.9641, 'grad_norm': 3.3334319591522217, 'learning_rate': 4.6839080459770116e-05, 'epoch': 1.1403746945424926}\n{'loss': 1.867, 'grad_norm': 2.5398097038269043, 'learning_rate': 4.67816091954023e-05, 'epoch': 1.1512354059190877}\n{'loss': 1.9612, 'grad_norm': 2.918323278427124, 'learning_rate': 4.672413793103448e-05, 'epoch': 1.162096117295683}\n{'loss': 1.884, 'grad_norm': 3.9228923320770264, 'learning_rate': 4.666666666666667e-05, 'epoch': 1.172956828672278}\n{'loss': 1.8729, 'grad_norm': 2.996835470199585, 'learning_rate': 4.6609195402298855e-05, 'epoch': 1.1838175400488733}\n{'loss': 1.9121, 'grad_norm': 2.860063076019287, 'learning_rate': 4.655172413793104e-05, 'epoch': 1.1946782514254684}\n{'loss': 1.9044, 'grad_norm': 1.9771616458892822, 'learning_rate': 4.649425287356322e-05, 'epoch': 1.2055389628020636}\n{'loss': 1.9125, 'grad_norm': 2.2734124660491943, 'learning_rate': 4.643678160919541e-05, 'epoch': 1.2163996741786587}\n{'loss': 1.7996, 'grad_norm': 3.1050846576690674, 'learning_rate': 4.6379310344827586e-05, 'epoch': 1.227260385555254}\n{'loss': 1.945, 'grad_norm': 2.2048516273498535, 'learning_rate': 4.632183908045977e-05, 'epoch': 1.238121096931849}\n{'loss': 1.8961, 'grad_norm': 2.049150228500366, 'learning_rate': 4.626436781609196e-05, 'epoch': 1.2489818083084443}\n{'loss': 1.8462, 'grad_norm': 2.8406543731689453, 'learning_rate': 4.6206896551724135e-05, 'epoch': 1.2598425196850394}\n{'loss': 1.9573, 'grad_norm': 2.2992172241210938, 'learning_rate': 4.6149425287356324e-05, 'epoch': 1.2707032310616344}\n{'loss': 1.8738, 'grad_norm': 2.4905614852905273, 'learning_rate': 4.609195402298851e-05, 'epoch': 1.2815639424382297}\n{'loss': 1.9451, 'grad_norm': 3.2440218925476074, 'learning_rate': 4.603448275862069e-05, 'epoch': 1.292424653814825}\n{'loss': 1.861, 'grad_norm': 4.229150295257568, 'learning_rate': 4.597701149425287e-05, 'epoch': 1.30328536519142}\n{'loss': 1.9498, 'grad_norm': 2.2097606658935547, 'learning_rate': 4.591954022988506e-05, 'epoch': 1.314146076568015}\n{'loss': 1.8166, 'grad_norm': 3.8946664333343506, 'learning_rate': 4.586206896551724e-05, 'epoch': 1.3250067879446104}\n{'loss': 1.809, 'grad_norm': 2.3960671424865723, 'learning_rate': 4.580459770114943e-05, 'epoch': 1.3358674993212056}\n{'loss': 1.9518, 'grad_norm': 2.236652135848999, 'learning_rate': 4.574712643678161e-05, 'epoch': 1.3467282106978007}\n{'loss': 1.8019, 'grad_norm': 2.82902193069458, 'learning_rate': 4.5689655172413794e-05, 'epoch': 1.3575889220743957}\n{'loss': 1.9763, 'grad_norm': 3.0577468872070312, 'learning_rate': 4.563218390804598e-05, 'epoch': 1.368449633450991}\n{'loss': 1.9118, 'grad_norm': 2.4699578285217285, 'learning_rate': 4.557471264367816e-05, 'epoch': 1.3793103448275863}\n{'loss': 1.8464, 'grad_norm': 2.5557684898376465, 'learning_rate': 4.551724137931035e-05, 'epoch': 1.3901710562041814}\n{'loss': 1.8857, 'grad_norm': 2.522953510284424, 'learning_rate': 4.5459770114942526e-05, 'epoch': 1.4010317675807764}\n{'loss': 1.9895, 'grad_norm': 2.1141960620880127, 'learning_rate': 4.5402298850574716e-05, 'epoch': 1.4118924789573717}\n{'loss': 1.8962, 'grad_norm': 2.5923123359680176, 'learning_rate': 4.53448275862069e-05, 'epoch': 1.422753190333967}\n{'loss': 1.8081, 'grad_norm': 2.402374267578125, 'learning_rate': 4.528735632183908e-05, 'epoch': 1.433613901710562}\n{'loss': 1.9183, 'grad_norm': 3.319798231124878, 'learning_rate': 4.5229885057471264e-05, 'epoch': 1.444474613087157}\n{'loss': 1.8767, 'grad_norm': 2.060715913772583, 'learning_rate': 4.5172413793103454e-05, 'epoch': 1.4553353244637524}\n{'loss': 1.8534, 'grad_norm': 2.2220914363861084, 'learning_rate': 4.511494252873563e-05, 'epoch': 1.4661960358403476}\n{'loss': 1.7938, 'grad_norm': 2.2598876953125, 'learning_rate': 4.505747126436782e-05, 'epoch': 1.4770567472169427}\n{'loss': 1.9372, 'grad_norm': 2.3311362266540527, 'learning_rate': 4.5e-05, 'epoch': 1.4879174585935377}\n{'loss': 1.9223, 'grad_norm': 6.1793904304504395, 'learning_rate': 4.4942528735632186e-05, 'epoch': 1.498778169970133}\n{'loss': 1.7368, 'grad_norm': 3.4578092098236084, 'learning_rate': 4.488505747126437e-05, 'epoch': 1.5096388813467283}\n{'loss': 1.9053, 'grad_norm': 2.1658992767333984, 'learning_rate': 4.482758620689655e-05, 'epoch': 1.5204995927233234}\n{'loss': 1.8781, 'grad_norm': 2.0525364875793457, 'learning_rate': 4.477011494252874e-05, 'epoch': 1.5313603040999184}\n{'loss': 1.8983, 'grad_norm': 2.316741943359375, 'learning_rate': 4.471264367816092e-05, 'epoch': 1.5422210154765137}\n{'loss': 1.8284, 'grad_norm': 2.2047364711761475, 'learning_rate': 4.465517241379311e-05, 'epoch': 1.553081726853109}\n{'loss': 1.8869, 'grad_norm': 4.920225620269775, 'learning_rate': 4.459770114942529e-05, 'epoch': 1.563942438229704}\n{'loss': 1.8785, 'grad_norm': 2.324106216430664, 'learning_rate': 4.454022988505747e-05, 'epoch': 1.574803149606299}\n{'loss': 1.8699, 'grad_norm': 2.778261423110962, 'learning_rate': 4.4482758620689656e-05, 'epoch': 1.5856638609828944}\n{'loss': 1.846, 'grad_norm': 2.8353443145751953, 'learning_rate': 4.4425287356321845e-05, 'epoch': 1.5965245723594896}\n{'loss': 1.8706, 'grad_norm': 2.954944610595703, 'learning_rate': 4.436781609195402e-05, 'epoch': 1.6073852837360847}\n{'loss': 1.8878, 'grad_norm': 2.6247401237487793, 'learning_rate': 4.431034482758621e-05, 'epoch': 1.6182459951126797}\n{'loss': 1.7759, 'grad_norm': 2.8150718212127686, 'learning_rate': 4.4252873563218394e-05, 'epoch': 1.629106706489275}\n{'eval_loss': 1.6932681798934937, 'eval_runtime': 12.5069, 'eval_samples_per_second': 65.404, 'eval_steps_per_second': 65.404, 'epoch': 1.629106706489275}\n{'loss': 1.9021, 'grad_norm': 2.608593225479126, 'learning_rate': 4.419540229885058e-05, 'epoch': 1.6399674178658703}\n{'loss': 1.7951, 'grad_norm': 2.933151960372925, 'learning_rate': 4.413793103448276e-05, 'epoch': 1.6508281292424654}\n{'loss': 1.8806, 'grad_norm': 2.4104278087615967, 'learning_rate': 4.408045977011494e-05, 'epoch': 1.6616888406190604}\n{'loss': 1.8327, 'grad_norm': 2.0677809715270996, 'learning_rate': 4.4022988505747126e-05, 'epoch': 1.6725495519956557}\n{'loss': 1.9432, 'grad_norm': 3.0493710041046143, 'learning_rate': 4.396551724137931e-05, 'epoch': 1.683410263372251}\n{'loss': 1.8284, 'grad_norm': 1.9536278247833252, 'learning_rate': 4.39080459770115e-05, 'epoch': 1.694270974748846}\n{'loss': 1.8069, 'grad_norm': 2.5261154174804688, 'learning_rate': 4.385057471264368e-05, 'epoch': 1.705131686125441}\n{'loss': 1.9181, 'grad_norm': 4.8567070960998535, 'learning_rate': 4.3793103448275864e-05, 'epoch': 1.7159923975020364}\n{'loss': 1.8591, 'grad_norm': 3.2450385093688965, 'learning_rate': 4.373563218390805e-05, 'epoch': 1.7268531088786316}\n{'loss': 1.752, 'grad_norm': 3.1211163997650146, 'learning_rate': 4.367816091954024e-05, 'epoch': 1.7377138202552267}\n{'loss': 1.8605, 'grad_norm': 3.8918347358703613, 'learning_rate': 4.362068965517241e-05, 'epoch': 1.7485745316318217}\n{'loss': 1.7931, 'grad_norm': 2.2179758548736572, 'learning_rate': 4.35632183908046e-05, 'epoch': 1.759435243008417}\n{'loss': 1.8486, 'grad_norm': 2.9688284397125244, 'learning_rate': 4.3505747126436785e-05, 'epoch': 1.7702959543850123}\n{'loss': 1.8607, 'grad_norm': 2.62304425239563, 'learning_rate': 4.344827586206897e-05, 'epoch': 1.7811566657616074}\n{'loss': 1.7488, 'grad_norm': 1.9995133876800537, 'learning_rate': 4.339080459770115e-05, 'epoch': 1.7920173771382024}\n{'loss': 1.8376, 'grad_norm': 2.750479221343994, 'learning_rate': 4.3333333333333334e-05, 'epoch': 1.8028780885147977}\n{'loss': 1.783, 'grad_norm': 2.8608734607696533, 'learning_rate': 4.327586206896552e-05, 'epoch': 1.813738799891393}\n{'loss': 1.9238, 'grad_norm': 2.0083436965942383, 'learning_rate': 4.32183908045977e-05, 'epoch': 1.824599511267988}\n{'loss': 1.8727, 'grad_norm': 2.823075294494629, 'learning_rate': 4.316091954022989e-05, 'epoch': 1.835460222644583}\n{'loss': 1.8511, 'grad_norm': 2.4723012447357178, 'learning_rate': 4.3103448275862066e-05, 'epoch': 1.8463209340211784}\n{'loss': 1.8132, 'grad_norm': 6.022322654724121, 'learning_rate': 4.3045977011494255e-05, 'epoch': 1.8571816453977736}\n{'loss': 1.9051, 'grad_norm': 3.3852007389068604, 'learning_rate': 4.298850574712644e-05, 'epoch': 1.8680423567743687}\n{'loss': 1.8588, 'grad_norm': 3.9858856201171875, 'learning_rate': 4.293103448275863e-05, 'epoch': 1.8789030681509638}\n{'loss': 1.8548, 'grad_norm': 2.1459314823150635, 'learning_rate': 4.2873563218390804e-05, 'epoch': 1.889763779527559}\n{'loss': 1.8838, 'grad_norm': 2.7134594917297363, 'learning_rate': 4.2816091954022994e-05, 'epoch': 1.9006244909041543}\n{'loss': 1.8441, 'grad_norm': 3.5232789516448975, 'learning_rate': 4.275862068965518e-05, 'epoch': 1.9114852022807494}\n{'loss': 1.8956, 'grad_norm': 2.1565165519714355, 'learning_rate': 4.270114942528736e-05, 'epoch': 1.9223459136573444}\n{'loss': 1.8274, 'grad_norm': 4.002519607543945, 'learning_rate': 4.264367816091954e-05, 'epoch': 1.9332066250339397}\n{'loss': 1.82, 'grad_norm': 3.6368584632873535, 'learning_rate': 4.2586206896551725e-05, 'epoch': 1.944067336410535}\n{'loss': 1.8373, 'grad_norm': 2.1231281757354736, 'learning_rate': 4.252873563218391e-05, 'epoch': 1.95492804778713}\n{'loss': 1.8273, 'grad_norm': 2.597548007965088, 'learning_rate': 4.247126436781609e-05, 'epoch': 1.965788759163725}\n{'loss': 1.8515, 'grad_norm': 3.2045655250549316, 'learning_rate': 4.241379310344828e-05, 'epoch': 1.9766494705403204}\n{'loss': 1.7868, 'grad_norm': 2.0374295711517334, 'learning_rate': 4.235632183908046e-05, 'epoch': 1.9875101819169156}\n{'loss': 1.8798, 'grad_norm': 2.9084811210632324, 'learning_rate': 4.2298850574712647e-05, 'epoch': 1.9983708932935107}\n{'loss': 1.9176, 'grad_norm': 2.6752119064331055, 'learning_rate': 4.224137931034483e-05, 'epoch': 2.0092316046701058}\n{'loss': 1.7434, 'grad_norm': 1.9123833179473877, 'learning_rate': 4.218390804597701e-05, 'epoch': 2.0200923160467013}\n{'loss': 1.8439, 'grad_norm': 1.9930799007415771, 'learning_rate': 4.2126436781609195e-05, 'epoch': 2.0309530274232963}\n{'loss': 1.7743, 'grad_norm': 2.6059603691101074, 'learning_rate': 4.2068965517241385e-05, 'epoch': 2.0418137387998914}\n{'loss': 1.83, 'grad_norm': 3.1092424392700195, 'learning_rate': 4.201149425287357e-05, 'epoch': 2.0526744501764864}\n{'loss': 1.7469, 'grad_norm': 2.6133546829223633, 'learning_rate': 4.195402298850575e-05, 'epoch': 2.063535161553082}\n{'loss': 1.7596, 'grad_norm': 1.994063138961792, 'learning_rate': 4.1896551724137934e-05, 'epoch': 2.074395872929677}\n{'loss': 1.8326, 'grad_norm': 1.8125081062316895, 'learning_rate': 4.1839080459770117e-05, 'epoch': 2.085256584306272}\n{'loss': 1.8371, 'grad_norm': 2.60434889793396, 'learning_rate': 4.17816091954023e-05, 'epoch': 2.096117295682867}\n{'loss': 1.7845, 'grad_norm': 1.8953698873519897, 'learning_rate': 4.172413793103448e-05, 'epoch': 2.1069780070594626}\n{'loss': 1.8292, 'grad_norm': 3.5776479244232178, 'learning_rate': 4.166666666666667e-05, 'epoch': 2.1178387184360576}\n{'loss': 1.8673, 'grad_norm': 2.8957626819610596, 'learning_rate': 4.160919540229885e-05, 'epoch': 2.1286994298126527}\n{'loss': 1.8381, 'grad_norm': 2.2129993438720703, 'learning_rate': 4.155172413793104e-05, 'epoch': 2.1395601411892478}\n{'loss': 1.7548, 'grad_norm': 2.1493661403656006, 'learning_rate': 4.149425287356322e-05, 'epoch': 2.1504208525658433}\n{'loss': 1.7843, 'grad_norm': 4.167323589324951, 'learning_rate': 4.1436781609195404e-05, 'epoch': 2.1612815639424383}\n{'loss': 1.8375, 'grad_norm': 3.2350854873657227, 'learning_rate': 4.1379310344827587e-05, 'epoch': 2.1721422753190334}\n{'eval_loss': 1.6745107173919678, 'eval_runtime': 12.4674, 'eval_samples_per_second': 65.611, 'eval_steps_per_second': 65.611, 'epoch': 2.1721422753190334}\n{'loss': 1.8236, 'grad_norm': 3.093440532684326, 'learning_rate': 4.1321839080459776e-05, 'epoch': 2.1830029866956284}\n{'loss': 1.7987, 'grad_norm': 6.58177375793457, 'learning_rate': 4.126436781609195e-05, 'epoch': 2.193863698072224}\n{'loss': 1.7603, 'grad_norm': 3.1389338970184326, 'learning_rate': 4.120689655172414e-05, 'epoch': 2.204724409448819}\n{'loss': 1.813, 'grad_norm': 2.2939395904541016, 'learning_rate': 4.1149425287356325e-05, 'epoch': 2.215585120825414}\n{'loss': 1.7657, 'grad_norm': 1.8805757761001587, 'learning_rate': 4.109195402298851e-05, 'epoch': 2.226445832202009}\n{'loss': 1.7959, 'grad_norm': 2.532853603363037, 'learning_rate': 4.103448275862069e-05, 'epoch': 2.2373065435786046}\n{'loss': 1.7734, 'grad_norm': 4.0649919509887695, 'learning_rate': 4.0977011494252874e-05, 'epoch': 2.2481672549551996}\n{'loss': 1.8498, 'grad_norm': 2.4181597232818604, 'learning_rate': 4.091954022988506e-05, 'epoch': 2.2590279663317947}\n{'loss': 1.7775, 'grad_norm': 2.062664031982422, 'learning_rate': 4.086206896551724e-05, 'epoch': 2.2698886777083898}\n{'loss': 1.753, 'grad_norm': 2.397552013397217, 'learning_rate': 4.080459770114943e-05, 'epoch': 2.2807493890849853}\n{'loss': 1.8368, 'grad_norm': 4.331635475158691, 'learning_rate': 4.074712643678161e-05, 'epoch': 2.2916101004615803}\n{'loss': 1.8387, 'grad_norm': 2.3276867866516113, 'learning_rate': 4.0689655172413795e-05, 'epoch': 2.3024708118381754}\n{'loss': 1.862, 'grad_norm': 2.5451905727386475, 'learning_rate': 4.063218390804598e-05, 'epoch': 2.3133315232147704}\n{'loss': 1.8216, 'grad_norm': 3.1504647731781006, 'learning_rate': 4.057471264367817e-05, 'epoch': 2.324192234591366}\n{'loss': 1.8023, 'grad_norm': 3.037179470062256, 'learning_rate': 4.0517241379310344e-05, 'epoch': 2.335052945967961}\n{'loss': 1.7784, 'grad_norm': 2.358626127243042, 'learning_rate': 4.045977011494253e-05, 'epoch': 2.345913657344556}\n{'loss': 1.8891, 'grad_norm': 2.286060333251953, 'learning_rate': 4.0402298850574716e-05, 'epoch': 2.356774368721151}\n{'loss': 1.8559, 'grad_norm': 3.407485008239746, 'learning_rate': 4.03448275862069e-05, 'epoch': 2.3676350800977466}\n{'loss': 1.8923, 'grad_norm': 2.2972679138183594, 'learning_rate': 4.028735632183908e-05, 'epoch': 2.3784957914743416}\n{'loss': 1.8346, 'grad_norm': 2.9924333095550537, 'learning_rate': 4.0229885057471265e-05, 'epoch': 2.3893565028509367}\n{'loss': 1.7004, 'grad_norm': 2.5377233028411865, 'learning_rate': 4.0172413793103455e-05, 'epoch': 2.4002172142275318}\n{'loss': 1.7543, 'grad_norm': 2.615994691848755, 'learning_rate': 4.011494252873563e-05, 'epoch': 2.4110779256041273}\n{'loss': 1.7223, 'grad_norm': 2.4428231716156006, 'learning_rate': 4.005747126436782e-05, 'epoch': 2.4219386369807223}\n{'loss': 1.7592, 'grad_norm': 2.277050495147705, 'learning_rate': 4e-05, 'epoch': 2.4327993483573174}\n{'loss': 1.7616, 'grad_norm': 1.9511828422546387, 'learning_rate': 3.9942528735632186e-05, 'epoch': 2.4436600597339124}\n{'loss': 1.7504, 'grad_norm': 2.2515530586242676, 'learning_rate': 3.988505747126437e-05, 'epoch': 2.454520771110508}\n{'loss': 1.8145, 'grad_norm': 2.162508487701416, 'learning_rate': 3.982758620689656e-05, 'epoch': 2.465381482487103}\n{'loss': 1.9256, 'grad_norm': 2.205156087875366, 'learning_rate': 3.9770114942528735e-05, 'epoch': 2.476242193863698}\n{'loss': 1.7472, 'grad_norm': 2.7414634227752686, 'learning_rate': 3.9712643678160925e-05, 'epoch': 2.487102905240293}\n{'loss': 1.8232, 'grad_norm': 2.286597967147827, 'learning_rate': 3.965517241379311e-05, 'epoch': 2.4979636166168886}\n{'loss': 1.8098, 'grad_norm': 2.121633768081665, 'learning_rate': 3.959770114942529e-05, 'epoch': 2.5088243279934836}\n{'loss': 1.7209, 'grad_norm': 2.8050708770751953, 'learning_rate': 3.954022988505747e-05, 'epoch': 2.5196850393700787}\n{'loss': 1.7973, 'grad_norm': 2.2256391048431396, 'learning_rate': 3.9482758620689656e-05, 'epoch': 2.5305457507466738}\n{'loss': 1.7356, 'grad_norm': 2.3731861114501953, 'learning_rate': 3.942528735632184e-05, 'epoch': 2.541406462123269}\n{'loss': 1.7376, 'grad_norm': 2.6663424968719482, 'learning_rate': 3.936781609195402e-05, 'epoch': 2.5522671734998643}\n{'loss': 1.85, 'grad_norm': 2.1795027256011963, 'learning_rate': 3.931034482758621e-05, 'epoch': 2.5631278848764594}\n{'loss': 1.8402, 'grad_norm': 2.3811049461364746, 'learning_rate': 3.9252873563218395e-05, 'epoch': 2.5739885962530544}\n{'loss': 1.8344, 'grad_norm': 2.2020349502563477, 'learning_rate': 3.919540229885058e-05, 'epoch': 2.58484930762965}\n{'loss': 1.8711, 'grad_norm': 2.2260868549346924, 'learning_rate': 3.913793103448276e-05, 'epoch': 2.595710019006245}\n{'loss': 1.8553, 'grad_norm': 2.319120168685913, 'learning_rate': 3.908045977011495e-05, 'epoch': 2.60657073038284}\n{'loss': 1.8537, 'grad_norm': 2.2349493503570557, 'learning_rate': 3.9022988505747126e-05, 'epoch': 2.617431441759435}\n{'loss': 1.8231, 'grad_norm': 2.6527724266052246, 'learning_rate': 3.896551724137931e-05, 'epoch': 2.62829215313603}\n{'loss': 1.8247, 'grad_norm': 3.0035722255706787, 'learning_rate': 3.89080459770115e-05, 'epoch': 2.6391528645126257}\n{'loss': 1.7572, 'grad_norm': 2.090999126434326, 'learning_rate': 3.8850574712643675e-05, 'epoch': 2.6500135758892207}\n{'loss': 1.806, 'grad_norm': 2.0744283199310303, 'learning_rate': 3.8793103448275865e-05, 'epoch': 2.6608742872658158}\n{'loss': 1.7524, 'grad_norm': 2.2526230812072754, 'learning_rate': 3.873563218390805e-05, 'epoch': 2.6717349986424113}\n{'loss': 1.7872, 'grad_norm': 2.8496358394622803, 'learning_rate': 3.867816091954023e-05, 'epoch': 2.6825957100190063}\n{'loss': 1.786, 'grad_norm': 3.22823429107666, 'learning_rate': 3.862068965517241e-05, 'epoch': 2.6934564213956014}\n{'loss': 1.7614, 'grad_norm': 9.587499618530273, 'learning_rate': 3.85632183908046e-05, 'epoch': 2.7043171327721964}\n{'loss': 1.9138, 'grad_norm': 2.542757272720337, 'learning_rate': 3.850574712643678e-05, 'epoch': 2.7151778441487915}\n{'eval_loss': 1.6544625759124756, 'eval_runtime': 12.3488, 'eval_samples_per_second': 66.241, 'eval_steps_per_second': 66.241, 'epoch': 2.7151778441487915}\n{'loss': 1.8071, 'grad_norm': 3.209087610244751, 'learning_rate': 3.844827586206897e-05, 'epoch': 2.726038555525387}\n{'loss': 1.8282, 'grad_norm': 1.6574859619140625, 'learning_rate': 3.839080459770115e-05, 'epoch': 2.736899266901982}\n{'loss': 1.8909, 'grad_norm': 3.3064327239990234, 'learning_rate': 3.8333333333333334e-05, 'epoch': 2.747759978278577}\n{'loss': 1.8463, 'grad_norm': 2.1443727016448975, 'learning_rate': 3.827586206896552e-05, 'epoch': 2.7586206896551726}\n{'loss': 1.796, 'grad_norm': 2.6158204078674316, 'learning_rate': 3.82183908045977e-05, 'epoch': 2.7694814010317677}\n{'loss': 1.8115, 'grad_norm': 4.89547872543335, 'learning_rate': 3.816091954022989e-05, 'epoch': 2.7803421124083627}\n{'loss': 1.7991, 'grad_norm': 2.434131622314453, 'learning_rate': 3.8103448275862066e-05, 'epoch': 2.7912028237849578}\n{'loss': 1.884, 'grad_norm': 2.8538949489593506, 'learning_rate': 3.8045977011494256e-05, 'epoch': 2.802063535161553}\n{'loss': 1.8268, 'grad_norm': 1.9280489683151245, 'learning_rate': 3.798850574712644e-05, 'epoch': 2.8129242465381483}\n{'loss': 1.7726, 'grad_norm': 1.9969090223312378, 'learning_rate': 3.793103448275862e-05, 'epoch': 2.8237849579147434}\n{'loss': 1.745, 'grad_norm': 2.0858983993530273, 'learning_rate': 3.7873563218390804e-05, 'epoch': 2.8346456692913384}\n{'loss': 1.7662, 'grad_norm': 2.2807414531707764, 'learning_rate': 3.7816091954022994e-05, 'epoch': 2.845506380667934}\n{'loss': 1.8197, 'grad_norm': 2.362654209136963, 'learning_rate': 3.775862068965517e-05, 'epoch': 2.856367092044529}\n{'loss': 1.8367, 'grad_norm': 1.8116207122802734, 'learning_rate': 3.770114942528736e-05, 'epoch': 2.867227803421124}\n{'loss': 1.7202, 'grad_norm': 4.279887676239014, 'learning_rate': 3.764367816091954e-05, 'epoch': 2.878088514797719}\n{'loss': 1.7613, 'grad_norm': 1.7690292596817017, 'learning_rate': 3.7586206896551726e-05, 'epoch': 2.888949226174314}\n{'loss': 1.8074, 'grad_norm': 2.9848949909210205, 'learning_rate': 3.752873563218391e-05, 'epoch': 2.8998099375509097}\n{'loss': 1.7995, 'grad_norm': 2.46329402923584, 'learning_rate': 3.747126436781609e-05, 'epoch': 2.9106706489275047}\n{'loss': 1.8337, 'grad_norm': 2.9499335289001465, 'learning_rate': 3.741379310344828e-05, 'epoch': 2.9215313603040998}\n{'loss': 1.7819, 'grad_norm': 2.339355945587158, 'learning_rate': 3.735632183908046e-05, 'epoch': 2.9323920716806953}\n{'loss': 1.7546, 'grad_norm': 2.36073637008667, 'learning_rate': 3.729885057471265e-05, 'epoch': 2.9432527830572903}\n{'loss': 1.7648, 'grad_norm': 2.7214066982269287, 'learning_rate': 3.724137931034483e-05, 'epoch': 2.9541134944338854}\n{'loss': 1.7467, 'grad_norm': 1.8784071207046509, 'learning_rate': 3.718390804597701e-05, 'epoch': 2.9649742058104804}\n{'loss': 1.7986, 'grad_norm': 5.007478713989258, 'learning_rate': 3.7126436781609196e-05, 'epoch': 2.9758349171870755}\n{'loss': 1.7974, 'grad_norm': 3.0179355144500732, 'learning_rate': 3.7068965517241385e-05, 'epoch': 2.986695628563671}\n{'loss': 1.7951, 'grad_norm': 3.00429105758667, 'learning_rate': 3.701149425287356e-05, 'epoch': 2.997556339940266}\n{'loss': 1.7185, 'grad_norm': 1.872196912765503, 'learning_rate': 3.695402298850575e-05, 'epoch': 3.008417051316861}\n{'loss': 1.7396, 'grad_norm': 3.355292558670044, 'learning_rate': 3.6896551724137934e-05, 'epoch': 3.0192777626934566}\n{'loss': 1.7793, 'grad_norm': 2.332833766937256, 'learning_rate': 3.683908045977012e-05, 'epoch': 3.0301384740700517}\n{'loss': 1.8302, 'grad_norm': 2.227144241333008, 'learning_rate': 3.67816091954023e-05, 'epoch': 3.0409991854466467}\n{'loss': 1.6713, 'grad_norm': 1.8994991779327393, 'learning_rate': 3.672413793103448e-05, 'epoch': 3.0518598968232418}\n{'loss': 1.6887, 'grad_norm': 2.209203004837036, 'learning_rate': 3.6666666666666666e-05, 'epoch': 3.0627206081998373}\n{'loss': 1.8678, 'grad_norm': 2.1043388843536377, 'learning_rate': 3.660919540229885e-05, 'epoch': 3.0735813195764323}\n{'loss': 1.8452, 'grad_norm': 1.7429296970367432, 'learning_rate': 3.655172413793104e-05, 'epoch': 3.0844420309530274}\n{'loss': 1.8273, 'grad_norm': 2.6478264331817627, 'learning_rate': 3.649425287356322e-05, 'epoch': 3.0953027423296224}\n{'loss': 1.8352, 'grad_norm': 1.9499608278274536, 'learning_rate': 3.6436781609195404e-05, 'epoch': 3.106163453706218}\n{'loss': 1.828, 'grad_norm': 2.438286781311035, 'learning_rate': 3.637931034482759e-05, 'epoch': 3.117024165082813}\n{'loss': 1.7144, 'grad_norm': 2.431774854660034, 'learning_rate': 3.632183908045978e-05, 'epoch': 3.127884876459408}\n{'loss': 1.8155, 'grad_norm': 9.238683700561523, 'learning_rate': 3.626436781609195e-05, 'epoch': 3.138745587836003}\n{'loss': 1.8223, 'grad_norm': 2.2704508304595947, 'learning_rate': 3.620689655172414e-05, 'epoch': 3.1496062992125986}\n{'loss': 1.6548, 'grad_norm': 2.5547304153442383, 'learning_rate': 3.6149425287356325e-05, 'epoch': 3.1604670105891937}\n{'loss': 1.8208, 'grad_norm': 2.747112512588501, 'learning_rate': 3.609195402298851e-05, 'epoch': 3.1713277219657887}\n{'loss': 1.7756, 'grad_norm': 3.073821544647217, 'learning_rate': 3.603448275862069e-05, 'epoch': 3.1821884333423838}\n{'loss': 1.8283, 'grad_norm': 1.9581447839736938, 'learning_rate': 3.5977011494252874e-05, 'epoch': 3.1930491447189793}\n{'loss': 1.7853, 'grad_norm': 3.372413158416748, 'learning_rate': 3.591954022988506e-05, 'epoch': 3.2039098560955743}\n{'loss': 1.704, 'grad_norm': 2.213691473007202, 'learning_rate': 3.586206896551724e-05, 'epoch': 3.2147705674721694}\n{'loss': 1.7889, 'grad_norm': 3.3059699535369873, 'learning_rate': 3.580459770114943e-05, 'epoch': 3.2256312788487644}\n{'loss': 1.7308, 'grad_norm': 2.1338014602661133, 'learning_rate': 3.5747126436781606e-05, 'epoch': 3.23649199022536}\n{'loss': 1.7456, 'grad_norm': 1.986314296722412, 'learning_rate': 3.5689655172413795e-05, 'epoch': 3.247352701601955}\n{'loss': 1.7569, 'grad_norm': 3.505105972290039, 'learning_rate': 3.563218390804598e-05, 'epoch': 3.25821341297855}\n{'eval_loss': 1.6494140625, 'eval_runtime': 12.466, 'eval_samples_per_second': 65.618, 'eval_steps_per_second': 65.618, 'epoch': 3.25821341297855}\n{'loss': 1.7384, 'grad_norm': 2.822432518005371, 'learning_rate': 3.557471264367817e-05, 'epoch': 3.269074124355145}\n{'loss': 1.8009, 'grad_norm': 2.8638997077941895, 'learning_rate': 3.5517241379310344e-05, 'epoch': 3.2799348357317406}\n{'loss': 1.6113, 'grad_norm': 2.4514336585998535, 'learning_rate': 3.5459770114942534e-05, 'epoch': 3.2907955471083357}\n{'loss': 1.7523, 'grad_norm': 3.695791244506836, 'learning_rate': 3.540229885057472e-05, 'epoch': 3.3016562584849307}\n{'loss': 1.8283, 'grad_norm': 2.3992507457733154, 'learning_rate': 3.53448275862069e-05, 'epoch': 3.3125169698615258}\n{'loss': 1.7843, 'grad_norm': 2.830296754837036, 'learning_rate': 3.528735632183908e-05, 'epoch': 3.3233776812381213}\n{'loss': 1.8092, 'grad_norm': 2.1557247638702393, 'learning_rate': 3.5229885057471265e-05, 'epoch': 3.3342383926147163}\n{'loss': 1.796, 'grad_norm': 2.5375864505767822, 'learning_rate': 3.517241379310345e-05, 'epoch': 3.3450991039913114}\n{'loss': 1.7005, 'grad_norm': 1.8857312202453613, 'learning_rate': 3.511494252873563e-05, 'epoch': 3.3559598153679064}\n{'loss': 1.8611, 'grad_norm': 2.029237985610962, 'learning_rate': 3.505747126436782e-05, 'epoch': 3.366820526744502}\n{'loss': 1.8095, 'grad_norm': 2.2957651615142822, 'learning_rate': 3.5e-05, 'epoch': 3.377681238121097}\n{'loss': 1.7374, 'grad_norm': 2.987358570098877, 'learning_rate': 3.4942528735632187e-05, 'epoch': 3.388541949497692}\n{'loss': 1.7277, 'grad_norm': 2.1344428062438965, 'learning_rate': 3.488505747126437e-05, 'epoch': 3.399402660874287}\n{'loss': 1.7585, 'grad_norm': 2.120929479598999, 'learning_rate': 3.482758620689655e-05, 'epoch': 3.4102633722508826}\n{'loss': 1.722, 'grad_norm': 2.638667345046997, 'learning_rate': 3.4770114942528735e-05, 'epoch': 3.4211240836274777}\n{'loss': 1.7738, 'grad_norm': 2.758768320083618, 'learning_rate': 3.4712643678160925e-05, 'epoch': 3.4319847950040727}\n{'loss': 1.8139, 'grad_norm': 2.2295331954956055, 'learning_rate': 3.465517241379311e-05, 'epoch': 3.4428455063806678}\n{'loss': 1.771, 'grad_norm': 2.52201509475708, 'learning_rate': 3.459770114942529e-05, 'epoch': 3.4537062177572633}\n{'loss': 1.7333, 'grad_norm': 2.5631401538848877, 'learning_rate': 3.4540229885057474e-05, 'epoch': 3.4645669291338583}\n{'loss': 1.8278, 'grad_norm': 2.6766083240509033, 'learning_rate': 3.4482758620689657e-05, 'epoch': 3.4754276405104534}\n{'loss': 1.7571, 'grad_norm': 2.078429698944092, 'learning_rate': 3.442528735632184e-05, 'epoch': 3.4862883518870484}\n{'loss': 1.8595, 'grad_norm': 2.651888608932495, 'learning_rate': 3.436781609195402e-05, 'epoch': 3.497149063263644}\n{'loss': 1.6897, 'grad_norm': 2.762025833129883, 'learning_rate': 3.431034482758621e-05, 'epoch': 3.508009774640239}\n{'loss': 1.6904, 'grad_norm': 3.8406951427459717, 'learning_rate': 3.425287356321839e-05, 'epoch': 3.518870486016834}\n{'loss': 1.7619, 'grad_norm': 8.90317440032959, 'learning_rate': 3.419540229885058e-05, 'epoch': 3.529731197393429}\n{'loss': 1.7185, 'grad_norm': 2.0030579566955566, 'learning_rate': 3.413793103448276e-05, 'epoch': 3.5405919087700246}\n{'loss': 1.7576, 'grad_norm': 3.0066428184509277, 'learning_rate': 3.4080459770114944e-05, 'epoch': 3.5514526201466197}\n{'loss': 1.6678, 'grad_norm': 1.9980547428131104, 'learning_rate': 3.4022988505747127e-05, 'epoch': 3.5623133315232147}\n{'loss': 1.7113, 'grad_norm': 2.209183692932129, 'learning_rate': 3.3965517241379316e-05, 'epoch': 3.5731740428998098}\n{'loss': 1.7377, 'grad_norm': 6.501672744750977, 'learning_rate': 3.390804597701149e-05, 'epoch': 3.584034754276405}\n{'loss': 1.7145, 'grad_norm': 2.2296459674835205, 'learning_rate': 3.385057471264368e-05, 'epoch': 3.5948954656530003}\n{'loss': 1.8047, 'grad_norm': 5.396641731262207, 'learning_rate': 3.3793103448275865e-05, 'epoch': 3.6057561770295954}\n{'loss': 1.771, 'grad_norm': 3.853062629699707, 'learning_rate': 3.373563218390805e-05, 'epoch': 3.6166168884061904}\n{'loss': 1.7953, 'grad_norm': 2.0061817169189453, 'learning_rate': 3.367816091954023e-05, 'epoch': 3.627477599782786}\n{'loss': 1.7536, 'grad_norm': 2.5312137603759766, 'learning_rate': 3.3620689655172414e-05, 'epoch': 3.638338311159381}\n{'loss': 1.766, 'grad_norm': 2.6544201374053955, 'learning_rate': 3.35632183908046e-05, 'epoch': 3.649199022535976}\n{'loss': 1.8517, 'grad_norm': 2.170884132385254, 'learning_rate': 3.350574712643678e-05, 'epoch': 3.660059733912571}\n{'loss': 1.6936, 'grad_norm': 5.319210529327393, 'learning_rate': 3.344827586206897e-05, 'epoch': 3.670920445289166}\n{'loss': 1.764, 'grad_norm': 2.1686205863952637, 'learning_rate': 3.339080459770115e-05, 'epoch': 3.6817811566657617}\n{'loss': 1.8333, 'grad_norm': 1.7665890455245972, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.6926418680423567}\n{'loss': 1.7366, 'grad_norm': 2.19258975982666, 'learning_rate': 3.327586206896552e-05, 'epoch': 3.703502579418952}\n{'loss': 1.6913, 'grad_norm': 3.388998031616211, 'learning_rate': 3.321839080459771e-05, 'epoch': 3.7143632907955473}\n{'loss': 1.6572, 'grad_norm': 2.322740316390991, 'learning_rate': 3.3160919540229884e-05, 'epoch': 3.7252240021721423}\n{'loss': 1.7568, 'grad_norm': 2.501368999481201, 'learning_rate': 3.310344827586207e-05, 'epoch': 3.7360847135487374}\n{'loss': 1.8101, 'grad_norm': 2.5849156379699707, 'learning_rate': 3.3045977011494256e-05, 'epoch': 3.7469454249253324}\n{'loss': 1.8062, 'grad_norm': 2.884077310562134, 'learning_rate': 3.298850574712644e-05, 'epoch': 3.7578061363019275}\n{'loss': 1.8008, 'grad_norm': 2.6778817176818848, 'learning_rate': 3.293103448275862e-05, 'epoch': 3.768666847678523}\n{'loss': 1.6832, 'grad_norm': 2.447723388671875, 'learning_rate': 3.2873563218390805e-05, 'epoch': 3.779527559055118}\n{'loss': 1.7687, 'grad_norm': 2.209789514541626, 'learning_rate': 3.2816091954022995e-05, 'epoch': 3.790388270431713}\n{'loss': 1.6987, 'grad_norm': 2.4650380611419678, 'learning_rate': 3.275862068965517e-05, 'epoch': 3.8012489818083086}\n{'eval_loss': 1.6355063915252686, 'eval_runtime': 12.3974, 'eval_samples_per_second': 65.982, 'eval_steps_per_second': 65.982, 'epoch': 3.8012489818083086}\n{'loss': 1.7972, 'grad_norm': 1.996033787727356, 'learning_rate': 3.270114942528736e-05, 'epoch': 3.8121096931849037}\n{'loss': 1.6439, 'grad_norm': 3.2960245609283447, 'learning_rate': 3.264367816091954e-05, 'epoch': 3.8229704045614987}\n{'loss': 1.7542, 'grad_norm': 2.199232339859009, 'learning_rate': 3.2586206896551726e-05, 'epoch': 3.833831115938094}\n{'loss': 1.7246, 'grad_norm': 2.9837138652801514, 'learning_rate': 3.252873563218391e-05, 'epoch': 3.844691827314689}\n{'loss': 1.7281, 'grad_norm': 2.0581812858581543, 'learning_rate': 3.24712643678161e-05, 'epoch': 3.8555525386912843}\n{'loss': 1.7503, 'grad_norm': 2.3261091709136963, 'learning_rate': 3.2413793103448275e-05, 'epoch': 3.8664132500678794}\n{'loss': 1.8459, 'grad_norm': 2.6088321208953857, 'learning_rate': 3.2356321839080465e-05, 'epoch': 3.8772739614444744}\n{'loss': 1.6771, 'grad_norm': 1.9182223081588745, 'learning_rate': 3.229885057471265e-05, 'epoch': 3.88813467282107}\n{'loss': 1.8262, 'grad_norm': 2.2306511402130127, 'learning_rate': 3.2241379310344824e-05, 'epoch': 3.898995384197665}\n{'loss': 1.7797, 'grad_norm': 2.070375919342041, 'learning_rate': 3.218390804597701e-05, 'epoch': 3.90985609557426}\n{'loss': 1.6741, 'grad_norm': 1.971621036529541, 'learning_rate': 3.2126436781609196e-05, 'epoch': 3.920716806950855}\n{'loss': 1.7242, 'grad_norm': 3.0343520641326904, 'learning_rate': 3.206896551724138e-05, 'epoch': 3.93157751832745}\n{'loss': 1.7673, 'grad_norm': 2.2206239700317383, 'learning_rate': 3.201149425287356e-05, 'epoch': 3.9424382297040457}\n{'loss': 1.7161, 'grad_norm': 1.9190382957458496, 'learning_rate': 3.195402298850575e-05, 'epoch': 3.9532989410806407}\n{'loss': 1.8009, 'grad_norm': 2.080341339111328, 'learning_rate': 3.1896551724137935e-05, 'epoch': 3.964159652457236}\n{'loss': 1.7104, 'grad_norm': 2.2859127521514893, 'learning_rate': 3.183908045977012e-05, 'epoch': 3.9750203638338313}\n{'loss': 1.6694, 'grad_norm': 3.40657377243042, 'learning_rate': 3.17816091954023e-05, 'epoch': 3.9858810752104263}\n{'loss': 1.7552, 'grad_norm': 2.6132280826568604, 'learning_rate': 3.172413793103448e-05, 'epoch': 3.9967417865870214}\n{'loss': 1.7272, 'grad_norm': 3.0110318660736084, 'learning_rate': 3.1666666666666666e-05, 'epoch': 4.0076024979636165}\n{'loss': 1.7591, 'grad_norm': 3.2677013874053955, 'learning_rate': 3.160919540229885e-05, 'epoch': 4.0184632093402115}\n{'loss': 1.7119, 'grad_norm': 2.435985565185547, 'learning_rate': 3.155172413793104e-05, 'epoch': 4.029323920716807}\n{'loss': 1.776, 'grad_norm': 2.0490617752075195, 'learning_rate': 3.1494252873563215e-05, 'epoch': 4.0401846320934025}\n{'loss': 1.6715, 'grad_norm': 1.9642226696014404, 'learning_rate': 3.1436781609195405e-05, 'epoch': 4.051045343469998}\n{'loss': 1.77, 'grad_norm': 2.7852044105529785, 'learning_rate': 3.137931034482759e-05, 'epoch': 4.061906054846593}\n{'loss': 1.6957, 'grad_norm': 1.9580107927322388, 'learning_rate': 3.132183908045977e-05, 'epoch': 4.072766766223188}\n{'loss': 1.7758, 'grad_norm': 2.350214958190918, 'learning_rate': 3.126436781609195e-05, 'epoch': 4.083627477599783}\n{'loss': 1.7173, 'grad_norm': 2.413473129272461, 'learning_rate': 3.120689655172414e-05, 'epoch': 4.094488188976378}\n{'loss': 1.7211, 'grad_norm': 2.3277928829193115, 'learning_rate': 3.114942528735632e-05, 'epoch': 4.105348900352973}\n{'loss': 1.6839, 'grad_norm': 1.956056833267212, 'learning_rate': 3.109195402298851e-05, 'epoch': 4.116209611729568}\n{'loss': 1.6562, 'grad_norm': 3.438645601272583, 'learning_rate': 3.103448275862069e-05, 'epoch': 4.127070323106164}\n{'loss': 1.8001, 'grad_norm': 2.7799220085144043, 'learning_rate': 3.0977011494252875e-05, 'epoch': 4.137931034482759}\n{'loss': 1.7471, 'grad_norm': 2.2343428134918213, 'learning_rate': 3.091954022988506e-05, 'epoch': 4.148791745859354}\n{'loss': 1.7304, 'grad_norm': 2.487230062484741, 'learning_rate': 3.086206896551724e-05, 'epoch': 4.159652457235949}\n{'loss': 1.766, 'grad_norm': 2.4127917289733887, 'learning_rate': 3.080459770114943e-05, 'epoch': 4.170513168612544}\n{'loss': 1.7784, 'grad_norm': 2.8459272384643555, 'learning_rate': 3.0747126436781606e-05, 'epoch': 4.181373879989139}\n{'loss': 1.7036, 'grad_norm': 3.3042311668395996, 'learning_rate': 3.0689655172413796e-05, 'epoch': 4.192234591365734}\n{'loss': 1.8136, 'grad_norm': 2.016303777694702, 'learning_rate': 3.063218390804598e-05, 'epoch': 4.203095302742329}\n{'loss': 1.7623, 'grad_norm': 1.9669275283813477, 'learning_rate': 3.057471264367816e-05, 'epoch': 4.213956014118925}\n{'loss': 1.6887, 'grad_norm': 2.4639768600463867, 'learning_rate': 3.0517241379310348e-05, 'epoch': 4.22481672549552}\n{'loss': 1.704, 'grad_norm': 3.0056240558624268, 'learning_rate': 3.045977011494253e-05, 'epoch': 4.235677436872115}\n{'loss': 1.6392, 'grad_norm': 2.2666890621185303, 'learning_rate': 3.040229885057471e-05, 'epoch': 4.24653814824871}\n{'loss': 1.7088, 'grad_norm': 2.5842061042785645, 'learning_rate': 3.0344827586206897e-05, 'epoch': 4.257398859625305}\n{'loss': 1.6992, 'grad_norm': 2.2899627685546875, 'learning_rate': 3.0287356321839083e-05, 'epoch': 4.2682595710019005}\n{'loss': 1.6161, 'grad_norm': 2.327972650527954, 'learning_rate': 3.0229885057471262e-05, 'epoch': 4.2791202823784955}\n{'loss': 1.7095, 'grad_norm': 2.427794933319092, 'learning_rate': 3.017241379310345e-05, 'epoch': 4.289980993755091}\n{'loss': 1.7041, 'grad_norm': 2.588916778564453, 'learning_rate': 3.0114942528735635e-05, 'epoch': 4.3008417051316865}\n{'loss': 1.7344, 'grad_norm': 2.274634838104248, 'learning_rate': 3.005747126436782e-05, 'epoch': 4.311702416508282}\n{'loss': 1.698, 'grad_norm': 2.4743525981903076, 'learning_rate': 3e-05, 'epoch': 4.322563127884877}\n{'loss': 1.7113, 'grad_norm': 2.6417171955108643, 'learning_rate': 2.9942528735632187e-05, 'epoch': 4.333423839261472}\n{'loss': 1.5776, 'grad_norm': 2.2878849506378174, 'learning_rate': 2.988505747126437e-05, 'epoch': 4.344284550638067}\n{'eval_loss': 1.632853388786316, 'eval_runtime': 12.3306, 'eval_samples_per_second': 66.339, 'eval_steps_per_second': 66.339, 'epoch': 4.344284550638067}\n{'loss': 1.7384, 'grad_norm': 2.2855775356292725, 'learning_rate': 2.9827586206896553e-05, 'epoch': 4.355145262014662}\n{'loss': 1.7068, 'grad_norm': 2.3874142169952393, 'learning_rate': 2.9770114942528736e-05, 'epoch': 4.366005973391257}\n{'loss': 1.7628, 'grad_norm': 2.82171893119812, 'learning_rate': 2.9712643678160922e-05, 'epoch': 4.376866684767852}\n{'loss': 1.8295, 'grad_norm': 2.2828965187072754, 'learning_rate': 2.96551724137931e-05, 'epoch': 4.387727396144448}\n{'loss': 1.7488, 'grad_norm': 2.3491694927215576, 'learning_rate': 2.9597701149425288e-05, 'epoch': 4.398588107521043}\n{'loss': 1.7194, 'grad_norm': 12.497673988342285, 'learning_rate': 2.9540229885057474e-05, 'epoch': 4.409448818897638}\n{'loss': 1.689, 'grad_norm': 3.096803665161133, 'learning_rate': 2.9482758620689654e-05, 'epoch': 4.420309530274233}\n{'loss': 1.6339, 'grad_norm': 1.816333293914795, 'learning_rate': 2.942528735632184e-05, 'epoch': 4.431170241650828}\n{'loss': 1.7815, 'grad_norm': 2.0767087936401367, 'learning_rate': 2.9367816091954026e-05, 'epoch': 4.442030953027423}\n{'loss': 1.7205, 'grad_norm': 2.1310319900512695, 'learning_rate': 2.9310344827586206e-05, 'epoch': 4.452891664404018}\n{'loss': 1.7365, 'grad_norm': 9.91504955291748, 'learning_rate': 2.9252873563218392e-05, 'epoch': 4.463752375780613}\n{'loss': 1.6719, 'grad_norm': 2.2975757122039795, 'learning_rate': 2.919540229885058e-05, 'epoch': 4.474613087157209}\n{'loss': 1.7674, 'grad_norm': 2.0195958614349365, 'learning_rate': 2.913793103448276e-05, 'epoch': 4.485473798533804}\n{'loss': 1.7239, 'grad_norm': 4.008654594421387, 'learning_rate': 2.9080459770114944e-05, 'epoch': 4.496334509910399}\n{'loss': 1.7398, 'grad_norm': 3.1738221645355225, 'learning_rate': 2.9022988505747127e-05, 'epoch': 4.507195221286994}\n{'loss': 1.6493, 'grad_norm': 1.8278263807296753, 'learning_rate': 2.8965517241379313e-05, 'epoch': 4.518055932663589}\n{'loss': 1.6752, 'grad_norm': 1.9436711072921753, 'learning_rate': 2.8908045977011493e-05, 'epoch': 4.5289166440401845}\n{'loss': 1.7616, 'grad_norm': 2.3792977333068848, 'learning_rate': 2.885057471264368e-05, 'epoch': 4.5397773554167795}\n{'loss': 1.757, 'grad_norm': 2.830214023590088, 'learning_rate': 2.8793103448275865e-05, 'epoch': 4.5506380667933755}\n{'loss': 1.7219, 'grad_norm': 2.5196425914764404, 'learning_rate': 2.8735632183908045e-05, 'epoch': 4.5614987781699705}\n{'loss': 1.8087, 'grad_norm': 3.1796958446502686, 'learning_rate': 2.867816091954023e-05, 'epoch': 4.572359489546566}\n{'loss': 1.6529, 'grad_norm': 2.4250335693359375, 'learning_rate': 2.8620689655172417e-05, 'epoch': 4.583220200923161}\n{'loss': 1.7453, 'grad_norm': 2.127418041229248, 'learning_rate': 2.8563218390804597e-05, 'epoch': 4.594080912299756}\n{'loss': 1.7465, 'grad_norm': 2.1344845294952393, 'learning_rate': 2.8505747126436783e-05, 'epoch': 4.604941623676351}\n{'loss': 1.7732, 'grad_norm': 2.2996788024902344, 'learning_rate': 2.844827586206897e-05, 'epoch': 4.615802335052946}\n{'loss': 1.7236, 'grad_norm': 2.843153715133667, 'learning_rate': 2.839080459770115e-05, 'epoch': 4.626663046429541}\n{'loss': 1.677, 'grad_norm': 2.1769089698791504, 'learning_rate': 2.8333333333333335e-05, 'epoch': 4.637523757806136}\n{'loss': 1.7561, 'grad_norm': 3.0788767337799072, 'learning_rate': 2.8275862068965518e-05, 'epoch': 4.648384469182732}\n{'loss': 1.6641, 'grad_norm': 1.5736225843429565, 'learning_rate': 2.8218390804597705e-05, 'epoch': 4.659245180559327}\n{'loss': 1.7337, 'grad_norm': 3.2041220664978027, 'learning_rate': 2.8160919540229884e-05, 'epoch': 4.670105891935922}\n{'loss': 1.7616, 'grad_norm': 2.502715587615967, 'learning_rate': 2.810344827586207e-05, 'epoch': 4.680966603312517}\n{'loss': 1.7194, 'grad_norm': 2.2416176795959473, 'learning_rate': 2.8045977011494257e-05, 'epoch': 4.691827314689112}\n{'loss': 1.7339, 'grad_norm': 2.4824674129486084, 'learning_rate': 2.7988505747126436e-05, 'epoch': 4.702688026065707}\n{'loss': 1.7366, 'grad_norm': 3.698441982269287, 'learning_rate': 2.7931034482758622e-05, 'epoch': 4.713548737442302}\n{'loss': 1.6932, 'grad_norm': 3.1856157779693604, 'learning_rate': 2.787356321839081e-05, 'epoch': 4.724409448818898}\n{'loss': 1.7175, 'grad_norm': 2.2573013305664062, 'learning_rate': 2.7816091954022988e-05, 'epoch': 4.735270160195493}\n{'loss': 1.723, 'grad_norm': 2.1698660850524902, 'learning_rate': 2.7758620689655175e-05, 'epoch': 4.746130871572088}\n{'loss': 1.7078, 'grad_norm': 2.34726619720459, 'learning_rate': 2.770114942528736e-05, 'epoch': 4.756991582948683}\n{'loss': 1.7666, 'grad_norm': 3.1133787631988525, 'learning_rate': 2.764367816091954e-05, 'epoch': 4.767852294325278}\n{'loss': 1.673, 'grad_norm': 1.8743592500686646, 'learning_rate': 2.7586206896551727e-05, 'epoch': 4.778713005701873}\n{'loss': 1.8331, 'grad_norm': 2.4344918727874756, 'learning_rate': 2.752873563218391e-05, 'epoch': 4.7895737170784685}\n{'loss': 1.7526, 'grad_norm': 2.604785919189453, 'learning_rate': 2.7471264367816092e-05, 'epoch': 4.8004344284550635}\n{'loss': 1.8343, 'grad_norm': 5.7756123542785645, 'learning_rate': 2.7413793103448275e-05, 'epoch': 4.811295139831659}\n{'loss': 1.711, 'grad_norm': 2.8207805156707764, 'learning_rate': 2.735632183908046e-05, 'epoch': 4.8221558512082545}\n{'loss': 1.7728, 'grad_norm': 2.819667100906372, 'learning_rate': 2.7298850574712648e-05, 'epoch': 4.83301656258485}\n{'loss': 1.7851, 'grad_norm': 3.2140727043151855, 'learning_rate': 2.7241379310344827e-05, 'epoch': 4.843877273961445}\n{'loss': 1.6698, 'grad_norm': 2.0797624588012695, 'learning_rate': 2.7183908045977014e-05, 'epoch': 4.85473798533804}\n{'loss': 1.7581, 'grad_norm': 2.5322444438934326, 'learning_rate': 2.71264367816092e-05, 'epoch': 4.865598696714635}\n{'loss': 1.7443, 'grad_norm': 2.7955167293548584, 'learning_rate': 2.706896551724138e-05, 'epoch': 4.87645940809123}\n{'loss': 1.6509, 'grad_norm': 2.4034059047698975, 'learning_rate': 2.7011494252873566e-05, 'epoch': 4.887320119467825}\n{'eval_loss': 1.6176718473434448, 'eval_runtime': 12.4111, 'eval_samples_per_second': 65.909, 'eval_steps_per_second': 65.909, 'epoch': 4.887320119467825}\n{'loss': 1.7857, 'grad_norm': 2.102065324783325, 'learning_rate': 2.6954022988505752e-05, 'epoch': 4.898180830844421}\n{'loss': 1.6343, 'grad_norm': 1.8233970403671265, 'learning_rate': 2.689655172413793e-05, 'epoch': 4.909041542221016}\n{'loss': 1.7428, 'grad_norm': 1.8357595205307007, 'learning_rate': 2.6839080459770118e-05, 'epoch': 4.919902253597611}\n{'loss': 1.749, 'grad_norm': 1.9031652212142944, 'learning_rate': 2.67816091954023e-05, 'epoch': 4.930762964974206}\n{'loss': 1.6477, 'grad_norm': 2.373598337173462, 'learning_rate': 2.672413793103448e-05, 'epoch': 4.941623676350801}\n{'loss': 1.7124, 'grad_norm': 2.845341920852661, 'learning_rate': 2.6666666666666667e-05, 'epoch': 4.952484387727396}\n{'loss': 1.6647, 'grad_norm': 2.3062074184417725, 'learning_rate': 2.6609195402298853e-05, 'epoch': 4.963345099103991}\n{'loss': 1.684, 'grad_norm': 3.0479581356048584, 'learning_rate': 2.6551724137931032e-05, 'epoch': 4.974205810480586}\n{'loss': 1.7029, 'grad_norm': 2.5818800926208496, 'learning_rate': 2.649425287356322e-05, 'epoch': 4.985066521857181}\n{'loss': 1.7067, 'grad_norm': 2.8312625885009766, 'learning_rate': 2.6436781609195405e-05, 'epoch': 4.995927233233777}\n{'loss': 1.6936, 'grad_norm': 1.9863121509552002, 'learning_rate': 2.637931034482759e-05, 'epoch': 5.006787944610372}\n{'loss': 1.7463, 'grad_norm': 2.823765754699707, 'learning_rate': 2.632183908045977e-05, 'epoch': 5.017648655986967}\n{'loss': 1.6985, 'grad_norm': 2.115234851837158, 'learning_rate': 2.6264367816091957e-05, 'epoch': 5.028509367363562}\n{'loss': 1.6807, 'grad_norm': 2.152053117752075, 'learning_rate': 2.620689655172414e-05, 'epoch': 5.039370078740157}\n{'loss': 1.7131, 'grad_norm': 2.552766799926758, 'learning_rate': 2.6149425287356323e-05, 'epoch': 5.0502307901167525}\n{'loss': 1.6798, 'grad_norm': 2.6027088165283203, 'learning_rate': 2.6091954022988506e-05, 'epoch': 5.0610915014933475}\n{'loss': 1.6831, 'grad_norm': 2.605088233947754, 'learning_rate': 2.6034482758620692e-05, 'epoch': 5.071952212869943}\n{'loss': 1.8131, 'grad_norm': 2.383310317993164, 'learning_rate': 2.597701149425287e-05, 'epoch': 5.0828129242465385}\n{'loss': 1.7753, 'grad_norm': 2.22662353515625, 'learning_rate': 2.5919540229885058e-05, 'epoch': 5.093673635623134}\n{'loss': 1.7503, 'grad_norm': 2.065533399581909, 'learning_rate': 2.5862068965517244e-05, 'epoch': 5.104534346999729}\n{'loss': 1.7348, 'grad_norm': 2.758261203765869, 'learning_rate': 2.5804597701149424e-05, 'epoch': 5.115395058376324}\n{'loss': 1.6779, 'grad_norm': 2.090709686279297, 'learning_rate': 2.574712643678161e-05, 'epoch': 5.126255769752919}\n{'loss': 1.6835, 'grad_norm': 2.5266799926757812, 'learning_rate': 2.5689655172413796e-05, 'epoch': 5.137116481129514}\n{'loss': 1.6993, 'grad_norm': 2.492605447769165, 'learning_rate': 2.5632183908045976e-05, 'epoch': 5.147977192506109}\n{'loss': 1.6763, 'grad_norm': 1.8204779624938965, 'learning_rate': 2.5574712643678162e-05, 'epoch': 5.158837903882704}\n{'loss': 1.6651, 'grad_norm': 1.8460839986801147, 'learning_rate': 2.551724137931035e-05, 'epoch': 5.1696986152593}\n{'loss': 1.6538, 'grad_norm': 2.807906150817871, 'learning_rate': 2.545977011494253e-05, 'epoch': 5.180559326635895}\n{'loss': 1.7009, 'grad_norm': 2.213149309158325, 'learning_rate': 2.5402298850574714e-05, 'epoch': 5.19142003801249}\n{'loss': 1.6679, 'grad_norm': 2.3179688453674316, 'learning_rate': 2.5344827586206897e-05, 'epoch': 5.202280749389085}\n{'loss': 1.7683, 'grad_norm': 2.5999462604522705, 'learning_rate': 2.5287356321839083e-05, 'epoch': 5.21314146076568}\n{'loss': 1.754, 'grad_norm': 2.019397258758545, 'learning_rate': 2.5229885057471263e-05, 'epoch': 5.224002172142275}\n{'loss': 1.6082, 'grad_norm': 4.192168712615967, 'learning_rate': 2.517241379310345e-05, 'epoch': 5.23486288351887}\n{'loss': 1.7046, 'grad_norm': 2.227792978286743, 'learning_rate': 2.5114942528735635e-05, 'epoch': 5.245723594895465}\n{'loss': 1.6464, 'grad_norm': 2.2815537452697754, 'learning_rate': 2.5057471264367815e-05, 'epoch': 5.256584306272061}\n{'loss': 1.7309, 'grad_norm': 1.8407644033432007, 'learning_rate': 2.5e-05, 'epoch': 5.267445017648656}\n{'loss': 1.732, 'grad_norm': 5.162815570831299, 'learning_rate': 2.4942528735632184e-05, 'epoch': 5.278305729025251}\n{'loss': 1.7458, 'grad_norm': 3.7556352615356445, 'learning_rate': 2.488505747126437e-05, 'epoch': 5.289166440401846}\n{'loss': 1.707, 'grad_norm': 3.0230424404144287, 'learning_rate': 2.4827586206896553e-05, 'epoch': 5.300027151778441}\n{'loss': 1.7354, 'grad_norm': 2.439565896987915, 'learning_rate': 2.4770114942528736e-05, 'epoch': 5.3108878631550365}\n{'loss': 1.6383, 'grad_norm': 1.9206085205078125, 'learning_rate': 2.4712643678160922e-05, 'epoch': 5.3217485745316315}\n{'loss': 1.7539, 'grad_norm': 2.5939815044403076, 'learning_rate': 2.4655172413793105e-05, 'epoch': 5.332609285908227}\n{'loss': 1.6967, 'grad_norm': 8.334480285644531, 'learning_rate': 2.4597701149425288e-05, 'epoch': 5.3434699972848225}\n{'loss': 1.7383, 'grad_norm': 6.017273426055908, 'learning_rate': 2.454022988505747e-05, 'epoch': 5.354330708661418}\n{'loss': 1.6527, 'grad_norm': 2.309725284576416, 'learning_rate': 2.4482758620689654e-05, 'epoch': 5.365191420038013}\n{'loss': 1.7508, 'grad_norm': 2.0929908752441406, 'learning_rate': 2.442528735632184e-05, 'epoch': 5.376052131414608}\n{'loss': 1.7117, 'grad_norm': 2.3289167881011963, 'learning_rate': 2.4367816091954023e-05, 'epoch': 5.386912842791203}\n{'loss': 1.7197, 'grad_norm': 2.5079798698425293, 'learning_rate': 2.4310344827586206e-05, 'epoch': 5.397773554167798}\n{'loss': 1.5989, 'grad_norm': 2.2899162769317627, 'learning_rate': 2.4252873563218392e-05, 'epoch': 5.408634265544393}\n{'loss': 1.7119, 'grad_norm': 1.8892581462860107, 'learning_rate': 2.4195402298850575e-05, 'epoch': 5.419494976920988}\n{'loss': 1.6561, 'grad_norm': 2.3720107078552246, 'learning_rate': 2.413793103448276e-05, 'epoch': 5.430355688297584}\n{'eval_loss': 1.620119333267212, 'eval_runtime': 12.2815, 'eval_samples_per_second': 66.604, 'eval_steps_per_second': 66.604, 'epoch': 5.430355688297584}\n{'loss': 1.6915, 'grad_norm': 1.9474984407424927, 'learning_rate': 2.4080459770114945e-05, 'epoch': 5.441216399674179}\n{'loss': 1.7799, 'grad_norm': 2.637051820755005, 'learning_rate': 2.4022988505747127e-05, 'epoch': 5.452077111050774}\n{'loss': 1.8166, 'grad_norm': 2.304732084274292, 'learning_rate': 2.3965517241379314e-05, 'epoch': 5.462937822427369}\n{'loss': 1.6649, 'grad_norm': 1.6792863607406616, 'learning_rate': 2.3908045977011497e-05, 'epoch': 5.473798533803964}\n{'loss': 1.6797, 'grad_norm': 1.8136204481124878, 'learning_rate': 2.385057471264368e-05, 'epoch': 5.484659245180559}\n{'loss': 1.6126, 'grad_norm': 1.9566001892089844, 'learning_rate': 2.3793103448275862e-05, 'epoch': 5.495519956557154}\n{'loss': 1.7267, 'grad_norm': 1.9135068655014038, 'learning_rate': 2.3735632183908045e-05, 'epoch': 5.506380667933749}\n{'loss': 1.7211, 'grad_norm': 1.7820273637771606, 'learning_rate': 2.367816091954023e-05, 'epoch': 5.517241379310345}\n{'loss': 1.6785, 'grad_norm': 3.0457048416137695, 'learning_rate': 2.3620689655172415e-05, 'epoch': 5.52810209068694}\n{'loss': 1.7228, 'grad_norm': 2.4502320289611816, 'learning_rate': 2.3563218390804597e-05, 'epoch': 5.538962802063535}\n{'loss': 1.6834, 'grad_norm': 1.9331835508346558, 'learning_rate': 2.3505747126436784e-05, 'epoch': 5.54982351344013}\n{'loss': 1.7169, 'grad_norm': 2.518969774246216, 'learning_rate': 2.3448275862068967e-05, 'epoch': 5.560684224816725}\n{'loss': 1.6033, 'grad_norm': 2.4429547786712646, 'learning_rate': 2.339080459770115e-05, 'epoch': 5.5715449361933205}\n{'loss': 1.7671, 'grad_norm': 2.5348010063171387, 'learning_rate': 2.3333333333333336e-05, 'epoch': 5.5824056475699155}\n{'loss': 1.657, 'grad_norm': 2.6725263595581055, 'learning_rate': 2.327586206896552e-05, 'epoch': 5.5932663589465115}\n{'loss': 1.7026, 'grad_norm': 4.697482109069824, 'learning_rate': 2.3218390804597705e-05, 'epoch': 5.6041270703231065}\n{'loss': 1.7371, 'grad_norm': 2.5886623859405518, 'learning_rate': 2.3160919540229885e-05, 'epoch': 5.614987781699702}\n{'loss': 1.7026, 'grad_norm': 2.06467342376709, 'learning_rate': 2.3103448275862067e-05, 'epoch': 5.625848493076297}\n{'loss': 1.71, 'grad_norm': 2.149045705795288, 'learning_rate': 2.3045977011494254e-05, 'epoch': 5.636709204452892}\n{'loss': 1.5965, 'grad_norm': 2.253664255142212, 'learning_rate': 2.2988505747126437e-05, 'epoch': 5.647569915829487}\n{'loss': 1.6769, 'grad_norm': 2.671625852584839, 'learning_rate': 2.293103448275862e-05, 'epoch': 5.658430627206082}\n{'loss': 1.7012, 'grad_norm': 2.661832571029663, 'learning_rate': 2.2873563218390806e-05, 'epoch': 5.669291338582677}\n{'loss': 1.6492, 'grad_norm': 2.1958742141723633, 'learning_rate': 2.281609195402299e-05, 'epoch': 5.680152049959272}\n{'loss': 1.7469, 'grad_norm': 3.0774924755096436, 'learning_rate': 2.2758620689655175e-05, 'epoch': 5.691012761335868}\n{'loss': 1.5928, 'grad_norm': 7.0810651779174805, 'learning_rate': 2.2701149425287358e-05, 'epoch': 5.701873472712463}\n{'loss': 1.6582, 'grad_norm': 3.357069253921509, 'learning_rate': 2.264367816091954e-05, 'epoch': 5.712734184089058}\n{'loss': 1.7057, 'grad_norm': 2.5428967475891113, 'learning_rate': 2.2586206896551727e-05, 'epoch': 5.723594895465653}\n{'loss': 1.6763, 'grad_norm': 2.3065245151519775, 'learning_rate': 2.252873563218391e-05, 'epoch': 5.734455606842248}\n{'loss': 1.7243, 'grad_norm': 3.410984516143799, 'learning_rate': 2.2471264367816093e-05, 'epoch': 5.745316318218843}\n{'loss': 1.6456, 'grad_norm': 10.855371475219727, 'learning_rate': 2.2413793103448276e-05, 'epoch': 5.756177029595438}\n{'loss': 1.7106, 'grad_norm': 2.236286163330078, 'learning_rate': 2.235632183908046e-05, 'epoch': 5.767037740972034}\n{'loss': 1.7303, 'grad_norm': 2.495469331741333, 'learning_rate': 2.2298850574712645e-05, 'epoch': 5.777898452348629}\n{'loss': 1.7221, 'grad_norm': 2.016788959503174, 'learning_rate': 2.2241379310344828e-05, 'epoch': 5.788759163725224}\n{'loss': 1.6894, 'grad_norm': 2.0834641456604004, 'learning_rate': 2.218390804597701e-05, 'epoch': 5.799619875101819}\n{'loss': 1.6235, 'grad_norm': 2.0328047275543213, 'learning_rate': 2.2126436781609197e-05, 'epoch': 5.810480586478414}\n{'loss': 1.6398, 'grad_norm': 2.4630300998687744, 'learning_rate': 2.206896551724138e-05, 'epoch': 5.821341297855009}\n{'loss': 1.7524, 'grad_norm': 1.8469431400299072, 'learning_rate': 2.2011494252873563e-05, 'epoch': 5.8322020092316045}\n{'loss': 1.6871, 'grad_norm': 2.168952465057373, 'learning_rate': 2.195402298850575e-05, 'epoch': 5.8430627206081995}\n{'loss': 1.7469, 'grad_norm': 2.1515424251556396, 'learning_rate': 2.1896551724137932e-05, 'epoch': 5.853923431984795}\n{'loss': 1.5854, 'grad_norm': 6.664257049560547, 'learning_rate': 2.183908045977012e-05, 'epoch': 5.8647841433613905}\n{'loss': 1.7231, 'grad_norm': 1.955454707145691, 'learning_rate': 2.17816091954023e-05, 'epoch': 5.875644854737986}\n{'loss': 1.6452, 'grad_norm': 2.7942087650299072, 'learning_rate': 2.1724137931034484e-05, 'epoch': 5.886505566114581}\n{'loss': 1.6634, 'grad_norm': 2.5513501167297363, 'learning_rate': 2.1666666666666667e-05, 'epoch': 5.897366277491176}\n{'loss': 1.7489, 'grad_norm': 2.254944086074829, 'learning_rate': 2.160919540229885e-05, 'epoch': 5.908226988867771}\n{'loss': 1.7706, 'grad_norm': 6.608852863311768, 'learning_rate': 2.1551724137931033e-05, 'epoch': 5.919087700244366}\n{'loss': 1.6296, 'grad_norm': 2.028637409210205, 'learning_rate': 2.149425287356322e-05, 'epoch': 5.929948411620961}\n{'loss': 1.6771, 'grad_norm': 2.2367522716522217, 'learning_rate': 2.1436781609195402e-05, 'epoch': 5.940809122997557}\n{'loss': 1.6106, 'grad_norm': 2.2092525959014893, 'learning_rate': 2.137931034482759e-05, 'epoch': 5.951669834374152}\n{'loss': 1.7897, 'grad_norm': 2.220029592514038, 'learning_rate': 2.132183908045977e-05, 'epoch': 5.962530545750747}\n{'loss': 1.7126, 'grad_norm': 2.2047455310821533, 'learning_rate': 2.1264367816091954e-05, 'epoch': 5.973391257127342}\n{'eval_loss': 1.6118414402008057, 'eval_runtime': 12.5465, 'eval_samples_per_second': 65.197, 'eval_steps_per_second': 65.197, 'epoch': 5.973391257127342}\n{'loss': 1.6309, 'grad_norm': 2.157578229904175, 'learning_rate': 2.120689655172414e-05, 'epoch': 5.984251968503937}\n{'loss': 1.8054, 'grad_norm': 1.9753992557525635, 'learning_rate': 2.1149425287356323e-05, 'epoch': 5.995112679880532}\n{'loss': 1.6658, 'grad_norm': 2.9331443309783936, 'learning_rate': 2.1091954022988506e-05, 'epoch': 6.005973391257127}\n{'loss': 1.6795, 'grad_norm': 4.676813125610352, 'learning_rate': 2.1034482758620692e-05, 'epoch': 6.016834102633722}\n{'loss': 1.7554, 'grad_norm': 1.7496623992919922, 'learning_rate': 2.0977011494252875e-05, 'epoch': 6.027694814010317}\n{'loss': 1.6463, 'grad_norm': 2.0746469497680664, 'learning_rate': 2.0919540229885058e-05, 'epoch': 6.038555525386913}\n{'loss': 1.6884, 'grad_norm': 2.1443724632263184, 'learning_rate': 2.086206896551724e-05, 'epoch': 6.049416236763508}\n{'loss': 1.6556, 'grad_norm': 2.6546242237091064, 'learning_rate': 2.0804597701149424e-05, 'epoch': 6.060276948140103}\n{'loss': 1.6575, 'grad_norm': 3.057694435119629, 'learning_rate': 2.074712643678161e-05, 'epoch': 6.071137659516698}\n{'loss': 1.7085, 'grad_norm': 1.889382004737854, 'learning_rate': 2.0689655172413793e-05, 'epoch': 6.081998370893293}\n{'loss': 1.6459, 'grad_norm': 2.1962602138519287, 'learning_rate': 2.0632183908045976e-05, 'epoch': 6.0928590822698885}\n{'loss': 1.6481, 'grad_norm': 2.181753396987915, 'learning_rate': 2.0574712643678162e-05, 'epoch': 6.1037197936464835}\n{'loss': 1.6538, 'grad_norm': 3.2153921127319336, 'learning_rate': 2.0517241379310345e-05, 'epoch': 6.114580505023079}\n{'loss': 1.6969, 'grad_norm': 2.6837165355682373, 'learning_rate': 2.045977011494253e-05, 'epoch': 6.1254412163996745}\n{'loss': 1.6639, 'grad_norm': 2.1319196224212646, 'learning_rate': 2.0402298850574715e-05, 'epoch': 6.13630192777627}\n{'loss': 1.6859, 'grad_norm': 1.8265749216079712, 'learning_rate': 2.0344827586206897e-05, 'epoch': 6.147162639152865}\n{'loss': 1.5716, 'grad_norm': 1.9280425310134888, 'learning_rate': 2.0287356321839084e-05, 'epoch': 6.15802335052946}\n{'loss': 1.724, 'grad_norm': 2.4166629314422607, 'learning_rate': 2.0229885057471267e-05, 'epoch': 6.168884061906055}\n{'loss': 1.5749, 'grad_norm': 1.8227580785751343, 'learning_rate': 2.017241379310345e-05, 'epoch': 6.17974477328265}\n{'loss': 1.7765, 'grad_norm': 2.3220109939575195, 'learning_rate': 2.0114942528735632e-05, 'epoch': 6.190605484659245}\n{'loss': 1.6809, 'grad_norm': 2.3173506259918213, 'learning_rate': 2.0057471264367815e-05, 'epoch': 6.20146619603584}\n{'loss': 1.6691, 'grad_norm': 2.1456212997436523, 'learning_rate': 2e-05, 'epoch': 6.212326907412436}\n{'loss': 1.6793, 'grad_norm': 2.1855695247650146, 'learning_rate': 1.9942528735632185e-05, 'epoch': 6.223187618789031}\n{'loss': 1.639, 'grad_norm': 1.7443926334381104, 'learning_rate': 1.9885057471264367e-05, 'epoch': 6.234048330165626}\n{'loss': 1.7016, 'grad_norm': 2.450301170349121, 'learning_rate': 1.9827586206896554e-05, 'epoch': 6.244909041542221}\n{'loss': 1.7108, 'grad_norm': 2.021761417388916, 'learning_rate': 1.9770114942528737e-05, 'epoch': 6.255769752918816}\n{'loss': 1.6896, 'grad_norm': 2.1316709518432617, 'learning_rate': 1.971264367816092e-05, 'epoch': 6.266630464295411}\n{'loss': 1.6678, 'grad_norm': 2.286186695098877, 'learning_rate': 1.9655172413793106e-05, 'epoch': 6.277491175672006}\n{'loss': 1.5635, 'grad_norm': 2.235567092895508, 'learning_rate': 1.959770114942529e-05, 'epoch': 6.288351887048602}\n{'loss': 1.6086, 'grad_norm': 2.2054357528686523, 'learning_rate': 1.9540229885057475e-05, 'epoch': 6.299212598425197}\n{'loss': 1.665, 'grad_norm': 2.435260772705078, 'learning_rate': 1.9482758620689655e-05, 'epoch': 6.310073309801792}\n{'loss': 1.6717, 'grad_norm': 2.256653308868408, 'learning_rate': 1.9425287356321837e-05, 'epoch': 6.320934021178387}\n{'loss': 1.7766, 'grad_norm': 2.6513280868530273, 'learning_rate': 1.9367816091954024e-05, 'epoch': 6.331794732554982}\n{'loss': 1.623, 'grad_norm': 2.4044461250305176, 'learning_rate': 1.9310344827586207e-05, 'epoch': 6.342655443931577}\n{'loss': 1.6877, 'grad_norm': 1.8253790140151978, 'learning_rate': 1.925287356321839e-05, 'epoch': 6.3535161553081725}\n{'loss': 1.7225, 'grad_norm': 1.6635562181472778, 'learning_rate': 1.9195402298850576e-05, 'epoch': 6.3643768666847675}\n{'loss': 1.7094, 'grad_norm': 2.0949902534484863, 'learning_rate': 1.913793103448276e-05, 'epoch': 6.375237578061363}\n{'loss': 1.6352, 'grad_norm': 1.9330706596374512, 'learning_rate': 1.9080459770114945e-05, 'epoch': 6.3860982894379585}\n{'loss': 1.6568, 'grad_norm': 2.2598719596862793, 'learning_rate': 1.9022988505747128e-05, 'epoch': 6.396959000814554}\n{'loss': 1.673, 'grad_norm': 2.5032660961151123, 'learning_rate': 1.896551724137931e-05, 'epoch': 6.407819712191149}\n{'loss': 1.7558, 'grad_norm': 2.2381396293640137, 'learning_rate': 1.8908045977011497e-05, 'epoch': 6.418680423567744}\n{'loss': 1.7495, 'grad_norm': 2.461520195007324, 'learning_rate': 1.885057471264368e-05, 'epoch': 6.429541134944339}\n{'loss': 1.706, 'grad_norm': 2.826395273208618, 'learning_rate': 1.8793103448275863e-05, 'epoch': 6.440401846320934}\n{'loss': 1.6994, 'grad_norm': 2.3304009437561035, 'learning_rate': 1.8735632183908046e-05, 'epoch': 6.451262557697529}\n{'loss': 1.6358, 'grad_norm': 2.465817451477051, 'learning_rate': 1.867816091954023e-05, 'epoch': 6.462123269074125}\n{'loss': 1.655, 'grad_norm': 2.156477451324463, 'learning_rate': 1.8620689655172415e-05, 'epoch': 6.47298398045072}\n{'loss': 1.6627, 'grad_norm': 2.8002679347991943, 'learning_rate': 1.8563218390804598e-05, 'epoch': 6.483844691827315}\n{'loss': 1.687, 'grad_norm': 2.4817914962768555, 'learning_rate': 1.850574712643678e-05, 'epoch': 6.49470540320391}\n{'loss': 1.6559, 'grad_norm': 2.399534225463867, 'learning_rate': 1.8448275862068967e-05, 'epoch': 6.505566114580505}\n{'loss': 1.706, 'grad_norm': 2.1909399032592773, 'learning_rate': 1.839080459770115e-05, 'epoch': 6.5164268259571}\n{'eval_loss': 1.6114379167556763, 'eval_runtime': 12.5026, 'eval_samples_per_second': 65.427, 'eval_steps_per_second': 65.427, 'epoch': 6.5164268259571}\n{'loss': 1.7046, 'grad_norm': 1.7235376834869385, 'learning_rate': 1.8333333333333333e-05, 'epoch': 6.527287537333695}\n{'loss': 1.7121, 'grad_norm': 2.3260910511016846, 'learning_rate': 1.827586206896552e-05, 'epoch': 6.53814824871029}\n{'loss': 1.713, 'grad_norm': 2.3345582485198975, 'learning_rate': 1.8218390804597702e-05, 'epoch': 6.549008960086885}\n{'loss': 1.6227, 'grad_norm': 2.2034263610839844, 'learning_rate': 1.816091954022989e-05, 'epoch': 6.559869671463481}\n{'loss': 1.7638, 'grad_norm': 2.5434389114379883, 'learning_rate': 1.810344827586207e-05, 'epoch': 6.570730382840076}\n{'loss': 1.7264, 'grad_norm': 2.891204357147217, 'learning_rate': 1.8045977011494254e-05, 'epoch': 6.581591094216671}\n{'loss': 1.6358, 'grad_norm': 3.203864097595215, 'learning_rate': 1.7988505747126437e-05, 'epoch': 6.592451805593266}\n{'loss': 1.6964, 'grad_norm': 2.736905336380005, 'learning_rate': 1.793103448275862e-05, 'epoch': 6.603312516969861}\n{'loss': 1.6695, 'grad_norm': 2.132498264312744, 'learning_rate': 1.7873563218390803e-05, 'epoch': 6.6141732283464565}\n{'loss': 1.701, 'grad_norm': 1.792603611946106, 'learning_rate': 1.781609195402299e-05, 'epoch': 6.6250339397230515}\n{'loss': 1.6793, 'grad_norm': 1.8259152173995972, 'learning_rate': 1.7758620689655172e-05, 'epoch': 6.6358946510996475}\n{'loss': 1.5829, 'grad_norm': 2.3920915126800537, 'learning_rate': 1.770114942528736e-05, 'epoch': 6.6467553624762425}\n{'loss': 1.6534, 'grad_norm': 3.8405885696411133, 'learning_rate': 1.764367816091954e-05, 'epoch': 6.657616073852838}\n{'loss': 1.6558, 'grad_norm': 2.541423797607422, 'learning_rate': 1.7586206896551724e-05, 'epoch': 6.668476785229433}\n{'loss': 1.7246, 'grad_norm': 4.319313049316406, 'learning_rate': 1.752873563218391e-05, 'epoch': 6.679337496606028}\n{'loss': 1.6663, 'grad_norm': 1.8755711317062378, 'learning_rate': 1.7471264367816093e-05, 'epoch': 6.690198207982623}\n{'loss': 1.68, 'grad_norm': 2.2843399047851562, 'learning_rate': 1.7413793103448276e-05, 'epoch': 6.701058919359218}\n{'loss': 1.587, 'grad_norm': 2.6386239528656006, 'learning_rate': 1.7356321839080462e-05, 'epoch': 6.711919630735813}\n{'loss': 1.7025, 'grad_norm': 8.227981567382812, 'learning_rate': 1.7298850574712645e-05, 'epoch': 6.722780342112408}\n{'loss': 1.6177, 'grad_norm': 2.411865234375, 'learning_rate': 1.7241379310344828e-05, 'epoch': 6.733641053489004}\n{'loss': 1.6644, 'grad_norm': 2.0615947246551514, 'learning_rate': 1.718390804597701e-05, 'epoch': 6.744501764865599}\n{'loss': 1.6701, 'grad_norm': 1.9814558029174805, 'learning_rate': 1.7126436781609194e-05, 'epoch': 6.755362476242194}\n{'loss': 1.7354, 'grad_norm': 5.0901198387146, 'learning_rate': 1.706896551724138e-05, 'epoch': 6.766223187618789}\n{'loss': 1.6807, 'grad_norm': 2.6006102561950684, 'learning_rate': 1.7011494252873563e-05, 'epoch': 6.777083898995384}\n{'loss': 1.6861, 'grad_norm': 2.4915597438812256, 'learning_rate': 1.6954022988505746e-05, 'epoch': 6.787944610371979}\n{'loss': 1.7077, 'grad_norm': 2.325524091720581, 'learning_rate': 1.6896551724137932e-05, 'epoch': 6.798805321748574}\n{'loss': 1.6895, 'grad_norm': 2.256077289581299, 'learning_rate': 1.6839080459770115e-05, 'epoch': 6.80966603312517}\n{'loss': 1.6653, 'grad_norm': 1.8998725414276123, 'learning_rate': 1.67816091954023e-05, 'epoch': 6.820526744501765}\n{'loss': 1.6787, 'grad_norm': 1.9596245288848877, 'learning_rate': 1.6724137931034485e-05, 'epoch': 6.83138745587836}\n{'loss': 1.641, 'grad_norm': 2.7131824493408203, 'learning_rate': 1.6666666666666667e-05, 'epoch': 6.842248167254955}\n{'loss': 1.706, 'grad_norm': 2.6876397132873535, 'learning_rate': 1.6609195402298854e-05, 'epoch': 6.85310887863155}\n{'loss': 1.7036, 'grad_norm': 1.8532524108886719, 'learning_rate': 1.6551724137931037e-05, 'epoch': 6.863969590008145}\n{'loss': 1.693, 'grad_norm': 2.1784491539001465, 'learning_rate': 1.649425287356322e-05, 'epoch': 6.8748303013847405}\n{'loss': 1.7055, 'grad_norm': 1.9177865982055664, 'learning_rate': 1.6436781609195402e-05, 'epoch': 6.8856910127613355}\n{'loss': 1.6211, 'grad_norm': 1.8130015134811401, 'learning_rate': 1.6379310344827585e-05, 'epoch': 6.896551724137931}\n{'loss': 1.7377, 'grad_norm': 2.4548423290252686, 'learning_rate': 1.632183908045977e-05, 'epoch': 6.9074124355145265}\n{'loss': 1.7095, 'grad_norm': 2.0234172344207764, 'learning_rate': 1.6264367816091955e-05, 'epoch': 6.918273146891122}\n{'loss': 1.7409, 'grad_norm': 2.5982561111450195, 'learning_rate': 1.6206896551724137e-05, 'epoch': 6.929133858267717}\n{'loss': 1.712, 'grad_norm': 2.7227141857147217, 'learning_rate': 1.6149425287356324e-05, 'epoch': 6.939994569644312}\n{'loss': 1.5613, 'grad_norm': 2.2463691234588623, 'learning_rate': 1.6091954022988507e-05, 'epoch': 6.950855281020907}\n{'loss': 1.6593, 'grad_norm': 2.977383613586426, 'learning_rate': 1.603448275862069e-05, 'epoch': 6.961715992397502}\n{'loss': 1.7167, 'grad_norm': 2.9290354251861572, 'learning_rate': 1.5977011494252876e-05, 'epoch': 6.972576703774097}\n{'loss': 1.6873, 'grad_norm': 3.304988384246826, 'learning_rate': 1.591954022988506e-05, 'epoch': 6.983437415150693}\n{'loss': 1.5896, 'grad_norm': 1.9939168691635132, 'learning_rate': 1.586206896551724e-05, 'epoch': 6.994298126527288}\n{'loss': 1.6282, 'grad_norm': 2.642059564590454, 'learning_rate': 1.5804597701149425e-05, 'epoch': 7.005158837903883}\n{'loss': 1.6539, 'grad_norm': 2.177570343017578, 'learning_rate': 1.5747126436781607e-05, 'epoch': 7.016019549280478}\n{'loss': 1.6574, 'grad_norm': 4.915096282958984, 'learning_rate': 1.5689655172413794e-05, 'epoch': 7.026880260657073}\n{'loss': 1.729, 'grad_norm': 2.7530922889709473, 'learning_rate': 1.5632183908045977e-05, 'epoch': 7.037740972033668}\n{'loss': 1.5597, 'grad_norm': 2.1246140003204346, 'learning_rate': 1.557471264367816e-05, 'epoch': 7.048601683410263}\n{'loss': 1.6094, 'grad_norm': 2.2663846015930176, 'learning_rate': 1.5517241379310346e-05, 'epoch': 7.059462394786858}\n{'eval_loss': 1.60762357711792, 'eval_runtime': 12.3405, 'eval_samples_per_second': 66.286, 'eval_steps_per_second': 66.286, 'epoch': 7.059462394786858}\n{'loss': 1.6843, 'grad_norm': 2.299710512161255, 'learning_rate': 1.545977011494253e-05, 'epoch': 7.070323106163453}\n{'loss': 1.7122, 'grad_norm': 2.0407607555389404, 'learning_rate': 1.5402298850574715e-05, 'epoch': 7.081183817540049}\n{'loss': 1.7308, 'grad_norm': 2.516174554824829, 'learning_rate': 1.5344827586206898e-05, 'epoch': 7.092044528916644}\n{'loss': 1.6299, 'grad_norm': 2.2609615325927734, 'learning_rate': 1.528735632183908e-05, 'epoch': 7.102905240293239}\n{'loss': 1.6924, 'grad_norm': 2.740635633468628, 'learning_rate': 1.5229885057471265e-05, 'epoch': 7.113765951669834}\n{'loss': 1.6588, 'grad_norm': 2.7894084453582764, 'learning_rate': 1.5172413793103448e-05, 'epoch': 7.124626663046429}\n{'loss': 1.6111, 'grad_norm': 2.1063365936279297, 'learning_rate': 1.5114942528735631e-05, 'epoch': 7.1354873744230245}\n{'loss': 1.6644, 'grad_norm': 1.9608083963394165, 'learning_rate': 1.5057471264367817e-05, 'epoch': 7.1463480857996196}\n{'loss': 1.6209, 'grad_norm': 3.2653415203094482, 'learning_rate': 1.5e-05, 'epoch': 7.1572087971762155}\n{'loss': 1.5471, 'grad_norm': 2.2540366649627686, 'learning_rate': 1.4942528735632185e-05, 'epoch': 7.1680695085528106}\n{'loss': 1.6197, 'grad_norm': 2.417245388031006, 'learning_rate': 1.4885057471264368e-05, 'epoch': 7.178930219929406}\n{'loss': 1.6674, 'grad_norm': 2.3348329067230225, 'learning_rate': 1.482758620689655e-05, 'epoch': 7.189790931306001}\n{'loss': 1.7197, 'grad_norm': 2.4401698112487793, 'learning_rate': 1.4770114942528737e-05, 'epoch': 7.200651642682596}\n{'loss': 1.6581, 'grad_norm': 3.407738208770752, 'learning_rate': 1.471264367816092e-05, 'epoch': 7.211512354059191}\n{'loss': 1.6123, 'grad_norm': 2.251591205596924, 'learning_rate': 1.4655172413793103e-05, 'epoch': 7.222373065435786}\n{'loss': 1.6489, 'grad_norm': 2.4418985843658447, 'learning_rate': 1.459770114942529e-05, 'epoch': 7.233233776812381}\n{'loss': 1.5963, 'grad_norm': 2.4370434284210205, 'learning_rate': 1.4540229885057472e-05, 'epoch': 7.244094488188976}\n{'loss': 1.6279, 'grad_norm': 2.7511839866638184, 'learning_rate': 1.4482758620689657e-05, 'epoch': 7.254955199565572}\n{'loss': 1.6514, 'grad_norm': 1.9339872598648071, 'learning_rate': 1.442528735632184e-05, 'epoch': 7.265815910942167}\n{'loss': 1.6556, 'grad_norm': 2.8461616039276123, 'learning_rate': 1.4367816091954022e-05, 'epoch': 7.276676622318762}\n{'loss': 1.6077, 'grad_norm': 2.9059929847717285, 'learning_rate': 1.4310344827586209e-05, 'epoch': 7.287537333695357}\n{'loss': 1.6583, 'grad_norm': 2.584618091583252, 'learning_rate': 1.4252873563218392e-05, 'epoch': 7.298398045071952}\n{'loss': 1.6824, 'grad_norm': 2.590555429458618, 'learning_rate': 1.4195402298850575e-05, 'epoch': 7.309258756448547}\n{'loss': 1.6826, 'grad_norm': 2.235137701034546, 'learning_rate': 1.4137931034482759e-05, 'epoch': 7.320119467825142}\n{'loss': 1.7191, 'grad_norm': 3.534608840942383, 'learning_rate': 1.4080459770114942e-05, 'epoch': 7.330980179201738}\n{'loss': 1.6299, 'grad_norm': 4.0142130851745605, 'learning_rate': 1.4022988505747128e-05, 'epoch': 7.341840890578333}\n{'loss': 1.7304, 'grad_norm': 2.147399663925171, 'learning_rate': 1.3965517241379311e-05, 'epoch': 7.352701601954928}\n{'loss': 1.6928, 'grad_norm': 3.0691964626312256, 'learning_rate': 1.3908045977011494e-05, 'epoch': 7.363562313331523}\n{'loss': 1.6591, 'grad_norm': 2.3920998573303223, 'learning_rate': 1.385057471264368e-05, 'epoch': 7.374423024708118}\n{'loss': 1.659, 'grad_norm': 1.9790304899215698, 'learning_rate': 1.3793103448275863e-05, 'epoch': 7.3852837360847134}\n{'loss': 1.6993, 'grad_norm': 2.1215667724609375, 'learning_rate': 1.3735632183908046e-05, 'epoch': 7.3961444474613085}\n{'loss': 1.653, 'grad_norm': 2.569206476211548, 'learning_rate': 1.367816091954023e-05, 'epoch': 7.407005158837904}\n{'loss': 1.703, 'grad_norm': 2.8834121227264404, 'learning_rate': 1.3620689655172414e-05, 'epoch': 7.417865870214499}\n{'loss': 1.7406, 'grad_norm': 2.5514352321624756, 'learning_rate': 1.35632183908046e-05, 'epoch': 7.428726581591095}\n{'loss': 1.6088, 'grad_norm': 2.7812578678131104, 'learning_rate': 1.3505747126436783e-05, 'epoch': 7.43958729296769}\n{'loss': 1.6928, 'grad_norm': 2.401961326599121, 'learning_rate': 1.3448275862068966e-05, 'epoch': 7.450448004344285}\n{'loss': 1.693, 'grad_norm': 10.545409202575684, 'learning_rate': 1.339080459770115e-05, 'epoch': 7.46130871572088}\n{'loss': 1.6193, 'grad_norm': 2.409998893737793, 'learning_rate': 1.3333333333333333e-05, 'epoch': 7.472169427097475}\n{'loss': 1.6864, 'grad_norm': 2.5439376831054688, 'learning_rate': 1.3275862068965516e-05, 'epoch': 7.48303013847407}\n{'loss': 1.715, 'grad_norm': 2.5504813194274902, 'learning_rate': 1.3218390804597702e-05, 'epoch': 7.493890849850665}\n{'loss': 1.7214, 'grad_norm': 2.7418441772460938, 'learning_rate': 1.3160919540229885e-05, 'epoch': 7.504751561227261}\n{'loss': 1.6382, 'grad_norm': 3.2522525787353516, 'learning_rate': 1.310344827586207e-05, 'epoch': 7.515612272603856}\n{'loss': 1.6192, 'grad_norm': 1.8137071132659912, 'learning_rate': 1.3045977011494253e-05, 'epoch': 7.526472983980451}\n{'loss': 1.6037, 'grad_norm': 2.4373388290405273, 'learning_rate': 1.2988505747126436e-05, 'epoch': 7.537333695357046}\n{'loss': 1.683, 'grad_norm': 2.350083589553833, 'learning_rate': 1.2931034482758622e-05, 'epoch': 7.548194406733641}\n{'loss': 1.7057, 'grad_norm': 2.378955125808716, 'learning_rate': 1.2873563218390805e-05, 'epoch': 7.559055118110236}\n{'loss': 1.6504, 'grad_norm': 2.3532004356384277, 'learning_rate': 1.2816091954022988e-05, 'epoch': 7.569915829486831}\n{'loss': 1.635, 'grad_norm': 1.9954906702041626, 'learning_rate': 1.2758620689655174e-05, 'epoch': 7.580776540863426}\n{'loss': 1.7042, 'grad_norm': 1.9017016887664795, 'learning_rate': 1.2701149425287357e-05, 'epoch': 7.591637252240021}\n{'loss': 1.5742, 'grad_norm': 2.3290040493011475, 'learning_rate': 1.2643678160919542e-05, 'epoch': 7.602497963616617}\n{'eval_loss': 1.6090424060821533, 'eval_runtime': 12.3146, 'eval_samples_per_second': 66.425, 'eval_steps_per_second': 66.425, 'epoch': 7.602497963616617}\n{'loss': 1.6994, 'grad_norm': 2.1531214714050293, 'learning_rate': 1.2586206896551725e-05, 'epoch': 7.613358674993212}\n{'loss': 1.6211, 'grad_norm': 2.1489293575286865, 'learning_rate': 1.2528735632183907e-05, 'epoch': 7.624219386369807}\n{'loss': 1.6107, 'grad_norm': 2.1633353233337402, 'learning_rate': 1.2471264367816092e-05, 'epoch': 7.635080097746402}\n{'loss': 1.7126, 'grad_norm': 3.109980583190918, 'learning_rate': 1.2413793103448277e-05, 'epoch': 7.6459408091229975}\n{'loss': 1.7105, 'grad_norm': 2.121387243270874, 'learning_rate': 1.2356321839080461e-05, 'epoch': 7.6568015204995925}\n{'loss': 1.6658, 'grad_norm': 1.6252882480621338, 'learning_rate': 1.2298850574712644e-05, 'epoch': 7.667662231876188}\n{'loss': 1.7288, 'grad_norm': 2.2965917587280273, 'learning_rate': 1.2241379310344827e-05, 'epoch': 7.6785229432527835}\n{'loss': 1.5824, 'grad_norm': 1.8709241151809692, 'learning_rate': 1.2183908045977012e-05, 'epoch': 7.689383654629379}\n{'loss': 1.6778, 'grad_norm': 2.261021375656128, 'learning_rate': 1.2126436781609196e-05, 'epoch': 7.700244366005974}\n{'loss': 1.7005, 'grad_norm': 1.8852204084396362, 'learning_rate': 1.206896551724138e-05, 'epoch': 7.711105077382569}\n{'loss': 1.6221, 'grad_norm': 2.599719285964966, 'learning_rate': 1.2011494252873564e-05, 'epoch': 7.721965788759164}\n{'loss': 1.615, 'grad_norm': 1.7559480667114258, 'learning_rate': 1.1954022988505748e-05, 'epoch': 7.732826500135759}\n{'loss': 1.732, 'grad_norm': 2.949965238571167, 'learning_rate': 1.1896551724137931e-05, 'epoch': 7.743687211512354}\n{'loss': 1.6855, 'grad_norm': 1.9221291542053223, 'learning_rate': 1.1839080459770116e-05, 'epoch': 7.754547922888949}\n{'loss': 1.6286, 'grad_norm': 2.5133438110351562, 'learning_rate': 1.1781609195402299e-05, 'epoch': 7.765408634265544}\n{'loss': 1.6885, 'grad_norm': 2.4263153076171875, 'learning_rate': 1.1724137931034483e-05, 'epoch': 7.77626934564214}\n{'loss': 1.6263, 'grad_norm': 2.021392822265625, 'learning_rate': 1.1666666666666668e-05, 'epoch': 7.787130057018735}\n{'loss': 1.7305, 'grad_norm': 2.013659954071045, 'learning_rate': 1.1609195402298852e-05, 'epoch': 7.79799076839533}\n{'loss': 1.6531, 'grad_norm': 2.3870465755462646, 'learning_rate': 1.1551724137931034e-05, 'epoch': 7.808851479771925}\n{'loss': 1.6057, 'grad_norm': 2.2465908527374268, 'learning_rate': 1.1494252873563218e-05, 'epoch': 7.81971219114852}\n{'loss': 1.6408, 'grad_norm': 1.9554179906845093, 'learning_rate': 1.1436781609195403e-05, 'epoch': 7.830572902525115}\n{'loss': 1.7076, 'grad_norm': 2.3586344718933105, 'learning_rate': 1.1379310344827587e-05, 'epoch': 7.84143361390171}\n{'loss': 1.6342, 'grad_norm': 2.1095595359802246, 'learning_rate': 1.132183908045977e-05, 'epoch': 7.852294325278306}\n{'loss': 1.6695, 'grad_norm': 1.9481987953186035, 'learning_rate': 1.1264367816091955e-05, 'epoch': 7.863155036654901}\n{'loss': 1.596, 'grad_norm': 2.224159002304077, 'learning_rate': 1.1206896551724138e-05, 'epoch': 7.874015748031496}\n{'loss': 1.7148, 'grad_norm': 1.922316074371338, 'learning_rate': 1.1149425287356322e-05, 'epoch': 7.884876459408091}\n{'loss': 1.6094, 'grad_norm': 2.007949113845825, 'learning_rate': 1.1091954022988505e-05, 'epoch': 7.895737170784686}\n{'loss': 1.6547, 'grad_norm': 4.877739429473877, 'learning_rate': 1.103448275862069e-05, 'epoch': 7.9065978821612815}\n{'loss': 1.6379, 'grad_norm': 2.268646478652954, 'learning_rate': 1.0977011494252875e-05, 'epoch': 7.9174585935378765}\n{'loss': 1.6435, 'grad_norm': 2.1965694427490234, 'learning_rate': 1.091954022988506e-05, 'epoch': 7.928319304914472}\n{'loss': 1.699, 'grad_norm': 2.4357290267944336, 'learning_rate': 1.0862068965517242e-05, 'epoch': 7.939180016291067}\n{'loss': 1.6503, 'grad_norm': 2.626326560974121, 'learning_rate': 1.0804597701149425e-05, 'epoch': 7.950040727667663}\n{'loss': 1.683, 'grad_norm': 2.357497453689575, 'learning_rate': 1.074712643678161e-05, 'epoch': 7.960901439044258}\n{'loss': 1.6766, 'grad_norm': 2.150768280029297, 'learning_rate': 1.0689655172413794e-05, 'epoch': 7.971762150420853}\n{'loss': 1.7465, 'grad_norm': 1.805051326751709, 'learning_rate': 1.0632183908045977e-05, 'epoch': 7.982622861797448}\n{'loss': 1.686, 'grad_norm': 2.5640811920166016, 'learning_rate': 1.0574712643678162e-05, 'epoch': 7.993483573174043}\n{'loss': 1.6407, 'grad_norm': 2.3233096599578857, 'learning_rate': 1.0517241379310346e-05, 'epoch': 8.004344284550639}\n{'loss': 1.6168, 'grad_norm': 2.401501417160034, 'learning_rate': 1.0459770114942529e-05, 'epoch': 8.015204995927233}\n{'loss': 1.6592, 'grad_norm': 2.00836443901062, 'learning_rate': 1.0402298850574712e-05, 'epoch': 8.026065707303829}\n{'loss': 1.6658, 'grad_norm': 2.684126853942871, 'learning_rate': 1.0344827586206897e-05, 'epoch': 8.036926418680423}\n{'loss': 1.6157, 'grad_norm': 2.236551523208618, 'learning_rate': 1.0287356321839081e-05, 'epoch': 8.047787130057019}\n{'loss': 1.5993, 'grad_norm': 2.553452968597412, 'learning_rate': 1.0229885057471266e-05, 'epoch': 8.058647841433613}\n{'loss': 1.6691, 'grad_norm': 2.6372861862182617, 'learning_rate': 1.0172413793103449e-05, 'epoch': 8.069508552810209}\n{'loss': 1.6462, 'grad_norm': 2.960259199142456, 'learning_rate': 1.0114942528735633e-05, 'epoch': 8.080369264186805}\n{'loss': 1.576, 'grad_norm': 2.09763503074646, 'learning_rate': 1.0057471264367816e-05, 'epoch': 8.0912299755634}\n{'loss': 1.6478, 'grad_norm': 4.932857036590576, 'learning_rate': 1e-05, 'epoch': 8.102090686939995}\n{'loss': 1.6404, 'grad_norm': 1.8704952001571655, 'learning_rate': 9.942528735632184e-06, 'epoch': 8.11295139831659}\n{'loss': 1.6426, 'grad_norm': 1.8853448629379272, 'learning_rate': 9.885057471264368e-06, 'epoch': 8.123812109693185}\n{'loss': 1.565, 'grad_norm': 2.1071152687072754, 'learning_rate': 9.827586206896553e-06, 'epoch': 8.13467282106978}\n{'loss': 1.6885, 'grad_norm': 2.812948703765869, 'learning_rate': 9.770114942528738e-06, 'epoch': 8.145533532446375}\n{'eval_loss': 1.6036611795425415, 'eval_runtime': 12.3949, 'eval_samples_per_second': 65.995, 'eval_steps_per_second': 65.995, 'epoch': 8.145533532446375}\n{'loss': 1.7132, 'grad_norm': 2.13448166847229, 'learning_rate': 9.712643678160919e-06, 'epoch': 8.15639424382297}\n{'loss': 1.6278, 'grad_norm': 2.9441864490509033, 'learning_rate': 9.655172413793103e-06, 'epoch': 8.167254955199565}\n{'loss': 1.6499, 'grad_norm': 1.960157036781311, 'learning_rate': 9.597701149425288e-06, 'epoch': 8.178115666576161}\n{'loss': 1.6308, 'grad_norm': 2.5990946292877197, 'learning_rate': 9.540229885057472e-06, 'epoch': 8.188976377952756}\n{'loss': 1.6805, 'grad_norm': 1.794532299041748, 'learning_rate': 9.482758620689655e-06, 'epoch': 8.199837089329352}\n{'loss': 1.6748, 'grad_norm': 2.1307225227355957, 'learning_rate': 9.42528735632184e-06, 'epoch': 8.210697800705946}\n{'loss': 1.695, 'grad_norm': 2.0753066539764404, 'learning_rate': 9.367816091954023e-06, 'epoch': 8.221558512082542}\n{'loss': 1.59, 'grad_norm': 2.378068685531616, 'learning_rate': 9.310344827586207e-06, 'epoch': 8.232419223459136}\n{'loss': 1.7071, 'grad_norm': 2.070343017578125, 'learning_rate': 9.25287356321839e-06, 'epoch': 8.243279934835732}\n{'loss': 1.6725, 'grad_norm': 2.305415153503418, 'learning_rate': 9.195402298850575e-06, 'epoch': 8.254140646212328}\n{'loss': 1.7531, 'grad_norm': 1.897918939590454, 'learning_rate': 9.13793103448276e-06, 'epoch': 8.265001357588922}\n{'loss': 1.5932, 'grad_norm': 1.9145811796188354, 'learning_rate': 9.080459770114944e-06, 'epoch': 8.275862068965518}\n{'loss': 1.6385, 'grad_norm': 2.8477420806884766, 'learning_rate': 9.022988505747127e-06, 'epoch': 8.286722780342112}\n{'loss': 1.6607, 'grad_norm': 2.191319227218628, 'learning_rate': 8.96551724137931e-06, 'epoch': 8.297583491718708}\n{'loss': 1.6532, 'grad_norm': 2.397825241088867, 'learning_rate': 8.908045977011495e-06, 'epoch': 8.308444203095302}\n{'loss': 1.6498, 'grad_norm': 3.658475875854492, 'learning_rate': 8.85057471264368e-06, 'epoch': 8.319304914471898}\n{'loss': 1.5623, 'grad_norm': 3.3398635387420654, 'learning_rate': 8.793103448275862e-06, 'epoch': 8.330165625848494}\n{'loss': 1.6644, 'grad_norm': 2.904554843902588, 'learning_rate': 8.735632183908047e-06, 'epoch': 8.341026337225088}\n{'loss': 1.6997, 'grad_norm': 2.4205434322357178, 'learning_rate': 8.678160919540231e-06, 'epoch': 8.351887048601684}\n{'loss': 1.7312, 'grad_norm': 1.6886825561523438, 'learning_rate': 8.620689655172414e-06, 'epoch': 8.362747759978278}\n{'loss': 1.6509, 'grad_norm': 2.039377212524414, 'learning_rate': 8.563218390804597e-06, 'epoch': 8.373608471354874}\n{'loss': 1.6358, 'grad_norm': 2.283060073852539, 'learning_rate': 8.505747126436782e-06, 'epoch': 8.384469182731468}\n{'loss': 1.5063, 'grad_norm': 2.0239810943603516, 'learning_rate': 8.448275862068966e-06, 'epoch': 8.395329894108064}\n{'loss': 1.5469, 'grad_norm': 2.14774489402771, 'learning_rate': 8.39080459770115e-06, 'epoch': 8.406190605484658}\n{'loss': 1.6725, 'grad_norm': 2.1114375591278076, 'learning_rate': 8.333333333333334e-06, 'epoch': 8.417051316861254}\n{'loss': 1.6524, 'grad_norm': 1.6950273513793945, 'learning_rate': 8.275862068965518e-06, 'epoch': 8.42791202823785}\n{'loss': 1.7145, 'grad_norm': 15.88107967376709, 'learning_rate': 8.218390804597701e-06, 'epoch': 8.438772739614445}\n{'loss': 1.5818, 'grad_norm': 2.3420472145080566, 'learning_rate': 8.160919540229886e-06, 'epoch': 8.44963345099104}\n{'loss': 1.6775, 'grad_norm': 2.062798500061035, 'learning_rate': 8.103448275862069e-06, 'epoch': 8.460494162367635}\n{'loss': 1.6625, 'grad_norm': 2.5743885040283203, 'learning_rate': 8.045977011494253e-06, 'epoch': 8.47135487374423}\n{'loss': 1.6468, 'grad_norm': 2.512429714202881, 'learning_rate': 7.988505747126438e-06, 'epoch': 8.482215585120825}\n{'loss': 1.7325, 'grad_norm': 2.605205535888672, 'learning_rate': 7.93103448275862e-06, 'epoch': 8.49307629649742}\n{'loss': 1.6878, 'grad_norm': 2.254368782043457, 'learning_rate': 7.873563218390804e-06, 'epoch': 8.503937007874015}\n{'loss': 1.6418, 'grad_norm': 1.879336953163147, 'learning_rate': 7.816091954022988e-06, 'epoch': 8.51479771925061}\n{'loss': 1.5904, 'grad_norm': 2.1786270141601562, 'learning_rate': 7.758620689655173e-06, 'epoch': 8.525658430627207}\n{'loss': 1.7041, 'grad_norm': 1.8718736171722412, 'learning_rate': 7.701149425287357e-06, 'epoch': 8.536519142003801}\n{'loss': 1.6143, 'grad_norm': 2.0866758823394775, 'learning_rate': 7.64367816091954e-06, 'epoch': 8.547379853380397}\n{'loss': 1.6387, 'grad_norm': 2.275205373764038, 'learning_rate': 7.586206896551724e-06, 'epoch': 8.558240564756991}\n{'loss': 1.6323, 'grad_norm': 5.837408542633057, 'learning_rate': 7.528735632183909e-06, 'epoch': 8.569101276133587}\n{'loss': 1.7053, 'grad_norm': 2.3046960830688477, 'learning_rate': 7.4712643678160925e-06, 'epoch': 8.579961987510181}\n{'loss': 1.7269, 'grad_norm': 2.103605031967163, 'learning_rate': 7.413793103448275e-06, 'epoch': 8.590822698886777}\n{'loss': 1.6902, 'grad_norm': 3.0330889225006104, 'learning_rate': 7.35632183908046e-06, 'epoch': 8.601683410263373}\n{'loss': 1.5916, 'grad_norm': 1.7742873430252075, 'learning_rate': 7.298850574712645e-06, 'epoch': 8.612544121639967}\n{'loss': 1.6243, 'grad_norm': 2.1762654781341553, 'learning_rate': 7.241379310344828e-06, 'epoch': 8.623404833016563}\n{'loss': 1.7451, 'grad_norm': 2.58854079246521, 'learning_rate': 7.183908045977011e-06, 'epoch': 8.634265544393157}\n{'loss': 1.6721, 'grad_norm': 2.5362982749938965, 'learning_rate': 7.126436781609196e-06, 'epoch': 8.645126255769753}\n{'loss': 1.5884, 'grad_norm': 2.1852173805236816, 'learning_rate': 7.0689655172413796e-06, 'epoch': 8.655986967146347}\n{'loss': 1.6947, 'grad_norm': 2.2930164337158203, 'learning_rate': 7.011494252873564e-06, 'epoch': 8.666847678522943}\n{'loss': 1.6132, 'grad_norm': 1.8923338651657104, 'learning_rate': 6.954022988505747e-06, 'epoch': 8.67770838989954}\n{'loss': 1.5584, 'grad_norm': 2.4703445434570312, 'learning_rate': 6.896551724137932e-06, 'epoch': 8.688569101276133}\n{'eval_loss': 1.6032536029815674, 'eval_runtime': 12.3116, 'eval_samples_per_second': 66.442, 'eval_steps_per_second': 66.442, 'epoch': 8.688569101276133}\n{'loss': 1.5793, 'grad_norm': 2.0114688873291016, 'learning_rate': 6.839080459770115e-06, 'epoch': 8.69942981265273}\n{'loss': 1.6026, 'grad_norm': 2.2947678565979004, 'learning_rate': 6.7816091954023e-06, 'epoch': 8.710290524029324}\n{'loss': 1.6917, 'grad_norm': 2.064561605453491, 'learning_rate': 6.724137931034483e-06, 'epoch': 8.72115123540592}\n{'loss': 1.707, 'grad_norm': 2.3431508541107178, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.732011946782514}\n{'loss': 1.7075, 'grad_norm': 5.379396915435791, 'learning_rate': 6.609195402298851e-06, 'epoch': 8.74287265815911}\n{'loss': 1.6761, 'grad_norm': 2.6436691284179688, 'learning_rate': 6.551724137931035e-06, 'epoch': 8.753733369535704}\n{'loss': 1.6735, 'grad_norm': 2.3490426540374756, 'learning_rate': 6.494252873563218e-06, 'epoch': 8.7645940809123}\n{'loss': 1.6991, 'grad_norm': 2.3522725105285645, 'learning_rate': 6.4367816091954025e-06, 'epoch': 8.775454792288896}\n{'loss': 1.7612, 'grad_norm': 3.9428088665008545, 'learning_rate': 6.379310344827587e-06, 'epoch': 8.78631550366549}\n{'loss': 1.5597, 'grad_norm': 3.0819380283355713, 'learning_rate': 6.321839080459771e-06, 'epoch': 8.797176215042086}\n{'loss': 1.6621, 'grad_norm': 3.255582809448242, 'learning_rate': 6.264367816091954e-06, 'epoch': 8.80803692641868}\n{'loss': 1.6312, 'grad_norm': 2.378894329071045, 'learning_rate': 6.206896551724138e-06, 'epoch': 8.818897637795276}\n{'loss': 1.6423, 'grad_norm': 1.8802943229675293, 'learning_rate': 6.149425287356322e-06, 'epoch': 8.82975834917187}\n{'loss': 1.6638, 'grad_norm': 1.961464762687683, 'learning_rate': 6.091954022988506e-06, 'epoch': 8.840619060548466}\n{'loss': 1.6472, 'grad_norm': 2.288630247116089, 'learning_rate': 6.03448275862069e-06, 'epoch': 8.85147977192506}\n{'loss': 1.6705, 'grad_norm': 3.109241008758545, 'learning_rate': 5.977011494252874e-06, 'epoch': 8.862340483301656}\n{'loss': 1.6207, 'grad_norm': 2.364614248275757, 'learning_rate': 5.919540229885058e-06, 'epoch': 8.873201194678252}\n{'loss': 1.6128, 'grad_norm': 2.1333298683166504, 'learning_rate': 5.862068965517242e-06, 'epoch': 8.884061906054846}\n{'loss': 1.667, 'grad_norm': 2.147820472717285, 'learning_rate': 5.804597701149426e-06, 'epoch': 8.894922617431442}\n{'loss': 1.6752, 'grad_norm': 2.042905807495117, 'learning_rate': 5.747126436781609e-06, 'epoch': 8.905783328808036}\n{'loss': 1.6394, 'grad_norm': 1.7647161483764648, 'learning_rate': 5.689655172413794e-06, 'epoch': 8.916644040184632}\n{'loss': 1.6069, 'grad_norm': 2.5580601692199707, 'learning_rate': 5.6321839080459775e-06, 'epoch': 8.927504751561226}\n{'loss': 1.6673, 'grad_norm': 2.3724257946014404, 'learning_rate': 5.574712643678161e-06, 'epoch': 8.938365462937822}\n{'loss': 1.6612, 'grad_norm': 2.422374963760376, 'learning_rate': 5.517241379310345e-06, 'epoch': 8.949226174314418}\n{'loss': 1.6267, 'grad_norm': 2.189120054244995, 'learning_rate': 5.45977011494253e-06, 'epoch': 8.960086885691013}\n{'loss': 1.6648, 'grad_norm': 3.1161270141601562, 'learning_rate': 5.4022988505747125e-06, 'epoch': 8.970947597067608}\n{'loss': 1.5945, 'grad_norm': 2.086884021759033, 'learning_rate': 5.344827586206897e-06, 'epoch': 8.981808308444203}\n{'loss': 1.6619, 'grad_norm': 2.598404884338379, 'learning_rate': 5.287356321839081e-06, 'epoch': 8.992669019820799}\n{'loss': 1.6738, 'grad_norm': 1.724966287612915, 'learning_rate': 5.2298850574712646e-06, 'epoch': 9.003529731197393}\n{'loss': 1.6678, 'grad_norm': 2.731973886489868, 'learning_rate': 5.172413793103448e-06, 'epoch': 9.014390442573989}\n{'loss': 1.7028, 'grad_norm': 1.9304611682891846, 'learning_rate': 5.114942528735633e-06, 'epoch': 9.025251153950585}\n{'loss': 1.6235, 'grad_norm': 2.2111082077026367, 'learning_rate': 5.057471264367817e-06, 'epoch': 9.036111865327179}\n{'loss': 1.6498, 'grad_norm': 2.4287497997283936, 'learning_rate': 5e-06, 'epoch': 9.046972576703775}\n{'loss': 1.6476, 'grad_norm': 3.812370538711548, 'learning_rate': 4.942528735632184e-06, 'epoch': 9.057833288080369}\n{'loss': 1.5326, 'grad_norm': 2.1225833892822266, 'learning_rate': 4.885057471264369e-06, 'epoch': 9.068693999456965}\n{'loss': 1.618, 'grad_norm': 1.9024001359939575, 'learning_rate': 4.827586206896552e-06, 'epoch': 9.079554710833559}\n{'loss': 1.6171, 'grad_norm': 1.9652241468429565, 'learning_rate': 4.770114942528736e-06, 'epoch': 9.090415422210155}\n{'loss': 1.6624, 'grad_norm': 2.1260037422180176, 'learning_rate': 4.71264367816092e-06, 'epoch': 9.10127613358675}\n{'loss': 1.7345, 'grad_norm': 2.4948058128356934, 'learning_rate': 4.655172413793104e-06, 'epoch': 9.112136844963345}\n{'loss': 1.5685, 'grad_norm': 1.7652599811553955, 'learning_rate': 4.5977011494252875e-06, 'epoch': 9.122997556339941}\n{'loss': 1.6194, 'grad_norm': 1.8527624607086182, 'learning_rate': 4.540229885057472e-06, 'epoch': 9.133858267716535}\n{'loss': 1.5774, 'grad_norm': 1.8045727014541626, 'learning_rate': 4.482758620689655e-06, 'epoch': 9.144718979093131}\n{'loss': 1.5403, 'grad_norm': 2.46761155128479, 'learning_rate': 4.42528735632184e-06, 'epoch': 9.155579690469725}\n{'loss': 1.6264, 'grad_norm': 2.4546430110931396, 'learning_rate': 4.367816091954023e-06, 'epoch': 9.166440401846321}\n{'loss': 1.6309, 'grad_norm': 2.54876708984375, 'learning_rate': 4.310344827586207e-06, 'epoch': 9.177301113222915}\n{'loss': 1.5804, 'grad_norm': 1.7749135494232178, 'learning_rate': 4.252873563218391e-06, 'epoch': 9.188161824599511}\n{'loss': 1.6096, 'grad_norm': 2.048410415649414, 'learning_rate': 4.195402298850575e-06, 'epoch': 9.199022535976107}\n{'loss': 1.801, 'grad_norm': 2.8279597759246826, 'learning_rate': 4.137931034482759e-06, 'epoch': 9.209883247352701}\n{'loss': 1.618, 'grad_norm': 2.195892095565796, 'learning_rate': 4.080459770114943e-06, 'epoch': 9.220743958729297}\n{'loss': 1.7484, 'grad_norm': 2.1923067569732666, 'learning_rate': 4.022988505747127e-06, 'epoch': 9.231604670105892}\n{'eval_loss': 1.60414719581604, 'eval_runtime': 12.2716, 'eval_samples_per_second': 66.658, 'eval_steps_per_second': 66.658, 'epoch': 9.231604670105892}\n{'loss': 1.6382, 'grad_norm': 6.56410026550293, 'learning_rate': 3.96551724137931e-06, 'epoch': 9.242465381482488}\n{'loss': 1.5969, 'grad_norm': 2.1384801864624023, 'learning_rate': 3.908045977011494e-06, 'epoch': 9.253326092859082}\n{'loss': 1.7044, 'grad_norm': 2.636787176132202, 'learning_rate': 3.850574712643679e-06, 'epoch': 9.264186804235678}\n{'loss': 1.6022, 'grad_norm': 2.047182321548462, 'learning_rate': 3.793103448275862e-06, 'epoch': 9.275047515612272}\n{'loss': 1.6843, 'grad_norm': 2.2484893798828125, 'learning_rate': 3.7356321839080462e-06, 'epoch': 9.285908226988868}\n{'loss': 1.6796, 'grad_norm': 6.311164855957031, 'learning_rate': 3.67816091954023e-06, 'epoch': 9.296768938365464}\n{'loss': 1.6022, 'grad_norm': 2.851389169692993, 'learning_rate': 3.620689655172414e-06, 'epoch': 9.307629649742058}\n{'loss': 1.6867, 'grad_norm': 2.6722028255462646, 'learning_rate': 3.563218390804598e-06, 'epoch': 9.318490361118654}\n{'loss': 1.6392, 'grad_norm': 1.9785295724868774, 'learning_rate': 3.505747126436782e-06, 'epoch': 9.329351072495248}\n{'loss': 1.7343, 'grad_norm': 2.2434022426605225, 'learning_rate': 3.448275862068966e-06, 'epoch': 9.340211783871844}\n{'loss': 1.5882, 'grad_norm': 2.1025428771972656, 'learning_rate': 3.39080459770115e-06, 'epoch': 9.351072495248438}\n{'loss': 1.6303, 'grad_norm': 3.0935840606689453, 'learning_rate': 3.3333333333333333e-06, 'epoch': 9.361933206625034}\n{'loss': 1.5994, 'grad_norm': 2.5574707984924316, 'learning_rate': 3.2758620689655175e-06, 'epoch': 9.37279391800163}\n{'loss': 1.5656, 'grad_norm': 2.362272262573242, 'learning_rate': 3.2183908045977012e-06, 'epoch': 9.383654629378224}\n{'loss': 1.5598, 'grad_norm': 2.1922247409820557, 'learning_rate': 3.1609195402298854e-06, 'epoch': 9.39451534075482}\n{'loss': 1.5781, 'grad_norm': 2.3685190677642822, 'learning_rate': 3.103448275862069e-06, 'epoch': 9.405376052131414}\n{'loss': 1.6213, 'grad_norm': 2.282896041870117, 'learning_rate': 3.045977011494253e-06, 'epoch': 9.41623676350801}\n{'loss': 1.6968, 'grad_norm': 2.0370521545410156, 'learning_rate': 2.988505747126437e-06, 'epoch': 9.427097474884604}\n{'loss': 1.6522, 'grad_norm': 2.363645315170288, 'learning_rate': 2.931034482758621e-06, 'epoch': 9.4379581862612}\n{'loss': 1.639, 'grad_norm': 2.727982997894287, 'learning_rate': 2.8735632183908046e-06, 'epoch': 9.448818897637794}\n{'loss': 1.6366, 'grad_norm': 4.438344955444336, 'learning_rate': 2.8160919540229887e-06, 'epoch': 9.45967960901439}\n{'loss': 1.6469, 'grad_norm': 3.7056503295898438, 'learning_rate': 2.7586206896551725e-06, 'epoch': 9.470540320390986}\n{'loss': 1.6893, 'grad_norm': 3.265169620513916, 'learning_rate': 2.7011494252873562e-06, 'epoch': 9.48140103176758}\n{'loss': 1.582, 'grad_norm': 1.73966383934021, 'learning_rate': 2.6436781609195404e-06, 'epoch': 9.492261743144176}\n{'loss': 1.579, 'grad_norm': 2.065854787826538, 'learning_rate': 2.586206896551724e-06, 'epoch': 9.50312245452077}\n{'loss': 1.7026, 'grad_norm': 2.4732439517974854, 'learning_rate': 2.5287356321839083e-06, 'epoch': 9.513983165897367}\n{'loss': 1.7075, 'grad_norm': 2.6838274002075195, 'learning_rate': 2.471264367816092e-06, 'epoch': 9.52484387727396}\n{'loss': 1.6897, 'grad_norm': 2.853703737258911, 'learning_rate': 2.413793103448276e-06, 'epoch': 9.535704588650557}\n{'loss': 1.5806, 'grad_norm': 1.9477992057800293, 'learning_rate': 2.35632183908046e-06, 'epoch': 9.54656530002715}\n{'loss': 1.6396, 'grad_norm': 2.0079126358032227, 'learning_rate': 2.2988505747126437e-06, 'epoch': 9.557426011403747}\n{'loss': 1.6749, 'grad_norm': 2.2946698665618896, 'learning_rate': 2.2413793103448275e-06, 'epoch': 9.568286722780343}\n{'loss': 1.6106, 'grad_norm': 2.1922049522399902, 'learning_rate': 2.1839080459770117e-06, 'epoch': 9.579147434156937}\n{'loss': 1.616, 'grad_norm': 1.9531210660934448, 'learning_rate': 2.1264367816091954e-06, 'epoch': 9.590008145533533}\n{'loss': 1.5869, 'grad_norm': 2.1356334686279297, 'learning_rate': 2.0689655172413796e-06, 'epoch': 9.600868856910127}\n{'loss': 1.6168, 'grad_norm': 2.253678798675537, 'learning_rate': 2.0114942528735633e-06, 'epoch': 9.611729568286723}\n{'loss': 1.721, 'grad_norm': 3.742278575897217, 'learning_rate': 1.954022988505747e-06, 'epoch': 9.622590279663317}\n{'loss': 1.6513, 'grad_norm': 2.198589563369751, 'learning_rate': 1.896551724137931e-06, 'epoch': 9.633450991039913}\n{'loss': 1.6421, 'grad_norm': 1.8570420742034912, 'learning_rate': 1.839080459770115e-06, 'epoch': 9.644311702416509}\n{'loss': 1.6829, 'grad_norm': 5.838590145111084, 'learning_rate': 1.781609195402299e-06, 'epoch': 9.655172413793103}\n{'loss': 1.6037, 'grad_norm': 2.975163698196411, 'learning_rate': 1.724137931034483e-06, 'epoch': 9.6660331251697}\n{'loss': 1.6931, 'grad_norm': 2.2578580379486084, 'learning_rate': 1.6666666666666667e-06, 'epoch': 9.676893836546293}\n{'loss': 1.6485, 'grad_norm': 2.71305775642395, 'learning_rate': 1.6091954022988506e-06, 'epoch': 9.68775454792289}\n{'loss': 1.6162, 'grad_norm': 2.5580148696899414, 'learning_rate': 1.5517241379310346e-06, 'epoch': 9.698615259299483}\n{'loss': 1.6762, 'grad_norm': 2.188922643661499, 'learning_rate': 1.4942528735632185e-06, 'epoch': 9.70947597067608}\n{'loss': 1.6422, 'grad_norm': 1.5884432792663574, 'learning_rate': 1.4367816091954023e-06, 'epoch': 9.720336682052675}\n{'loss': 1.6652, 'grad_norm': 2.781306743621826, 'learning_rate': 1.3793103448275862e-06, 'epoch': 9.73119739342927}\n{'loss': 1.7314, 'grad_norm': 2.3078808784484863, 'learning_rate': 1.3218390804597702e-06, 'epoch': 9.742058104805865}\n{'loss': 1.6725, 'grad_norm': 2.3263139724731445, 'learning_rate': 1.2643678160919542e-06, 'epoch': 9.75291881618246}\n{'loss': 1.6025, 'grad_norm': 1.9510442018508911, 'learning_rate': 1.206896551724138e-06, 'epoch': 9.763779527559056}\n{'loss': 1.5975, 'grad_norm': 2.6217801570892334, 'learning_rate': 1.1494252873563219e-06, 'epoch': 9.77464023893565}\n{'eval_loss': 1.602613091468811, 'eval_runtime': 12.3398, 'eval_samples_per_second': 66.289, 'eval_steps_per_second': 66.289, 'epoch': 9.77464023893565}\n{'loss': 1.6396, 'grad_norm': 3.2634358406066895, 'learning_rate': 1.0919540229885058e-06, 'epoch': 9.785500950312246}\n{'loss': 1.6409, 'grad_norm': 2.6978492736816406, 'learning_rate': 1.0344827586206898e-06, 'epoch': 9.79636166168884}\n{'loss': 1.6652, 'grad_norm': 1.891231894493103, 'learning_rate': 9.770114942528735e-07, 'epoch': 9.807222373065436}\n{'loss': 1.7198, 'grad_norm': 2.489650011062622, 'learning_rate': 9.195402298850575e-07, 'epoch': 9.818083084442032}\n{'loss': 1.7148, 'grad_norm': 3.0803470611572266, 'learning_rate': 8.620689655172415e-07, 'epoch': 9.828943795818626}\n{'loss': 1.6895, 'grad_norm': 2.031097888946533, 'learning_rate': 8.045977011494253e-07, 'epoch': 9.839804507195222}\n{'loss': 1.7275, 'grad_norm': 1.866657018661499, 'learning_rate': 7.471264367816093e-07, 'epoch': 9.850665218571816}\n{'loss': 1.6492, 'grad_norm': 2.0018670558929443, 'learning_rate': 6.896551724137931e-07, 'epoch': 9.861525929948412}\n{'loss': 1.6636, 'grad_norm': 1.9948700666427612, 'learning_rate': 6.321839080459771e-07, 'epoch': 9.872386641325006}\n{'loss': 1.6854, 'grad_norm': 1.7927756309509277, 'learning_rate': 5.747126436781609e-07, 'epoch': 9.883247352701602}\n{'loss': 1.7544, 'grad_norm': 2.5542171001434326, 'learning_rate': 5.172413793103449e-07, 'epoch': 9.894108064078196}\n{'loss': 1.6348, 'grad_norm': 2.7204294204711914, 'learning_rate': 4.5977011494252875e-07, 'epoch': 9.904968775454792}\n{'loss': 1.6629, 'grad_norm': 2.139580011367798, 'learning_rate': 4.0229885057471266e-07, 'epoch': 9.915829486831388}\n{'loss': 1.532, 'grad_norm': 1.8411059379577637, 'learning_rate': 3.4482758620689656e-07, 'epoch': 9.926690198207982}\n{'loss': 1.6426, 'grad_norm': 2.0198583602905273, 'learning_rate': 2.8735632183908047e-07, 'epoch': 9.937550909584578}\n{'loss': 1.5403, 'grad_norm': 2.362248659133911, 'learning_rate': 2.2988505747126437e-07, 'epoch': 9.948411620961172}\n{'loss': 1.6609, 'grad_norm': 2.4136905670166016, 'learning_rate': 1.7241379310344828e-07, 'epoch': 9.959272332337768}\n{'loss': 1.5757, 'grad_norm': 2.3067190647125244, 'learning_rate': 1.1494252873563219e-07, 'epoch': 9.970133043714362}\n{'loss': 1.6592, 'grad_norm': 2.831968069076538, 'learning_rate': 5.7471264367816094e-08, 'epoch': 9.980993755090958}\n{'loss': 1.6217, 'grad_norm': 2.428292989730835, 'learning_rate': 0.0, 'epoch': 9.991854466467554}\n{'train_runtime': 7086.719, 'train_samples_per_second': 20.788, 'train_steps_per_second': 1.298, 'train_loss': 1.7420032549941022, 'epoch': 9.991854466467554}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 410/410 [03:58<00:00,  1.72it/s]\n",
     "output_type": "stream"
    },
    {
     "execution_count": 83,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        BLEU\nT5  0.108711",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BLEU</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>T5</th>\n      <td>0.108711</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Generating Dialogue Summarie",
   "metadata": {
    "id": "B8wFuleSG4I1"
   }
  },
  {
   "cell_type": "code",
   "source": "# Importing the necessary module from transformers\nimport transformers\nfrom transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n\n# Setting the logging level to suppress warnings, only show errors\ntransformers.logging.set_verbosity_error()\n\n# Defining the generation parameters for the summarization model\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\": 8, \"max_length\": 128}\n\n# Sample text from the dataset to summarize\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n\n# The reference summary for comparison\nreference = dataset_samsum[\"test\"][0][\"summary\"]\n\n# Load the fine-tuned model and tokenizer from the specified checkpoint directory\ncheckpoint_dir = '/kaggle/working/t5/checkpoint-920'\nmodel = T5ForConditionalGeneration.from_pretrained(checkpoint_dir)\ntokenizer = T5Tokenizer.from_pretrained(checkpoint_dir)\n\n# Initialize the summarization pipeline with the locally saved model\npipe = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n\n# Display the sample dialogue\nprint(\"Dialogue:\")\nprint(sample_text)\n\n# Display the reference summary\nprint(\"\\nReference Summary:\")\nprint(reference)\n\n# Generate and display the summary using the model pipeline\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-09T20:48:18.780717Z",
     "iopub.execute_input": "2024-11-09T20:48:18.781638Z",
     "iopub.status.idle": "2024-11-09T20:48:20.687445Z",
     "shell.execute_reply.started": "2024-11-09T20:48:18.781594Z",
     "shell.execute_reply": "2024-11-09T20:48:20.686365Z"
    },
    "id": "5H-PzvdwG4I4",
    "outputId": "f358a218-6508-4f54-dd0a-7c149992185d"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Dialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nReference Summary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n\nModel Summary:\nAmanda has Betty's number. He called her last time we were at the park together. She'd rather Amanda text him .\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Define a custom dialogue text for testing the summarization pipeline\ncustom_dialogue = \"\"\"\\\nThom: Hi guys, have you heard of transformers?\nLewis: Yes, I used them recently!\nLeandro: Indeed, there is a great library by Hugging Face.\nThom: I know, I helped build it ;)\nLewis: Cool, maybe we should write a book about it. What do you think?\nLeandro: Great idea, how hard can it be?!\nThom: I am in!\nLewis: Awesome, let's do it together!\n\"\"\"\n\n# Generate and display the summary for the custom dialogue\nprint(pipe(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-09T20:48:45.722053Z",
     "iopub.execute_input": "2024-11-09T20:48:45.723078Z",
     "iopub.status.idle": "2024-11-09T20:48:47.001430Z",
     "shell.execute_reply.started": "2024-11-09T20:48:45.723033Z",
     "shell.execute_reply": "2024-11-09T20:48:47.000190Z"
    },
    "id": "8J0kqrltG4I4",
    "outputId": "b63807b7-70ff-4846-c9d2-a392ffd77dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Leandro and Lewis have heard of transformers recently. They have a great library by Hugging Face. They will write a book about it.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true,
    "id": "rO6fzwdsG4I4"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}